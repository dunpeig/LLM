{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Training Pipeline\n",
    "[run_training_dpo_pipeline.ipynb](https://github.com/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb)    | [Open In Colab](https://colab.research.google.com/github/shibing624/MedicalGPT/blob/main/run_training_dpo_pipeline.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Stage 1: Continue Pretraining\n",
    "\n",
    "第一阶段：PT(Continue PreTraining)增量预训练，在海量领域文本数据上二次预训练GPT模型，以适配领域数据分布\n",
    "\n",
    "注意：\n",
    "1. 此阶段是可选的，如果你没有海量领域文本，可以跳过此阶段，直接进行SFT阶段的有监督微调\n",
    "2. 我实验发现：做领域知识注入，SFT比PT更高效，也可以跳过PT阶段\n",
    "\n",
    "| Stage 1: Continue Pretraining   |  [pretraining.py](https://github.com/shibing624/MedicalGPT/blob/main/pretraining.py) | [run_pt.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_pt.sh)    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 说明：\n",
    "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
    "\n",
    "1. 生成模型：使用的是Bloom的`bigscience/bloomz-560m`\n",
    "2. 数据集：PT阶段使用的是中文天龙八部小说部分文本和英文书籍部分文本，位于`data/pretrain`文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置运行环境\n",
    "\n",
    "本地执行可注释以下配置环境的命令，colab执行要打开注释，用于配置环境\n",
    "\n",
    "colab建议使用T4 GPU训练，设置方式：`代码执行程序 -> 更改运行时类型 -> 运行时类型：Python3，硬件加速器：GPU，GPU类型：T4 -> 保存`\n",
    "\n",
    "步骤：\n",
    "1. 下载最新代码到本地\n",
    "2. 安装依赖包\n",
    "\n",
    "依赖包如下，保证最新版本：\n",
    "\n",
    "```\n",
    "loguru\n",
    "transformers\n",
    "sentencepiece\n",
    "datasets\n",
    "tensorboard\n",
    "tqdm\n",
    "peft\n",
    "trl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] The system cannot find the file specified: 'MedicalGPT'\n",
      "d:\\llm\\whole_process\\MedicalGPT\n",
      " Volume in drive D is New Volume\n",
      " Volume Serial Number is B4E8-CC63\n",
      "\n",
      " Directory of d:\\llm\\whole_process\\MedicalGPT\n",
      "\n",
      "06/17/2024  04:52 PM    <DIR>          .\n",
      "06/14/2024  09:35 PM    <DIR>          ..\n",
      "06/12/2024  07:20 PM    <DIR>          .github\n",
      "06/12/2024  07:20 PM             1,936 .gitignore\n",
      "06/13/2024  12:57 PM    <DIR>          __pycache__\n",
      "06/12/2024  07:20 PM                26 _config.yml\n",
      "06/12/2024  07:20 PM             2,127 build_domain_tokenizer.py\n",
      "06/15/2024  01:36 PM    <DIR>          cache\n",
      "06/12/2024  07:20 PM            20,272 chatpdf.py\n",
      "06/12/2024  07:20 PM               317 CITATION.cff\n",
      "06/12/2024  07:20 PM               473 CONTRIBUTING.md\n",
      "06/12/2024  07:20 PM             2,673 convert_dataset.py\n",
      "06/12/2024  07:20 PM    <DIR>          data\n",
      "06/12/2024  07:20 PM             1,171 deepspeed_zero_stage2_config.json\n",
      "06/12/2024  07:20 PM             1,277 deepspeed_zero_stage3_config.json\n",
      "06/12/2024  07:20 PM             3,549 DISCLAIMER\n",
      "06/12/2024  07:20 PM    <DIR>          docs\n",
      "06/12/2024  07:20 PM            23,360 dpo_training.py\n",
      "06/12/2024  07:20 PM             7,012 fastapi_server_demo.py\n",
      "06/12/2024  07:20 PM             5,167 gradio_demo.py\n",
      "06/12/2024  07:20 PM            10,467 inference.py\n",
      "06/12/2024  07:20 PM             8,833 inference_multigpu_demo.py\n",
      "06/12/2024  07:20 PM            11,558 LICENSE\n",
      "06/12/2024  07:20 PM             4,470 merge_peft_adapter.py\n",
      "06/12/2024  07:20 PM             6,475 merge_tokenizers.py\n",
      "06/14/2024  09:54 PM    <DIR>          merged-dpo\n",
      "06/13/2024  01:35 PM    <DIR>          merged-pt\n",
      "06/13/2024  02:23 PM    <DIR>          merged-rm\n",
      "06/13/2024  01:42 PM    <DIR>          merged-sft\n",
      "06/12/2024  07:20 PM            21,812 openai_api.py\n",
      "06/14/2024  09:47 PM    <DIR>          OpenHermes-2.5-Mistral-7B\n",
      "06/12/2024  07:20 PM            23,492 orpo_training.py\n",
      "06/15/2024  03:49 PM    <DIR>          outputs-dpo-Mistral\n",
      "06/14/2024  09:35 PM    <DIR>          outputs-dpo-v1\n",
      "06/13/2024  11:48 AM    <DIR>          outputs-pt-qwen-v1\n",
      "06/13/2024  01:30 PM    <DIR>          outputs-pt-v1\n",
      "06/13/2024  12:58 PM    <DIR>          outputs-rm-qwen-v1\n",
      "06/13/2024  02:23 PM    <DIR>          outputs-rm-v1\n",
      "06/13/2024  11:57 AM    <DIR>          outputs-sft-qwen-v1\n",
      "06/13/2024  01:37 PM    <DIR>          outputs-sft-v1\n",
      "06/15/2024  03:56 PM            24,074 ppo_training.py\n",
      "06/12/2024  07:20 PM            34,740 pretraining.py\n",
      "06/13/2024  11:03 AM    <DIR>          Qwen\n",
      "06/12/2024  07:20 PM            46,205 README.md\n",
      "06/12/2024  07:20 PM            16,049 README_EN.md\n",
      "06/12/2024  07:20 PM               148 requirements.txt\n",
      "06/12/2024  07:20 PM            29,248 reward_modeling.py\n",
      "06/12/2024  07:20 PM               906 run_dpo.sh\n",
      "06/12/2024  07:20 PM               888 run_orpo.sh\n",
      "06/12/2024  07:20 PM               856 run_ppo.sh\n",
      "06/13/2024  01:27 PM             1,313 run_pt.sh\n",
      "06/13/2024  01:30 PM             1,203 run_rm.sh\n",
      "06/13/2024  01:18 PM             1,311 run_sft.sh\n",
      "06/20/2024  08:13 PM           363,353 run_training_dpo_pipeline.ipynb\n",
      "06/15/2024  03:56 PM           146,818 run_training_ppo_pipeline.ipynb\n",
      "06/13/2024  02:03 PM    <DIR>          shibing624data\n",
      "06/12/2024  07:20 PM            45,787 supervised_finetuning.py\n",
      "06/12/2024  07:20 PM            16,490 template.py\n",
      "06/13/2024  12:53 PM               288 test_script.sh\n",
      "              37 File(s)        886,144 bytes\n",
      "              22 Dir(s)  446,528,978,944 bytes free\n",
      "Requirement already satisfied: accelerate~=0.27.2 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from -r requirements.txt (line 1)) (0.27.2)\n",
      "Requirement already satisfied: datasets>=2.14.6 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from -r requirements.txt (line 2)) (2.14.6)\n",
      "Requirement already satisfied: loguru in d:\\anaconda\\envs\\py39\\lib\\site-packages (from -r requirements.txt (line 3)) (0.7.2)\n",
      "Requirement already satisfied: peft~=0.10.0 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from -r requirements.txt (line 4)) (0.10.0)\n",
      "Requirement already satisfied: sentencepiece in d:\\anaconda\\envs\\py39\\lib\\site-packages (from -r requirements.txt (line 5)) (0.1.99)\n",
      "Requirement already satisfied: scikit-learn in d:\\anaconda\\envs\\py39\\lib\\site-packages (from -r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: tensorboard in d:\\anaconda\\envs\\py39\\lib\\site-packages (from -r requirements.txt (line 7)) (2.16.2)\n",
      "Requirement already satisfied: tqdm>=4.47.0 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from -r requirements.txt (line 8)) (4.66.4)\n",
      "Requirement already satisfied: transformers>=4.39.3 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from -r requirements.txt (line 9)) (4.39.3)\n",
      "Requirement already satisfied: trl~=0.8.3 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from -r requirements.txt (line 10)) (0.8.3)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from accelerate~=0.27.2->-r requirements.txt (line 1)) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from accelerate~=0.27.2->-r requirements.txt (line 1)) (23.2)\n",
      "Requirement already satisfied: psutil in d:\\anaconda\\envs\\py39\\lib\\site-packages (from accelerate~=0.27.2->-r requirements.txt (line 1)) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in d:\\anaconda\\envs\\py39\\lib\\site-packages (from accelerate~=0.27.2->-r requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from accelerate~=0.27.2->-r requirements.txt (line 1)) (2.3.0)\n",
      "Requirement already satisfied: huggingface-hub in d:\\anaconda\\envs\\py39\\lib\\site-packages (from accelerate~=0.27.2->-r requirements.txt (line 1)) (0.23.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from accelerate~=0.27.2->-r requirements.txt (line 1)) (0.4.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (0.3.7)\n",
      "Requirement already satisfied: pandas in d:\\anaconda\\envs\\py39\\lib\\site-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: xxhash in d:\\anaconda\\envs\\py39\\lib\\site-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in d:\\anaconda\\envs\\py39\\lib\\site-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets>=2.14.6->-r requirements.txt (line 2)) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in d:\\anaconda\\envs\\py39\\lib\\site-packages (from datasets>=2.14.6->-r requirements.txt (line 2)) (3.9.3)\n",
      "Requirement already satisfied: colorama>=0.3.4 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from loguru->-r requirements.txt (line 3)) (0.4.6)\n",
      "Requirement already satisfied: win32-setctime>=1.0.0 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from loguru->-r requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 6)) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 6)) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from scikit-learn->-r requirements.txt (line 6)) (2.2.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from tensorboard->-r requirements.txt (line 7)) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from tensorboard->-r requirements.txt (line 7)) (1.63.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from tensorboard->-r requirements.txt (line 7)) (3.4.1)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from tensorboard->-r requirements.txt (line 7)) (4.25.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from tensorboard->-r requirements.txt (line 7)) (68.2.2)\n",
      "Requirement already satisfied: six>1.9 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from tensorboard->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from tensorboard->-r requirements.txt (line 7)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from tensorboard->-r requirements.txt (line 7)) (2.2.3)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\envs\\py39\\lib\\site-packages (from transformers>=4.39.3->-r requirements.txt (line 9)) (3.13.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from transformers>=4.39.3->-r requirements.txt (line 9)) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from transformers>=4.39.3->-r requirements.txt (line 9)) (0.15.2)\n",
      "Requirement already satisfied: tyro>=0.5.11 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from trl~=0.8.3->-r requirements.txt (line 10)) (0.8.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2)) (1.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from aiohttp->datasets>=2.14.6->-r requirements.txt (line 2)) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from huggingface-hub->accelerate~=0.27.2->-r requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 7)) (7.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from requests>=2.19.0->datasets>=2.14.6->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from requests>=2.19.0->datasets>=2.14.6->-r requirements.txt (line 2)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from requests>=2.19.0->datasets>=2.14.6->-r requirements.txt (line 2)) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from requests>=2.19.0->datasets>=2.14.6->-r requirements.txt (line 2)) (2024.2.2)\n",
      "Requirement already satisfied: sympy in d:\\anaconda\\envs\\py39\\lib\\site-packages (from torch>=1.10.0->accelerate~=0.27.2->-r requirements.txt (line 1)) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\anaconda\\envs\\py39\\lib\\site-packages (from torch>=1.10.0->accelerate~=0.27.2->-r requirements.txt (line 1)) (3.1)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from torch>=1.10.0->accelerate~=0.27.2->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from torch>=1.10.0->accelerate~=0.27.2->-r requirements.txt (line 1)) (2021.4.0)\n",
      "Requirement already satisfied: docstring-parser>=0.14.1 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from tyro>=0.5.11->trl~=0.8.3->-r requirements.txt (line 10)) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from tyro>=0.5.11->trl~=0.8.3->-r requirements.txt (line 10)) (13.3.5)\n",
      "Requirement already satisfied: shtab>=1.5.6 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from tyro>=0.5.11->trl~=0.8.3->-r requirements.txt (line 10)) (1.7.1)\n",
      "Requirement already satisfied: eval-type-backport>=0.1.3 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from tyro>=0.5.11->trl~=0.8.3->-r requirements.txt (line 10)) (0.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 7)) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from pandas->datasets>=2.14.6->-r requirements.txt (line 2)) (2023.3)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 7)) (3.17.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in d:\\anaconda\\envs\\py39\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate~=0.27.2->-r requirements.txt (line 1)) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in d:\\anaconda\\envs\\py39\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.10.0->accelerate~=0.27.2->-r requirements.txt (line 1)) (2021.12.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl~=0.8.3->-r requirements.txt (line 10)) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from rich>=11.1.0->tyro>=0.5.11->trl~=0.8.3->-r requirements.txt (line 10)) (2.15.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate~=0.27.2->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\anaconda\\envs\\py39\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl~=0.8.3->-r requirements.txt (line 10)) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "#!git clone --depth 1 https://github.com/shibing624/MedicalGPT.git\n",
    "%cd MedicalGPT\n",
    "%ls\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage1 咱们开始吧\n",
    "\n",
    "训练步骤如下：\n",
    "\n",
    "1. 确认训练集\n",
    "2. 执行训练脚本\n",
    "\n",
    "训练脚本的执行逻辑如下：\n",
    "1. 导入依赖包\n",
    "2. 设置参数\n",
    "3. 定义各函数并加载训练集\n",
    "4. 加载模型和tokenizer\n",
    "5. 开始训练并评估\n",
    "6. 查看训练结果\n",
    "\n",
    "**以下参数可以根据你的GPU实际情况修改，当前参数是根据Colab的T4单卡GPU（16GB显存）配置的**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is New Volume\n",
      " Volume Serial Number is B4E8-CC63\n",
      "\n",
      " Directory of d:\\llm\\whole_process\\MedicalGPT\\data\\pretrain\n",
      "\n",
      "06/12/2024  07:20 PM    <DIR>          .\n",
      "06/12/2024  07:20 PM    <DIR>          ..\n",
      "06/12/2024  07:20 PM            27,992 en_article_tail500.txt\n",
      "06/12/2024  07:20 PM           352,651 fever.txt\n",
      "06/12/2024  07:20 PM           853,842 tianlongbabu.txt\n",
      "               3 File(s)      1,234,485 bytes\n",
      "               2 Dir(s)  446,528,978,944 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls data\\pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is New Volume\n",
      " Volume Serial Number is B4E8-CC63\n",
      "\n",
      " Directory of d:\\llm\\whole_process\\MedicalGPT\\data\\pretrain\n",
      "\n",
      "06/12/2024  07:20 PM    <DIR>          .\n",
      "06/12/2024  07:20 PM    <DIR>          ..\n",
      "06/12/2024  07:20 PM            27,992 en_article_tail500.txt\n",
      "06/12/2024  07:20 PM           352,651 fever.txt\n",
      "06/12/2024  07:20 PM           853,842 tianlongbabu.txt\n",
      "               3 File(s)      1,234,485 bytes\n",
      "               2 Dir(s)  446,528,978,944 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls data\\pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7,495,680 || all params: 1,844,324,352 || trainable%: 0.4064187512284173\n",
      "{'loss': 4.2204, 'grad_norm': 1.4093387126922607, 'learning_rate': 4.7619047619047615e-06, 'epoch': 0.0}\n",
      "{'loss': 3.7902, 'grad_norm': 1.3315517902374268, 'learning_rate': 4.761904761904762e-05, 'epoch': 0.01}\n",
      "{'loss': 3.7574, 'grad_norm': 1.3139597177505493, 'learning_rate': 9.047619047619048e-05, 'epoch': 0.02}\n",
      "{'loss': 3.619, 'grad_norm': 1.4987647533416748, 'learning_rate': 0.0001380952380952381, 'epoch': 0.04}\n",
      "{'loss': 3.4705, 'grad_norm': 1.748633623123169, 'learning_rate': 0.00018571428571428572, 'epoch': 0.05}\n",
      "{'loss': 3.5659, 'grad_norm': 2.238861560821533, 'learning_rate': 0.00019823232323232324, 'epoch': 0.06}\n",
      "{'eval_loss': 3.651273012161255, 'eval_accuracy': 0.3590551181102362, 'eval_runtime': 0.184, 'eval_samples_per_second': 54.337, 'eval_steps_per_second': 21.735, 'epoch': 0.06}\n",
      "{'loss': 3.4443, 'grad_norm': 1.8516820669174194, 'learning_rate': 0.0001957070707070707, 'epoch': 0.07}\n",
      "{'loss': 3.4877, 'grad_norm': 4.302374839782715, 'learning_rate': 0.0001931818181818182, 'epoch': 0.08}\n",
      "{'loss': 3.4054, 'grad_norm': 1.9403491020202637, 'learning_rate': 0.00019065656565656565, 'epoch': 0.1}\n",
      "{'loss': 3.3824, 'grad_norm': 1.8350316286087036, 'learning_rate': 0.00018813131313131313, 'epoch': 0.11}\n",
      "{'loss': 3.3201, 'grad_norm': 1.7611865997314453, 'learning_rate': 0.00018560606060606061, 'epoch': 0.12}\n",
      "{'eval_loss': 3.5914275646209717, 'eval_accuracy': 0.36220472440944884, 'eval_runtime': 0.164, 'eval_samples_per_second': 60.989, 'eval_steps_per_second': 24.396, 'epoch': 0.12}\n",
      "{'loss': 3.4245, 'grad_norm': 2.093655824661255, 'learning_rate': 0.0001830808080808081, 'epoch': 0.13}\n",
      "{'loss': 3.2641, 'grad_norm': 2.0090558528900146, 'learning_rate': 0.00018055555555555557, 'epoch': 0.14}\n",
      "{'loss': 3.4537, 'grad_norm': 1.768031358718872, 'learning_rate': 0.00017803030303030303, 'epoch': 0.16}\n",
      "{'loss': 3.3196, 'grad_norm': 2.1576249599456787, 'learning_rate': 0.0001755050505050505, 'epoch': 0.17}\n",
      "{'loss': 3.3982, 'grad_norm': 1.9122105836868286, 'learning_rate': 0.000172979797979798, 'epoch': 0.18}\n",
      "{'eval_loss': 3.523881196975708, 'eval_accuracy': 0.36850393700787404, 'eval_runtime': 0.1693, 'eval_samples_per_second': 59.081, 'eval_steps_per_second': 23.632, 'epoch': 0.18}\n",
      "{'loss': 3.2162, 'grad_norm': 2.122091054916382, 'learning_rate': 0.00017045454545454547, 'epoch': 0.19}\n",
      "{'loss': 3.1626, 'grad_norm': 2.117394208908081, 'learning_rate': 0.00016792929292929295, 'epoch': 0.2}\n",
      "{'loss': 3.2541, 'grad_norm': 2.121256113052368, 'learning_rate': 0.0001654040404040404, 'epoch': 0.22}\n",
      "{'loss': 3.0158, 'grad_norm': 1.8896160125732422, 'learning_rate': 0.0001628787878787879, 'epoch': 0.23}\n",
      "{'loss': 3.2782, 'grad_norm': 1.7915613651275635, 'learning_rate': 0.00016035353535353536, 'epoch': 0.24}\n",
      "{'eval_loss': 3.5424156188964844, 'eval_accuracy': 0.3669291338582677, 'eval_runtime': 0.183, 'eval_samples_per_second': 54.641, 'eval_steps_per_second': 21.856, 'epoch': 0.24}\n",
      "{'loss': 3.163, 'grad_norm': 1.961311936378479, 'learning_rate': 0.00015782828282828284, 'epoch': 0.25}\n",
      "{'loss': 3.0789, 'grad_norm': 1.967221975326538, 'learning_rate': 0.0001553030303030303, 'epoch': 0.26}\n",
      "{'loss': 3.2295, 'grad_norm': 1.9796439409255981, 'learning_rate': 0.00015277777777777777, 'epoch': 0.28}\n",
      "{'loss': 3.1863, 'grad_norm': 1.9878870248794556, 'learning_rate': 0.00015025252525252526, 'epoch': 0.29}\n",
      "{'loss': 3.2739, 'grad_norm': 1.801017165184021, 'learning_rate': 0.00014772727272727274, 'epoch': 0.3}\n",
      "{'eval_loss': 3.4478111267089844, 'eval_accuracy': 0.37716535433070864, 'eval_runtime': 0.165, 'eval_samples_per_second': 60.606, 'eval_steps_per_second': 24.242, 'epoch': 0.3}\n",
      "{'loss': 3.1406, 'grad_norm': 1.8521324396133423, 'learning_rate': 0.00014520202020202022, 'epoch': 0.31}\n",
      "{'loss': 3.2487, 'grad_norm': 1.9875980615615845, 'learning_rate': 0.00014267676767676767, 'epoch': 0.32}\n",
      "{'loss': 3.0543, 'grad_norm': 1.670246958732605, 'learning_rate': 0.00014015151515151518, 'epoch': 0.34}\n",
      "{'loss': 3.0536, 'grad_norm': 1.9346598386764526, 'learning_rate': 0.00013762626262626263, 'epoch': 0.35}\n",
      "{'loss': 3.2055, 'grad_norm': 2.3571386337280273, 'learning_rate': 0.0001351010101010101, 'epoch': 0.36}\n",
      "{'eval_loss': 3.3789172172546387, 'eval_accuracy': 0.3850393700787402, 'eval_runtime': 0.1601, 'eval_samples_per_second': 62.471, 'eval_steps_per_second': 24.989, 'epoch': 0.36}\n",
      "{'loss': 3.1012, 'grad_norm': 2.0797717571258545, 'learning_rate': 0.00013257575757575756, 'epoch': 0.37}\n",
      "{'loss': 3.1628, 'grad_norm': 2.0993752479553223, 'learning_rate': 0.00013005050505050507, 'epoch': 0.38}\n",
      "{'loss': 3.0996, 'grad_norm': 2.1772234439849854, 'learning_rate': 0.00012752525252525255, 'epoch': 0.4}\n",
      "{'loss': 3.1963, 'grad_norm': 1.9457014799118042, 'learning_rate': 0.000125, 'epoch': 0.41}\n",
      "{'loss': 3.2223, 'grad_norm': 1.9386433362960815, 'learning_rate': 0.00012247474747474748, 'epoch': 0.42}\n",
      "{'eval_loss': 3.3842263221740723, 'eval_accuracy': 0.3826771653543307, 'eval_runtime': 0.157, 'eval_samples_per_second': 63.701, 'eval_steps_per_second': 25.48, 'epoch': 0.42}\n",
      "{'loss': 3.1609, 'grad_norm': 1.6217540502548218, 'learning_rate': 0.00011994949494949495, 'epoch': 0.43}\n",
      "{'loss': 3.1513, 'grad_norm': 2.113149642944336, 'learning_rate': 0.00011742424242424244, 'epoch': 0.44}\n",
      "{'loss': 3.1672, 'grad_norm': 2.1333582401275635, 'learning_rate': 0.00011489898989898991, 'epoch': 0.46}\n",
      "{'loss': 3.1935, 'grad_norm': 1.81485915184021, 'learning_rate': 0.00011237373737373738, 'epoch': 0.47}\n",
      "{'loss': 3.2588, 'grad_norm': 2.0584566593170166, 'learning_rate': 0.00010984848484848484, 'epoch': 0.48}\n",
      "{'eval_loss': 3.3559927940368652, 'eval_accuracy': 0.384251968503937, 'eval_runtime': 0.1608, 'eval_samples_per_second': 62.2, 'eval_steps_per_second': 24.88, 'epoch': 0.48}\n",
      "{'loss': 3.2679, 'grad_norm': 1.9262604713439941, 'learning_rate': 0.00010732323232323234, 'epoch': 0.49}\n",
      "{'loss': 3.1961, 'grad_norm': 1.9666885137557983, 'learning_rate': 0.0001047979797979798, 'epoch': 0.5}\n",
      "{'loss': 3.1495, 'grad_norm': 2.118630886077881, 'learning_rate': 0.00010227272727272727, 'epoch': 0.52}\n",
      "{'loss': 3.044, 'grad_norm': 1.8746839761734009, 'learning_rate': 9.974747474747475e-05, 'epoch': 0.53}\n",
      "{'loss': 3.2825, 'grad_norm': 2.0390076637268066, 'learning_rate': 9.722222222222223e-05, 'epoch': 0.54}\n",
      "{'eval_loss': 3.300185441970825, 'eval_accuracy': 0.3921259842519685, 'eval_runtime': 0.1571, 'eval_samples_per_second': 63.664, 'eval_steps_per_second': 25.466, 'epoch': 0.54}\n",
      "{'loss': 3.0995, 'grad_norm': 2.2249677181243896, 'learning_rate': 9.469696969696971e-05, 'epoch': 0.55}\n",
      "{'loss': 3.2026, 'grad_norm': 1.8961007595062256, 'learning_rate': 9.217171717171718e-05, 'epoch': 0.56}\n",
      "{'loss': 3.186, 'grad_norm': 2.1096153259277344, 'learning_rate': 8.964646464646466e-05, 'epoch': 0.58}\n",
      "{'loss': 3.1115, 'grad_norm': 2.050887107849121, 'learning_rate': 8.712121212121212e-05, 'epoch': 0.59}\n",
      "{'loss': 3.1994, 'grad_norm': 1.8868460655212402, 'learning_rate': 8.459595959595959e-05, 'epoch': 0.6}\n",
      "{'eval_loss': 3.2839744091033936, 'eval_accuracy': 0.3968503937007874, 'eval_runtime': 0.1558, 'eval_samples_per_second': 64.173, 'eval_steps_per_second': 25.669, 'epoch': 0.6}\n",
      "{'loss': 3.0978, 'grad_norm': 2.0557193756103516, 'learning_rate': 8.207070707070707e-05, 'epoch': 0.61}\n",
      "{'loss': 3.1468, 'grad_norm': 1.905272364616394, 'learning_rate': 7.954545454545455e-05, 'epoch': 0.62}\n",
      "{'loss': 3.3049, 'grad_norm': 1.8825229406356812, 'learning_rate': 7.702020202020203e-05, 'epoch': 0.64}\n",
      "{'loss': 3.2662, 'grad_norm': 2.2125887870788574, 'learning_rate': 7.44949494949495e-05, 'epoch': 0.65}\n",
      "{'loss': 3.2314, 'grad_norm': 1.9597405195236206, 'learning_rate': 7.196969696969698e-05, 'epoch': 0.66}\n",
      "{'eval_loss': 3.242342710494995, 'eval_accuracy': 0.3960629921259842, 'eval_runtime': 0.1654, 'eval_samples_per_second': 60.465, 'eval_steps_per_second': 24.186, 'epoch': 0.66}\n",
      "{'loss': 3.1076, 'grad_norm': 1.8464350700378418, 'learning_rate': 6.944444444444444e-05, 'epoch': 0.67}\n",
      "{'loss': 3.0406, 'grad_norm': 1.9244009256362915, 'learning_rate': 6.691919191919192e-05, 'epoch': 0.68}\n",
      "{'loss': 3.182, 'grad_norm': 1.87586510181427, 'learning_rate': 6.439393939393939e-05, 'epoch': 0.7}\n",
      "{'loss': 2.9222, 'grad_norm': 1.9609254598617554, 'learning_rate': 6.186868686868687e-05, 'epoch': 0.71}\n",
      "{'loss': 3.1368, 'grad_norm': 2.316594362258911, 'learning_rate': 5.9343434343434345e-05, 'epoch': 0.72}\n",
      "{'eval_loss': 3.2299835681915283, 'eval_accuracy': 0.4023622047244094, 'eval_runtime': 0.1532, 'eval_samples_per_second': 65.264, 'eval_steps_per_second': 26.105, 'epoch': 0.72}\n",
      "{'loss': 3.041, 'grad_norm': 2.101675033569336, 'learning_rate': 5.6818181818181825e-05, 'epoch': 0.73}\n",
      "{'loss': 3.0596, 'grad_norm': 2.223268985748291, 'learning_rate': 5.42929292929293e-05, 'epoch': 0.74}\n",
      "{'loss': 3.1537, 'grad_norm': 1.814854621887207, 'learning_rate': 5.1767676767676765e-05, 'epoch': 0.76}\n",
      "{'loss': 3.0396, 'grad_norm': 1.948578119277954, 'learning_rate': 4.9242424242424245e-05, 'epoch': 0.77}\n",
      "{'loss': 3.0897, 'grad_norm': 1.9285523891448975, 'learning_rate': 4.671717171717172e-05, 'epoch': 0.78}\n",
      "{'eval_loss': 3.198770523071289, 'eval_accuracy': 0.4047244094488189, 'eval_runtime': 0.1531, 'eval_samples_per_second': 65.301, 'eval_steps_per_second': 26.12, 'epoch': 0.78}\n",
      "{'loss': 3.1289, 'grad_norm': 2.1421258449554443, 'learning_rate': 4.41919191919192e-05, 'epoch': 0.79}\n",
      "{'loss': 3.028, 'grad_norm': 1.9835450649261475, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.8}\n",
      "{'loss': 3.1141, 'grad_norm': 2.287881374359131, 'learning_rate': 3.9141414141414145e-05, 'epoch': 0.82}\n",
      "{'loss': 3.1345, 'grad_norm': 1.7949906587600708, 'learning_rate': 3.661616161616162e-05, 'epoch': 0.83}\n",
      "{'loss': 3.041, 'grad_norm': 1.8974567651748657, 'learning_rate': 3.409090909090909e-05, 'epoch': 0.84}\n",
      "{'eval_loss': 3.2007508277893066, 'eval_accuracy': 0.40551181102362205, 'eval_runtime': 0.1532, 'eval_samples_per_second': 65.281, 'eval_steps_per_second': 26.112, 'epoch': 0.84}\n",
      "{'loss': 3.0568, 'grad_norm': 2.054835796356201, 'learning_rate': 3.1565656565656566e-05, 'epoch': 0.85}\n",
      "{'loss': 3.1405, 'grad_norm': 2.2064692974090576, 'learning_rate': 2.904040404040404e-05, 'epoch': 0.86}\n",
      "{'loss': 3.132, 'grad_norm': 2.076385498046875, 'learning_rate': 2.6515151515151516e-05, 'epoch': 0.88}\n",
      "{'loss': 2.9982, 'grad_norm': 1.839121699333191, 'learning_rate': 2.398989898989899e-05, 'epoch': 0.89}\n",
      "{'loss': 3.2636, 'grad_norm': 2.202150344848633, 'learning_rate': 2.1464646464646466e-05, 'epoch': 0.9}\n",
      "{'eval_loss': 3.1858108043670654, 'eval_accuracy': 0.40551181102362205, 'eval_runtime': 0.1576, 'eval_samples_per_second': 63.459, 'eval_steps_per_second': 25.384, 'epoch': 0.9}\n",
      "{'loss': 3.1939, 'grad_norm': 1.9389069080352783, 'learning_rate': 1.893939393939394e-05, 'epoch': 0.91}\n",
      "{'loss': 3.0406, 'grad_norm': 2.227975368499756, 'learning_rate': 1.6414141414141416e-05, 'epoch': 0.92}\n",
      "{'loss': 2.8362, 'grad_norm': 2.0004374980926514, 'learning_rate': 1.388888888888889e-05, 'epoch': 0.94}\n",
      "{'loss': 3.1806, 'grad_norm': 1.9509416818618774, 'learning_rate': 1.1363636363636365e-05, 'epoch': 0.95}\n",
      "{'loss': 3.2147, 'grad_norm': 1.967081069946289, 'learning_rate': 8.838383838383838e-06, 'epoch': 0.96}\n",
      "{'eval_loss': 3.1859333515167236, 'eval_accuracy': 0.4062992125984252, 'eval_runtime': 0.1551, 'eval_samples_per_second': 64.467, 'eval_steps_per_second': 25.787, 'epoch': 0.96}\n",
      "{'loss': 3.1739, 'grad_norm': 1.7348896265029907, 'learning_rate': 6.313131313131314e-06, 'epoch': 0.97}\n",
      "{'loss': 3.1499, 'grad_norm': 1.9933961629867554, 'learning_rate': 3.7878787878787882e-06, 'epoch': 0.98}\n",
      "{'loss': 3.1593, 'grad_norm': 1.830169677734375, 'learning_rate': 1.2626262626262627e-06, 'epoch': 1.0}\n",
      "{'train_runtime': 141.8365, 'train_samples_per_second': 17.64, 'train_steps_per_second': 5.88, 'train_loss': 3.2034788028799372, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     3.2035\n",
      "  train_runtime            = 0:02:21.83\n",
      "  train_samples            =       2502\n",
      "  train_samples_per_second =      17.64\n",
      "  train_steps_per_second   =       5.88\n",
      "***** eval metrics *****\n",
      "  epoch                   =        1.0\n",
      "  eval_accuracy           =     0.4055\n",
      "  eval_loss               =     3.1834\n",
      "  eval_runtime            = 0:00:00.16\n",
      "  eval_samples            =         10\n",
      "  eval_samples_per_second =     59.594\n",
      "  eval_steps_per_second   =     23.837\n",
      "  perplexity              =    24.1294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 20:26:01.779863: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-21 20:26:02.189344: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[32m2024-08-21 20:26:02.881\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m377\u001b[0m - \u001b[1mModel args: ModelArguments(model_type='auto', model_name_or_path='Qwen1.5-1.8B-Chat', tokenizer_name_or_path=None, load_in_8bit=False, load_in_4bit=False, cache_dir=None, model_revision='main', hf_hub_token=None, use_fast_tokenizer=False, torch_dtype='float16', device_map='auto', trust_remote_code=True)\u001b[0m\n",
      "\u001b[32m2024-08-21 20:26:02.881\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m378\u001b[0m - \u001b[1mData args: DataArguments(dataset_name=None, dataset_config_name=None, train_file_dir='data\\\\pretrain', validation_file_dir='data\\\\pretrain', max_train_samples=20000, max_eval_samples=10, streaming=False, block_size=128, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=1, keep_linebreaks=True)\u001b[0m\n",
      "\u001b[32m2024-08-21 20:26:02.881\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m379\u001b[0m - \u001b[1mTraining args: Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=30000,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=50,\n",
      "evaluation_strategy=steps,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=True,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0002,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=outputs-pt-v1\\runs\\Aug21_20-26-02_dpg,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=outputs-pt-v1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=3,\n",
      "per_device_train_batch_size=3,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=outputs-pt-v1,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=3,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      ")\u001b[0m\n",
      "\u001b[32m2024-08-21 20:26:02.881\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m380\u001b[0m - \u001b[1mScript args: ScriptArguments(use_peft=True, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, qlora=False)\u001b[0m\n",
      "\u001b[32m2024-08-21 20:26:02.881\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m381\u001b[0m - \u001b[1mProcess rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: True\u001b[0m\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m2024-08-21 20:26:03.036\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m491\u001b[0m - \u001b[1mtrain files: ['data\\\\pretrain\\\\en_article_tail500.txt', 'data\\\\pretrain\\\\fever.txt', 'data\\\\pretrain\\\\tianlongbabu.txt']\u001b[0m\n",
      "\u001b[32m2024-08-21 20:26:03.036\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m501\u001b[0m - \u001b[1meval files: ['data\\\\pretrain\\\\en_article_tail500.txt', 'data\\\\pretrain\\\\fever.txt', 'data\\\\pretrain\\\\tianlongbabu.txt']\u001b[0m\n",
      "\u001b[32m2024-08-21 20:26:03.359\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m533\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3876\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3876\n",
      "    })\n",
      "})\u001b[0m\n",
      "\n",
      "Running tokenizer on dataset:   0%|          | 0/3876 [00:00<?, ? examples/s]\n",
      "Running tokenizer on dataset:  26%|██▌       | 1000/3876 [00:01<00:03, 926.58 examples/s]\n",
      "Running tokenizer on dataset:  52%|█████▏    | 2000/3876 [00:02<00:02, 719.72 examples/s]\n",
      "Running tokenizer on dataset:  77%|███████▋  | 3000/3876 [00:04<00:01, 718.83 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 3876/3876 [00:05<00:00, 691.72 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 3876/3876 [00:05<00:00, 713.03 examples/s]\n",
      "\n",
      "Running tokenizer on dataset:   0%|          | 0/3876 [00:00<?, ? examples/s]\n",
      "Running tokenizer on dataset:  52%|█████▏    | 2000/3876 [00:00<00:00, 10633.58 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 3876/3876 [00:00<00:00, 9319.26 examples/s] \n",
      "Running tokenizer on dataset: 100%|██████████| 3876/3876 [00:00<00:00, 9351.81 examples/s]\n",
      "\n",
      "Grouping texts in chunks of 128:   0%|          | 0/3876 [00:00<?, ? examples/s]\n",
      "Grouping texts in chunks of 128:  77%|███████▋  | 3000/3876 [00:00<00:00, 21587.93 examples/s]\n",
      "Grouping texts in chunks of 128: 100%|██████████| 3876/3876 [00:00<00:00, 19659.68 examples/s]\n",
      "\n",
      "Grouping texts in chunks of 128:   0%|          | 0/3876 [00:00<?, ? examples/s]\n",
      "Grouping texts in chunks of 128:  77%|███████▋  | 3000/3876 [00:00<00:00, 21498.59 examples/s]\n",
      "Grouping texts in chunks of 128: 100%|██████████| 3876/3876 [00:00<00:00, 19411.65 examples/s]\n",
      "\u001b[32m2024-08-21 20:26:12.620\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m596\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 2502\u001b[0m\n",
      "\u001b[32m2024-08-21 20:26:12.620\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m597\u001b[0m - \u001b[34m\u001b[1mTokenized training example:\u001b[0m\n",
      "\u001b[32m2024-08-21 20:26:12.621\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m598\u001b[0m - \u001b[34m\u001b[1mcontract to work in specified mines and mills. There seemed to be no\n",
      "limit to the factories, forges, refineries, and railways that could be\n",
      "built, to the multitudes that could be employed in conquering a\n",
      "continent. As for the future, that was in the hands of Providence!\n",
      "\n",
      "=Business Theories of Politics.=--As the statesmen of Hamilton's school\n",
      "and the planters of Calhoun's had their theories of government and\n",
      "politics, so the leaders in business enterprise had theirs. It was\n",
      "simple and easily stated. \"It is the duty of the government,\" they\n",
      "ur\u001b[0m\n",
      "\u001b[32m2024-08-21 20:26:12.622\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m610\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
      "\u001b[32m2024-08-21 20:26:12.622\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m611\u001b[0m - \u001b[34m\u001b[1mTokenized eval example:\u001b[0m\n",
      "\u001b[32m2024-08-21 20:26:12.622\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m612\u001b[0m - \u001b[34m\u001b[1mcontract to work in specified mines and mills. There seemed to be no\n",
      "limit to the factories, forges, refineries, and railways that could be\n",
      "built, to the multitudes that could be employed in conquering a\n",
      "continent. As for the future, that was in the hands of Providence!\n",
      "\n",
      "=Business Theories of Politics.=--As the statesmen of Hamilton's school\n",
      "and the planters of Calhoun's had their theories of government and\n",
      "politics, so the leaders in business enterprise had theirs. It was\n",
      "simple and easily stated. \"It is the duty of the government,\" they\n",
      "ur\u001b[0m\n",
      "\u001b[32m2024-08-21 20:26:15.131\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m671\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
      "\u001b[32m2024-08-21 20:26:15.131\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m676\u001b[0m - \u001b[1mInit new peft model\u001b[0m\n",
      "\u001b[32m2024-08-21 20:26:15.131\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m689\u001b[0m - \u001b[1mPeft target_modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[0m\n",
      "\u001b[32m2024-08-21 20:26:15.131\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m690\u001b[0m - \u001b[1mPeft lora_rank: 8\u001b[0m\n",
      "d:\\anaconda\\envs\\py39\\lib\\site-packages\\transformers\\utils\\import_utils.py:519: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-08-21 20:26:15.517\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m735\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
      "\u001b[32m2024-08-21 20:26:15.780\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m736\u001b[0m - \u001b[34m\u001b[1mTrain dataloader example: {'input_ids': tensor([[ 99395,  99316,  68065,  49567,   1773, 100270,  13343,  18493,  31843,\n",
      "         100811,  16872, 101041,  81433,  99389,   9909, 102890,  99619, 100506,\n",
      "           3837,  38212, 100898,  81433,  99389,   3837, 107833, 111328, 100067,\n",
      "          49567,  74276, 101364, 111328,  64689, 100297, 105897, 100771, 107429,\n",
      "          97706, 107769, 107174,  81217, 104160, 101899,   1773, 107429, 110305,\n",
      "          13343,  50511, 105109, 101924, 104432, 104405,  90395,  89012, 100316,\n",
      "           8997, 100016,  28291, 101304,   5122, 114380, 101364, 100439, 101924,\n",
      "         118670, 106080, 101304,   3837, 100645, 101046, 105110,   3837, 100470,\n",
      "         105202, 105604,   1773, 101304,  42140,  99726,  34204, 101335,  44793,\n",
      "           3837, 101317, 102504,   3837, 115610,   3837, 101690, 102395,  44793,\n",
      "          49567,   1773, 104110, 100347,   3837, 100345, 107368, 101112,  59151,\n",
      "          81217, 104595, 100034,  50404, 115340,   1773, 101335,  38176,  81217,\n",
      "         101317, 101232, 101304,  23031,  99575,  99533, 100205,  33071, 118039,\n",
      "          42140,  88970,   3837,  30440, 106423,  64355,  99963, 100517,  71138,\n",
      "          21515,  57191],\n",
      "        [  3837,  94443,  44934,  14777,  99933,   3837, 108468, 106677,  42411,\n",
      "          18947, 100229,  99225,   3837, 108233,  99786,  99754, 103301,  42411,\n",
      "         112188,   3837, 106006, 107856, 110197, 100560,  71618,   8997,  37474,\n",
      "          99783, 100929,  99164,  99283, 117746,   3837, 106052,  36987,  56568,\n",
      "          27733, 102933,   9370,  99364,  75437,  17340,   3837,  88051,  99521,\n",
      "         101209, 106156,   9370,  34187,  75758, 106116,  36987, 102903, 119516,\n",
      "          99315, 101036,  11319,  42411, 102972,  99817,  49567,  35946,  81596,\n",
      "          81264,  75405, 106783,  79766,  44793,  36987, 104389,  36667,  49567,\n",
      "          34187,  56568,  99612,   8903,  99612,  99530,   3837, 113364,  81596,\n",
      "          11319,  42411,  99314, 104060,  32945,  37474,  99783,  28641,  13343,\n",
      "          99315,  99433, 114831,   3837,  99370,  44793,  36987, 100279,  99226,\n",
      "           3837, 100279,  99226,   6313,  35946, 105872,  21287, 102142,   1773,\n",
      "          42411, 100155,  99927,  30534, 102328,  35946, 100090,  42411,  17714,\n",
      "          99235,   3837,  30440, 102085, 100007,  20412, 104334,  32945,    198,\n",
      "          75405, 106783],\n",
      "        [    20,     11,  10778,    315,  12066,    594,    198,     82,    843,\n",
      "            900,     11,    566,   1030,  79508,     25,    330,   3862,    374,\n",
      "           2494,  40692,    304,   3432,    264,    198,  11141,    311,   2948,\n",
      "           8958,  17695,   1635,   2937,     11,    979,   4588,    311,   3270,\n",
      "            458,  72894,    369,    279,    198,   1168,  31231,    518,  19335,\n",
      "            304,    220,     16,     23,     22,     21,     11,    566,   1410,\n",
      "           1744,   1172,    315,    264,  67096,    198,  36468,    554,    389,\n",
      "            279,   6995,    510,    198,    262,    330,   7812,    697,   1584,\n",
      "          14963,   2789,     26,   1473,    697,  46280,    280,    257,   1597,\n",
      "           8645,   4505,    311,   8193,   1741,   2513,    198,    257,   1634,\n",
      "           1550,   7359,  11699,   4279,    304,  13929,    198,    257,   2014,\n",
      "           4332,    279,  87702,    323,   5046,   2513,   1290,    624,    257,\n",
      "           1416,    429,   1513,    944,   7807,   1059,     11,   3170,     11,\n",
      "            498,   1184,   1172,    198,    257,   2014,   1473,    697,   5535,\n",
      "           1707,    304]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'labels': tensor([[ 99395,  99316,  68065,  49567,   1773, 100270,  13343,  18493,  31843,\n",
      "         100811,  16872, 101041,  81433,  99389,   9909, 102890,  99619, 100506,\n",
      "           3837,  38212, 100898,  81433,  99389,   3837, 107833, 111328, 100067,\n",
      "          49567,  74276, 101364, 111328,  64689, 100297, 105897, 100771, 107429,\n",
      "          97706, 107769, 107174,  81217, 104160, 101899,   1773, 107429, 110305,\n",
      "          13343,  50511, 105109, 101924, 104432, 104405,  90395,  89012, 100316,\n",
      "           8997, 100016,  28291, 101304,   5122, 114380, 101364, 100439, 101924,\n",
      "         118670, 106080, 101304,   3837, 100645, 101046, 105110,   3837, 100470,\n",
      "         105202, 105604,   1773, 101304,  42140,  99726,  34204, 101335,  44793,\n",
      "           3837, 101317, 102504,   3837, 115610,   3837, 101690, 102395,  44793,\n",
      "          49567,   1773, 104110, 100347,   3837, 100345, 107368, 101112,  59151,\n",
      "          81217, 104595, 100034,  50404, 115340,   1773, 101335,  38176,  81217,\n",
      "         101317, 101232, 101304,  23031,  99575,  99533, 100205,  33071, 118039,\n",
      "          42140,  88970,   3837,  30440, 106423,  64355,  99963, 100517,  71138,\n",
      "          21515,  57191],\n",
      "        [  3837,  94443,  44934,  14777,  99933,   3837, 108468, 106677,  42411,\n",
      "          18947, 100229,  99225,   3837, 108233,  99786,  99754, 103301,  42411,\n",
      "         112188,   3837, 106006, 107856, 110197, 100560,  71618,   8997,  37474,\n",
      "          99783, 100929,  99164,  99283, 117746,   3837, 106052,  36987,  56568,\n",
      "          27733, 102933,   9370,  99364,  75437,  17340,   3837,  88051,  99521,\n",
      "         101209, 106156,   9370,  34187,  75758, 106116,  36987, 102903, 119516,\n",
      "          99315, 101036,  11319,  42411, 102972,  99817,  49567,  35946,  81596,\n",
      "          81264,  75405, 106783,  79766,  44793,  36987, 104389,  36667,  49567,\n",
      "          34187,  56568,  99612,   8903,  99612,  99530,   3837, 113364,  81596,\n",
      "          11319,  42411,  99314, 104060,  32945,  37474,  99783,  28641,  13343,\n",
      "          99315,  99433, 114831,   3837,  99370,  44793,  36987, 100279,  99226,\n",
      "           3837, 100279,  99226,   6313,  35946, 105872,  21287, 102142,   1773,\n",
      "          42411, 100155,  99927,  30534, 102328,  35946, 100090,  42411,  17714,\n",
      "          99235,   3837,  30440, 102085, 100007,  20412, 104334,  32945,    198,\n",
      "          75405, 106783],\n",
      "        [    20,     11,  10778,    315,  12066,    594,    198,     82,    843,\n",
      "            900,     11,    566,   1030,  79508,     25,    330,   3862,    374,\n",
      "           2494,  40692,    304,   3432,    264,    198,  11141,    311,   2948,\n",
      "           8958,  17695,   1635,   2937,     11,    979,   4588,    311,   3270,\n",
      "            458,  72894,    369,    279,    198,   1168,  31231,    518,  19335,\n",
      "            304,    220,     16,     23,     22,     21,     11,    566,   1410,\n",
      "           1744,   1172,    315,    264,  67096,    198,  36468,    554,    389,\n",
      "            279,   6995,    510,    198,    262,    330,   7812,    697,   1584,\n",
      "          14963,   2789,     26,   1473,    697,  46280,    280,    257,   1597,\n",
      "           8645,   4505,    311,   8193,   1741,   2513,    198,    257,   1634,\n",
      "           1550,   7359,  11699,   4279,    304,  13929,    198,    257,   2014,\n",
      "           4332,    279,  87702,    323,   5046,   2513,   1290,    624,    257,\n",
      "           1416,    429,   1513,    944,   7807,   1059,     11,   3170,     11,\n",
      "            498,   1184,   1172,    198,    257,   2014,   1473,    697,   5535,\n",
      "           1707,    304]], device='cuda:0')}\u001b[0m\n",
      "\n",
      "  0%|          | 0/834 [00:00<?, ?it/s]d:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\py39\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:698: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "\n",
      "  0%|          | 1/834 [00:00<08:50,  1.57it/s]\n",
      "                                               \n",
      "\n",
      "  0%|          | 1/834 [00:00<08:50,  1.57it/s]\n",
      "  0%|          | 2/834 [00:00<04:58,  2.79it/s]\n",
      "  0%|          | 3/834 [00:00<03:46,  3.67it/s]\n",
      "  0%|          | 4/834 [00:01<03:10,  4.35it/s]\n",
      "  1%|          | 5/834 [00:01<02:54,  4.76it/s]\n",
      "  1%|          | 6/834 [00:01<02:41,  5.13it/s]\n",
      "  1%|          | 7/834 [00:01<02:34,  5.35it/s]\n",
      "  1%|          | 8/834 [00:01<02:28,  5.58it/s]\n",
      "  1%|          | 9/834 [00:01<02:24,  5.71it/s]\n",
      "  1%|          | 10/834 [00:02<02:25,  5.68it/s]\n",
      "                                                \n",
      "\n",
      "  1%|          | 10/834 [00:02<02:25,  5.68it/s]\n",
      "  1%|▏         | 11/834 [00:02<02:21,  5.81it/s]\n",
      "  1%|▏         | 12/834 [00:02<02:18,  5.93it/s]\n",
      "  2%|▏         | 13/834 [00:02<02:18,  5.93it/s]\n",
      "  2%|▏         | 14/834 [00:02<02:15,  6.04it/s]\n",
      "  2%|▏         | 15/834 [00:02<02:15,  6.06it/s]\n",
      "  2%|▏         | 16/834 [00:03<02:15,  6.04it/s]\n",
      "  2%|▏         | 17/834 [00:03<02:13,  6.11it/s]\n",
      "  2%|▏         | 18/834 [00:03<02:15,  6.01it/s]\n",
      "  2%|▏         | 19/834 [00:03<02:15,  6.03it/s]\n",
      "  2%|▏         | 20/834 [00:03<02:19,  5.85it/s]\n",
      "                                                \n",
      "\n",
      "  2%|▏         | 20/834 [00:04<02:19,  5.85it/s]\n",
      "  3%|▎         | 21/834 [00:04<04:03,  3.34it/s]\n",
      "  3%|▎         | 22/834 [00:04<03:28,  3.89it/s]\n",
      "  3%|▎         | 23/834 [00:04<03:04,  4.39it/s]\n",
      "  3%|▎         | 24/834 [00:04<02:50,  4.76it/s]\n",
      "  3%|▎         | 25/834 [00:05<02:40,  5.04it/s]\n",
      "  3%|▎         | 26/834 [00:05<02:31,  5.35it/s]\n",
      "  3%|▎         | 27/834 [00:05<02:31,  5.34it/s]\n",
      "  3%|▎         | 28/834 [00:05<02:25,  5.54it/s]\n",
      "  3%|▎         | 29/834 [00:05<02:22,  5.66it/s]\n",
      "  4%|▎         | 30/834 [00:05<02:19,  5.77it/s]\n",
      "                                                \n",
      "\n",
      "  4%|▎         | 30/834 [00:05<02:19,  5.77it/s]\n",
      "  4%|▎         | 31/834 [00:06<02:19,  5.76it/s]\n",
      "  4%|▍         | 32/834 [00:06<02:18,  5.78it/s]\n",
      "  4%|▍         | 33/834 [00:06<02:17,  5.85it/s]\n",
      "  4%|▍         | 34/834 [00:06<02:16,  5.86it/s]\n",
      "  4%|▍         | 35/834 [00:06<02:13,  5.97it/s]\n",
      "  4%|▍         | 36/834 [00:06<02:13,  5.97it/s]\n",
      "  4%|▍         | 37/834 [00:07<02:11,  6.06it/s]\n",
      "  5%|▍         | 38/834 [00:07<02:12,  5.99it/s]\n",
      "  5%|▍         | 39/834 [00:07<02:16,  5.83it/s]\n",
      "  5%|▍         | 40/834 [00:07<02:12,  5.98it/s]\n",
      "                                                \n",
      "\n",
      "  5%|▍         | 40/834 [00:07<02:12,  5.98it/s]\n",
      "  5%|▍         | 41/834 [00:07<02:12,  5.98it/s]\n",
      "  5%|▌         | 42/834 [00:07<02:14,  5.88it/s]\n",
      "  5%|▌         | 43/834 [00:08<02:13,  5.92it/s]\n",
      "  5%|▌         | 44/834 [00:08<02:15,  5.82it/s]\n",
      "  5%|▌         | 45/834 [00:08<02:15,  5.80it/s]\n",
      "  6%|▌         | 46/834 [00:08<02:14,  5.85it/s]\n",
      "  6%|▌         | 47/834 [00:08<02:12,  5.93it/s]\n",
      "  6%|▌         | 48/834 [00:08<02:11,  6.00it/s]\n",
      "  6%|▌         | 49/834 [00:09<02:11,  5.99it/s]\n",
      "  6%|▌         | 50/834 [00:09<02:12,  5.91it/s]\n",
      "                                                \n",
      "\n",
      "  6%|▌         | 50/834 [00:09<02:12,  5.91it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 32.70it/s]\u001b[A\n",
      "                                                \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      "  6%|▌         | 50/834 [00:09<02:12,  5.91it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 32.70it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      "  6%|▌         | 51/834 [00:09<02:54,  4.48it/s]\n",
      "  6%|▌         | 52/834 [00:09<02:43,  4.78it/s]\n",
      "  6%|▋         | 53/834 [00:10<02:36,  4.99it/s]\n",
      "  6%|▋         | 54/834 [00:10<02:26,  5.32it/s]\n",
      "  7%|▋         | 55/834 [00:10<02:18,  5.61it/s]\n",
      "  7%|▋         | 56/834 [00:10<02:14,  5.78it/s]\n",
      "  7%|▋         | 57/834 [00:10<02:12,  5.87it/s]\n",
      "  7%|▋         | 58/834 [00:10<02:09,  5.99it/s]\n",
      "  7%|▋         | 59/834 [00:10<02:07,  6.06it/s]\n",
      "  7%|▋         | 60/834 [00:11<02:07,  6.06it/s]\n",
      "                                                \n",
      "\n",
      "  7%|▋         | 60/834 [00:11<02:07,  6.06it/s]\n",
      "  7%|▋         | 61/834 [00:11<02:06,  6.10it/s]\n",
      "  7%|▋         | 62/834 [00:11<02:05,  6.18it/s]\n",
      "  8%|▊         | 63/834 [00:11<02:04,  6.21it/s]\n",
      "  8%|▊         | 64/834 [00:11<02:03,  6.21it/s]\n",
      "  8%|▊         | 65/834 [00:11<02:06,  6.06it/s]\n",
      "  8%|▊         | 66/834 [00:12<02:05,  6.12it/s]\n",
      "  8%|▊         | 67/834 [00:12<02:05,  6.11it/s]\n",
      "  8%|▊         | 68/834 [00:12<02:06,  6.07it/s]\n",
      "  8%|▊         | 69/834 [00:12<02:04,  6.14it/s]\n",
      "  8%|▊         | 70/834 [00:12<02:03,  6.18it/s]\n",
      "                                                \n",
      "\n",
      "  8%|▊         | 70/834 [00:12<02:03,  6.18it/s]\n",
      "  9%|▊         | 71/834 [00:12<02:08,  5.94it/s]\n",
      "  9%|▊         | 72/834 [00:13<02:07,  5.99it/s]\n",
      "  9%|▉         | 73/834 [00:13<02:07,  5.97it/s]\n",
      "  9%|▉         | 74/834 [00:13<02:09,  5.86it/s]\n",
      "  9%|▉         | 75/834 [00:13<02:09,  5.85it/s]\n",
      "  9%|▉         | 76/834 [00:13<02:06,  6.00it/s]\n",
      "  9%|▉         | 77/834 [00:13<02:04,  6.07it/s]\n",
      "  9%|▉         | 78/834 [00:14<02:03,  6.11it/s]\n",
      "  9%|▉         | 79/834 [00:14<02:01,  6.22it/s]\n",
      " 10%|▉         | 80/834 [00:14<02:04,  6.08it/s]\n",
      "                                                \n",
      "\n",
      " 10%|▉         | 80/834 [00:14<02:04,  6.08it/s]\n",
      " 10%|▉         | 81/834 [00:14<02:08,  5.87it/s]\n",
      " 10%|▉         | 82/834 [00:14<02:05,  5.99it/s]\n",
      " 10%|▉         | 83/834 [00:14<02:06,  5.93it/s]\n",
      " 10%|█         | 84/834 [00:15<02:06,  5.95it/s]\n",
      " 10%|█         | 85/834 [00:15<02:04,  5.99it/s]\n",
      " 10%|█         | 86/834 [00:15<02:06,  5.93it/s]\n",
      " 10%|█         | 87/834 [00:15<02:06,  5.92it/s]\n",
      " 11%|█         | 88/834 [00:15<02:06,  5.90it/s]\n",
      " 11%|█         | 89/834 [00:15<02:09,  5.75it/s]\n",
      " 11%|█         | 90/834 [00:16<02:09,  5.75it/s]\n",
      "                                                \n",
      "\n",
      " 11%|█         | 90/834 [00:16<02:09,  5.75it/s]\n",
      " 11%|█         | 91/834 [00:16<02:11,  5.67it/s]\n",
      " 11%|█         | 92/834 [00:16<02:09,  5.75it/s]\n",
      " 11%|█         | 93/834 [00:16<02:08,  5.78it/s]\n",
      " 11%|█▏        | 94/834 [00:16<02:08,  5.77it/s]\n",
      " 11%|█▏        | 95/834 [00:17<02:08,  5.73it/s]\n",
      " 12%|█▏        | 96/834 [00:17<02:07,  5.78it/s]\n",
      " 12%|█▏        | 97/834 [00:17<02:08,  5.72it/s]\n",
      " 12%|█▏        | 98/834 [00:17<02:07,  5.76it/s]\n",
      " 12%|█▏        | 99/834 [00:17<02:07,  5.75it/s]\n",
      " 12%|█▏        | 100/834 [00:17<02:07,  5.77it/s]\n",
      "                                                 \n",
      "\n",
      " 12%|█▏        | 100/834 [00:17<02:07,  5.77it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 34.43it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 12%|█▏        | 100/834 [00:18<02:07,  5.77it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 34.43it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 12%|█▏        | 101/834 [00:18<02:44,  4.46it/s]\n",
      " 12%|█▏        | 102/834 [00:18<02:29,  4.88it/s]\n",
      " 12%|█▏        | 103/834 [00:18<02:21,  5.17it/s]\n",
      " 12%|█▏        | 104/834 [00:18<02:13,  5.48it/s]\n",
      " 13%|█▎        | 105/834 [00:18<02:09,  5.62it/s]\n",
      " 13%|█▎        | 106/834 [00:19<02:06,  5.76it/s]\n",
      " 13%|█▎        | 107/834 [00:19<02:04,  5.83it/s]\n",
      " 13%|█▎        | 108/834 [00:19<02:04,  5.81it/s]\n",
      " 13%|█▎        | 109/834 [00:19<02:04,  5.83it/s]\n",
      " 13%|█▎        | 110/834 [00:19<02:02,  5.92it/s]\n",
      "                                                 \n",
      "\n",
      " 13%|█▎        | 110/834 [00:19<02:02,  5.92it/s]\n",
      " 13%|█▎        | 111/834 [00:19<02:04,  5.82it/s]\n",
      " 13%|█▎        | 112/834 [00:20<02:03,  5.84it/s]\n",
      " 14%|█▎        | 113/834 [00:20<02:02,  5.91it/s]\n",
      " 14%|█▎        | 114/834 [00:20<02:00,  5.97it/s]\n",
      " 14%|█▍        | 115/834 [00:20<01:57,  6.10it/s]\n",
      " 14%|█▍        | 116/834 [00:20<01:57,  6.09it/s]\n",
      " 14%|█▍        | 117/834 [00:20<01:56,  6.17it/s]\n",
      " 14%|█▍        | 118/834 [00:21<01:58,  6.06it/s]\n",
      " 14%|█▍        | 119/834 [00:21<01:59,  6.00it/s]\n",
      " 14%|█▍        | 120/834 [00:21<02:01,  5.89it/s]\n",
      "                                                 \n",
      "\n",
      " 14%|█▍        | 120/834 [00:21<02:01,  5.89it/s]\n",
      " 15%|█▍        | 121/834 [00:21<02:04,  5.75it/s]\n",
      " 15%|█▍        | 122/834 [00:21<02:04,  5.70it/s]\n",
      " 15%|█▍        | 123/834 [00:21<02:03,  5.78it/s]\n",
      " 15%|█▍        | 124/834 [00:22<02:00,  5.88it/s]\n",
      " 15%|█▍        | 125/834 [00:22<01:58,  5.98it/s]\n",
      " 15%|█▌        | 126/834 [00:22<02:00,  5.86it/s]\n",
      " 15%|█▌        | 127/834 [00:22<01:58,  5.96it/s]\n",
      " 15%|█▌        | 128/834 [00:22<01:57,  6.00it/s]\n",
      " 15%|█▌        | 129/834 [00:22<01:56,  6.03it/s]\n",
      " 16%|█▌        | 130/834 [00:23<01:56,  6.03it/s]\n",
      "                                                 \n",
      "\n",
      " 16%|█▌        | 130/834 [00:23<01:56,  6.03it/s]\n",
      " 16%|█▌        | 131/834 [00:23<01:59,  5.87it/s]\n",
      " 16%|█▌        | 132/834 [00:23<01:58,  5.95it/s]\n",
      " 16%|█▌        | 133/834 [00:23<01:58,  5.90it/s]\n",
      " 16%|█▌        | 134/834 [00:23<01:59,  5.87it/s]\n",
      " 16%|█▌        | 135/834 [00:23<01:58,  5.88it/s]\n",
      " 16%|█▋        | 136/834 [00:24<02:00,  5.78it/s]\n",
      " 16%|█▋        | 137/834 [00:24<01:59,  5.86it/s]\n",
      " 17%|█▋        | 138/834 [00:24<01:59,  5.81it/s]\n",
      " 17%|█▋        | 139/834 [00:24<01:57,  5.91it/s]\n",
      " 17%|█▋        | 140/834 [00:24<01:56,  5.96it/s]\n",
      "                                                 \n",
      "\n",
      " 17%|█▋        | 140/834 [00:24<01:56,  5.96it/s]\n",
      " 17%|█▋        | 141/834 [00:24<01:57,  5.91it/s]\n",
      " 17%|█▋        | 142/834 [00:25<01:59,  5.80it/s]\n",
      " 17%|█▋        | 143/834 [00:25<02:00,  5.75it/s]\n",
      " 17%|█▋        | 144/834 [00:25<01:59,  5.79it/s]\n",
      " 17%|█▋        | 145/834 [00:25<01:56,  5.90it/s]\n",
      " 18%|█▊        | 146/834 [00:25<01:55,  5.98it/s]\n",
      " 18%|█▊        | 147/834 [00:25<01:53,  6.04it/s]\n",
      " 18%|█▊        | 148/834 [00:26<01:54,  6.00it/s]\n",
      " 18%|█▊        | 149/834 [00:26<01:52,  6.10it/s]\n",
      " 18%|█▊        | 150/834 [00:26<01:51,  6.16it/s]\n",
      "                                                 \n",
      "\n",
      " 18%|█▊        | 150/834 [00:26<01:51,  6.16it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 34.42it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 18%|█▊        | 150/834 [00:26<01:51,  6.16it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 34.42it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 18%|█▊        | 151/834 [00:26<02:28,  4.60it/s]\n",
      " 18%|█▊        | 152/834 [00:26<02:16,  5.01it/s]\n",
      " 18%|█▊        | 153/834 [00:27<02:08,  5.29it/s]\n",
      " 18%|█▊        | 154/834 [00:27<02:03,  5.50it/s]\n",
      " 19%|█▊        | 155/834 [00:27<01:58,  5.73it/s]\n",
      " 19%|█▊        | 156/834 [00:27<01:54,  5.91it/s]\n",
      " 19%|█▉        | 157/834 [00:27<01:52,  6.04it/s]\n",
      " 19%|█▉        | 158/834 [00:27<01:52,  5.99it/s]\n",
      " 19%|█▉        | 159/834 [00:28<01:50,  6.12it/s]\n",
      " 19%|█▉        | 160/834 [00:28<01:52,  6.00it/s]\n",
      "                                                 \n",
      "\n",
      " 19%|█▉        | 160/834 [00:28<01:52,  6.00it/s]\n",
      " 19%|█▉        | 161/834 [00:28<01:53,  5.94it/s]\n",
      " 19%|█▉        | 162/834 [00:28<01:53,  5.91it/s]\n",
      " 20%|█▉        | 163/834 [00:28<01:51,  6.02it/s]\n",
      " 20%|█▉        | 164/834 [00:28<01:49,  6.13it/s]\n",
      " 20%|█▉        | 165/834 [00:29<01:50,  6.03it/s]\n",
      " 20%|█▉        | 166/834 [00:29<01:49,  6.08it/s]\n",
      " 20%|██        | 167/834 [00:29<01:49,  6.09it/s]\n",
      " 20%|██        | 168/834 [00:29<01:49,  6.08it/s]\n",
      " 20%|██        | 169/834 [00:29<01:48,  6.15it/s]\n",
      " 20%|██        | 170/834 [00:29<01:46,  6.21it/s]\n",
      "                                                 \n",
      "\n",
      " 20%|██        | 170/834 [00:29<01:46,  6.21it/s]\n",
      " 21%|██        | 171/834 [00:30<01:48,  6.12it/s]\n",
      " 21%|██        | 172/834 [00:30<01:48,  6.12it/s]\n",
      " 21%|██        | 173/834 [00:30<01:47,  6.16it/s]\n",
      " 21%|██        | 174/834 [00:30<01:49,  6.05it/s]\n",
      " 21%|██        | 175/834 [00:30<01:47,  6.11it/s]\n",
      " 21%|██        | 176/834 [00:30<01:47,  6.10it/s]\n",
      " 21%|██        | 177/834 [00:31<01:49,  6.02it/s]\n",
      " 21%|██▏       | 178/834 [00:31<01:49,  5.99it/s]\n",
      " 21%|██▏       | 179/834 [00:31<01:47,  6.09it/s]\n",
      " 22%|██▏       | 180/834 [00:31<01:46,  6.15it/s]\n",
      "                                                 \n",
      "\n",
      " 22%|██▏       | 180/834 [00:31<01:46,  6.15it/s]\n",
      " 22%|██▏       | 181/834 [00:31<01:46,  6.11it/s]\n",
      " 22%|██▏       | 182/834 [00:31<01:44,  6.21it/s]\n",
      " 22%|██▏       | 183/834 [00:32<01:46,  6.11it/s]\n",
      " 22%|██▏       | 184/834 [00:32<01:49,  5.96it/s]\n",
      " 22%|██▏       | 185/834 [00:32<01:50,  5.88it/s]\n",
      " 22%|██▏       | 186/834 [00:32<01:50,  5.86it/s]\n",
      " 22%|██▏       | 187/834 [00:32<01:48,  5.97it/s]\n",
      " 23%|██▎       | 188/834 [00:32<01:47,  6.04it/s]\n",
      " 23%|██▎       | 189/834 [00:33<01:45,  6.09it/s]\n",
      " 23%|██▎       | 190/834 [00:33<01:45,  6.11it/s]\n",
      "                                                 \n",
      "\n",
      " 23%|██▎       | 190/834 [00:33<01:45,  6.11it/s]\n",
      " 23%|██▎       | 191/834 [00:33<01:46,  6.06it/s]\n",
      " 23%|██▎       | 192/834 [00:33<01:45,  6.06it/s]\n",
      " 23%|██▎       | 193/834 [00:33<01:46,  6.04it/s]\n",
      " 23%|██▎       | 194/834 [00:33<01:46,  5.99it/s]\n",
      " 23%|██▎       | 195/834 [00:34<01:47,  5.93it/s]\n",
      " 24%|██▎       | 196/834 [00:34<01:50,  5.79it/s]\n",
      " 24%|██▎       | 197/834 [00:34<01:48,  5.89it/s]\n",
      " 24%|██▎       | 198/834 [00:34<01:46,  5.97it/s]\n",
      " 24%|██▍       | 199/834 [00:34<01:48,  5.88it/s]\n",
      " 24%|██▍       | 200/834 [00:34<01:47,  5.92it/s]\n",
      "                                                 \n",
      "\n",
      " 24%|██▍       | 200/834 [00:34<01:47,  5.92it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 29.78it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 24%|██▍       | 200/834 [00:35<01:47,  5.92it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 29.78it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 24%|██▍       | 201/834 [00:35<02:25,  4.34it/s]\n",
      " 24%|██▍       | 202/834 [00:35<02:15,  4.65it/s]\n",
      " 24%|██▍       | 203/834 [00:35<02:08,  4.93it/s]\n",
      " 24%|██▍       | 204/834 [00:35<02:01,  5.19it/s]\n",
      " 25%|██▍       | 205/834 [00:35<01:55,  5.46it/s]\n",
      " 25%|██▍       | 206/834 [00:36<01:54,  5.47it/s]\n",
      " 25%|██▍       | 207/834 [00:36<01:52,  5.55it/s]\n",
      " 25%|██▍       | 208/834 [00:36<01:51,  5.61it/s]\n",
      " 25%|██▌       | 209/834 [00:36<01:47,  5.83it/s]\n",
      " 25%|██▌       | 210/834 [00:36<01:48,  5.77it/s]\n",
      "                                                 \n",
      "\n",
      " 25%|██▌       | 210/834 [00:36<01:48,  5.77it/s]\n",
      " 25%|██▌       | 211/834 [00:36<01:46,  5.85it/s]\n",
      " 25%|██▌       | 212/834 [00:37<01:47,  5.78it/s]\n",
      " 26%|██▌       | 213/834 [00:37<01:47,  5.77it/s]\n",
      " 26%|██▌       | 214/834 [00:37<01:47,  5.78it/s]\n",
      " 26%|██▌       | 215/834 [00:37<01:48,  5.69it/s]\n",
      " 26%|██▌       | 216/834 [00:37<01:46,  5.78it/s]\n",
      " 26%|██▌       | 217/834 [00:38<01:45,  5.82it/s]\n",
      " 26%|██▌       | 218/834 [00:38<01:45,  5.83it/s]\n",
      " 26%|██▋       | 219/834 [00:38<01:45,  5.84it/s]\n",
      " 26%|██▋       | 220/834 [00:38<01:45,  5.84it/s]\n",
      "                                                 \n",
      "\n",
      " 26%|██▋       | 220/834 [00:38<01:45,  5.84it/s]\n",
      " 26%|██▋       | 221/834 [00:38<01:48,  5.66it/s]\n",
      " 27%|██▋       | 222/834 [00:38<01:46,  5.73it/s]\n",
      " 27%|██▋       | 223/834 [00:39<01:48,  5.64it/s]\n",
      " 27%|██▋       | 224/834 [00:39<01:46,  5.74it/s]\n",
      " 27%|██▋       | 225/834 [00:39<01:44,  5.81it/s]\n",
      " 27%|██▋       | 226/834 [00:39<01:42,  5.91it/s]\n",
      " 27%|██▋       | 227/834 [00:39<01:42,  5.94it/s]\n",
      " 27%|██▋       | 228/834 [00:39<01:42,  5.93it/s]\n",
      " 27%|██▋       | 229/834 [00:40<01:42,  5.92it/s]\n",
      " 28%|██▊       | 230/834 [00:40<01:42,  5.88it/s]\n",
      "                                                 \n",
      "\n",
      " 28%|██▊       | 230/834 [00:40<01:42,  5.88it/s]\n",
      " 28%|██▊       | 231/834 [00:40<01:42,  5.87it/s]\n",
      " 28%|██▊       | 232/834 [00:40<01:42,  5.86it/s]\n",
      " 28%|██▊       | 233/834 [00:40<01:42,  5.89it/s]\n",
      " 28%|██▊       | 234/834 [00:40<01:41,  5.93it/s]\n",
      " 28%|██▊       | 235/834 [00:41<01:44,  5.73it/s]\n",
      " 28%|██▊       | 236/834 [00:41<01:43,  5.75it/s]\n",
      " 28%|██▊       | 237/834 [00:41<01:44,  5.70it/s]\n",
      " 29%|██▊       | 238/834 [00:41<01:43,  5.78it/s]\n",
      " 29%|██▊       | 239/834 [00:41<01:41,  5.86it/s]\n",
      " 29%|██▉       | 240/834 [00:41<01:38,  6.02it/s]\n",
      "                                                 \n",
      "\n",
      " 29%|██▉       | 240/834 [00:41<01:38,  6.02it/s]\n",
      " 29%|██▉       | 241/834 [00:42<01:41,  5.83it/s]\n",
      " 29%|██▉       | 242/834 [00:42<01:41,  5.83it/s]\n",
      " 29%|██▉       | 243/834 [00:42<01:39,  5.93it/s]\n",
      " 29%|██▉       | 244/834 [00:42<01:39,  5.94it/s]\n",
      " 29%|██▉       | 245/834 [00:42<01:38,  5.95it/s]\n",
      " 29%|██▉       | 246/834 [00:42<01:40,  5.85it/s]\n",
      " 30%|██▉       | 247/834 [00:43<01:39,  5.90it/s]\n",
      " 30%|██▉       | 248/834 [00:43<01:38,  5.97it/s]\n",
      " 30%|██▉       | 249/834 [00:43<01:41,  5.77it/s]\n",
      " 30%|██▉       | 250/834 [00:43<01:40,  5.80it/s]\n",
      "                                                 \n",
      "\n",
      " 30%|██▉       | 250/834 [00:43<01:40,  5.80it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 32.71it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 30%|██▉       | 250/834 [00:43<01:40,  5.80it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 32.71it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 30%|███       | 251/834 [00:44<02:10,  4.46it/s]\n",
      " 30%|███       | 252/834 [00:44<02:01,  4.81it/s]\n",
      " 30%|███       | 253/834 [00:44<01:52,  5.17it/s]\n",
      " 30%|███       | 254/834 [00:44<01:48,  5.36it/s]\n",
      " 31%|███       | 255/834 [00:44<01:45,  5.49it/s]\n",
      " 31%|███       | 256/834 [00:44<01:41,  5.67it/s]\n",
      " 31%|███       | 257/834 [00:44<01:38,  5.84it/s]\n",
      " 31%|███       | 258/834 [00:45<01:40,  5.75it/s]\n",
      " 31%|███       | 259/834 [00:45<01:39,  5.77it/s]\n",
      " 31%|███       | 260/834 [00:45<01:37,  5.88it/s]\n",
      "                                                 \n",
      "\n",
      " 31%|███       | 260/834 [00:45<01:37,  5.88it/s]\n",
      " 31%|███▏      | 261/834 [00:45<01:38,  5.80it/s]\n",
      " 31%|███▏      | 262/834 [00:45<01:36,  5.92it/s]\n",
      " 32%|███▏      | 263/834 [00:46<01:38,  5.81it/s]\n",
      " 32%|███▏      | 264/834 [00:46<01:38,  5.81it/s]\n",
      " 32%|███▏      | 265/834 [00:46<01:35,  5.93it/s]\n",
      " 32%|███▏      | 266/834 [00:46<01:34,  5.99it/s]\n",
      " 32%|███▏      | 267/834 [00:46<01:35,  5.97it/s]\n",
      " 32%|███▏      | 268/834 [00:46<01:33,  6.05it/s]\n",
      " 32%|███▏      | 269/834 [00:47<01:34,  6.00it/s]\n",
      " 32%|███▏      | 270/834 [00:47<01:33,  6.06it/s]\n",
      "                                                 \n",
      "\n",
      " 32%|███▏      | 270/834 [00:47<01:33,  6.06it/s]\n",
      " 32%|███▏      | 271/834 [00:47<01:37,  5.78it/s]\n",
      " 33%|███▎      | 272/834 [00:47<01:38,  5.68it/s]\n",
      " 33%|███▎      | 273/834 [00:47<01:38,  5.67it/s]\n",
      " 33%|███▎      | 274/834 [00:47<01:36,  5.79it/s]\n",
      " 33%|███▎      | 275/834 [00:48<01:33,  5.96it/s]\n",
      " 33%|███▎      | 276/834 [00:48<01:33,  5.98it/s]\n",
      " 33%|███▎      | 277/834 [00:48<01:34,  5.91it/s]\n",
      " 33%|███▎      | 278/834 [00:48<01:37,  5.73it/s]\n",
      " 33%|███▎      | 279/834 [00:48<01:35,  5.79it/s]\n",
      " 34%|███▎      | 280/834 [00:48<01:34,  5.86it/s]\n",
      "                                                 \n",
      "\n",
      " 34%|███▎      | 280/834 [00:48<01:34,  5.86it/s]\n",
      " 34%|███▎      | 281/834 [00:49<01:34,  5.85it/s]\n",
      " 34%|███▍      | 282/834 [00:49<01:31,  6.03it/s]\n",
      " 34%|███▍      | 283/834 [00:49<01:29,  6.16it/s]\n",
      " 34%|███▍      | 284/834 [00:49<01:29,  6.18it/s]\n",
      " 34%|███▍      | 285/834 [00:49<01:29,  6.11it/s]\n",
      " 34%|███▍      | 286/834 [00:49<01:34,  5.80it/s]\n",
      " 34%|███▍      | 287/834 [00:50<01:32,  5.93it/s]\n",
      " 35%|███▍      | 288/834 [00:50<01:32,  5.90it/s]\n",
      " 35%|███▍      | 289/834 [00:50<01:31,  5.95it/s]\n",
      " 35%|███▍      | 290/834 [00:50<01:31,  5.96it/s]\n",
      "                                                 \n",
      "\n",
      " 35%|███▍      | 290/834 [00:50<01:31,  5.96it/s]\n",
      " 35%|███▍      | 291/834 [00:50<01:35,  5.68it/s]\n",
      " 35%|███▌      | 292/834 [00:50<01:33,  5.81it/s]\n",
      " 35%|███▌      | 293/834 [00:51<01:33,  5.76it/s]\n",
      " 35%|███▌      | 294/834 [00:51<01:32,  5.83it/s]\n",
      " 35%|███▌      | 295/834 [00:51<01:30,  5.94it/s]\n",
      " 35%|███▌      | 296/834 [00:51<01:29,  6.02it/s]\n",
      " 36%|███▌      | 297/834 [00:51<01:29,  6.01it/s]\n",
      " 36%|███▌      | 298/834 [00:51<01:28,  6.07it/s]\n",
      " 36%|███▌      | 299/834 [00:52<01:29,  5.95it/s]\n",
      " 36%|███▌      | 300/834 [00:52<01:30,  5.89it/s]\n",
      "                                                 \n",
      "\n",
      " 36%|███▌      | 300/834 [00:52<01:30,  5.89it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 35.36it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 36%|███▌      | 300/834 [00:52<01:30,  5.89it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 35.36it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 36%|███▌      | 301/834 [00:52<01:57,  4.54it/s]\n",
      " 36%|███▌      | 302/834 [00:52<01:47,  4.95it/s]\n",
      " 36%|███▋      | 303/834 [00:52<01:40,  5.26it/s]\n",
      " 36%|███▋      | 304/834 [00:53<01:36,  5.51it/s]\n",
      " 37%|███▋      | 305/834 [00:53<01:32,  5.75it/s]\n",
      " 37%|███▋      | 306/834 [00:53<01:30,  5.82it/s]\n",
      " 37%|███▋      | 307/834 [00:53<01:29,  5.91it/s]\n",
      " 37%|███▋      | 308/834 [00:53<01:30,  5.82it/s]\n",
      " 37%|███▋      | 309/834 [00:53<01:30,  5.82it/s]\n",
      " 37%|███▋      | 310/834 [00:54<01:29,  5.87it/s]\n",
      "                                                 \n",
      "\n",
      " 37%|███▋      | 310/834 [00:54<01:29,  5.87it/s]\n",
      " 37%|███▋      | 311/834 [00:54<01:31,  5.73it/s]\n",
      " 37%|███▋      | 312/834 [00:54<01:29,  5.85it/s]\n",
      " 38%|███▊      | 313/834 [00:54<01:30,  5.75it/s]\n",
      " 38%|███▊      | 314/834 [00:54<01:29,  5.83it/s]\n",
      " 38%|███▊      | 315/834 [00:54<01:29,  5.83it/s]\n",
      " 38%|███▊      | 316/834 [00:55<01:26,  5.97it/s]\n",
      " 38%|███▊      | 317/834 [00:55<01:24,  6.10it/s]\n",
      " 38%|███▊      | 318/834 [00:55<01:24,  6.12it/s]\n",
      " 38%|███▊      | 319/834 [00:55<01:25,  6.04it/s]\n",
      " 38%|███▊      | 320/834 [00:55<01:26,  5.95it/s]\n",
      "                                                 \n",
      "\n",
      " 38%|███▊      | 320/834 [00:55<01:26,  5.95it/s]\n",
      " 38%|███▊      | 321/834 [00:55<01:26,  5.92it/s]\n",
      " 39%|███▊      | 322/834 [00:56<01:25,  5.96it/s]\n",
      " 39%|███▊      | 323/834 [00:56<01:28,  5.76it/s]\n",
      " 39%|███▉      | 324/834 [00:56<01:27,  5.84it/s]\n",
      " 39%|███▉      | 325/834 [00:56<01:24,  6.00it/s]\n",
      " 39%|███▉      | 326/834 [00:56<01:26,  5.90it/s]\n",
      " 39%|███▉      | 327/834 [00:56<01:26,  5.89it/s]\n",
      " 39%|███▉      | 328/834 [00:57<01:27,  5.78it/s]\n",
      " 39%|███▉      | 329/834 [00:57<01:26,  5.81it/s]\n",
      " 40%|███▉      | 330/834 [00:57<01:27,  5.79it/s]\n",
      "                                                 \n",
      "\n",
      " 40%|███▉      | 330/834 [00:57<01:27,  5.79it/s]\n",
      " 40%|███▉      | 331/834 [00:57<01:28,  5.69it/s]\n",
      " 40%|███▉      | 332/834 [00:57<01:25,  5.84it/s]\n",
      " 40%|███▉      | 333/834 [00:58<01:24,  5.94it/s]\n",
      " 40%|████      | 334/834 [00:58<01:23,  6.00it/s]\n",
      " 40%|████      | 335/834 [00:58<01:22,  6.02it/s]\n",
      " 40%|████      | 336/834 [00:58<01:22,  6.06it/s]\n",
      " 40%|████      | 337/834 [00:58<01:21,  6.07it/s]\n",
      " 41%|████      | 338/834 [00:58<01:23,  5.98it/s]\n",
      " 41%|████      | 339/834 [00:59<01:22,  6.02it/s]\n",
      " 41%|████      | 340/834 [00:59<01:22,  6.00it/s]\n",
      "                                                 \n",
      "\n",
      " 41%|████      | 340/834 [00:59<01:22,  6.00it/s]\n",
      " 41%|████      | 341/834 [00:59<01:32,  5.36it/s]\n",
      " 41%|████      | 342/834 [00:59<01:32,  5.35it/s]\n",
      " 41%|████      | 343/834 [00:59<01:33,  5.25it/s]\n",
      " 41%|████      | 344/834 [00:59<01:30,  5.40it/s]\n",
      " 41%|████▏     | 345/834 [01:00<01:27,  5.56it/s]\n",
      " 41%|████▏     | 346/834 [01:00<01:26,  5.62it/s]\n",
      " 42%|████▏     | 347/834 [01:00<01:25,  5.67it/s]\n",
      " 42%|████▏     | 348/834 [01:00<01:23,  5.80it/s]\n",
      " 42%|████▏     | 349/834 [01:00<01:21,  5.93it/s]\n",
      " 42%|████▏     | 350/834 [01:00<01:21,  5.97it/s]\n",
      "                                                 \n",
      "\n",
      " 42%|████▏     | 350/834 [01:00<01:21,  5.97it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 35.11it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 42%|████▏     | 350/834 [01:01<01:21,  5.97it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 35.11it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 42%|████▏     | 351/834 [01:01<01:44,  4.64it/s]\n",
      " 42%|████▏     | 352/834 [01:01<01:36,  5.01it/s]\n",
      " 42%|████▏     | 353/834 [01:01<01:33,  5.17it/s]\n",
      " 42%|████▏     | 354/834 [01:01<01:28,  5.43it/s]\n",
      " 43%|████▎     | 355/834 [01:01<01:24,  5.66it/s]\n",
      " 43%|████▎     | 356/834 [01:02<01:22,  5.76it/s]\n",
      " 43%|████▎     | 357/834 [01:02<01:23,  5.72it/s]\n",
      " 43%|████▎     | 358/834 [01:02<01:23,  5.72it/s]\n",
      " 43%|████▎     | 359/834 [01:02<01:20,  5.87it/s]\n",
      " 43%|████▎     | 360/834 [01:02<01:20,  5.86it/s]\n",
      "                                                 \n",
      "\n",
      " 43%|████▎     | 360/834 [01:02<01:20,  5.86it/s]\n",
      " 43%|████▎     | 361/834 [01:02<01:20,  5.86it/s]\n",
      " 43%|████▎     | 362/834 [01:03<01:20,  5.85it/s]\n",
      " 44%|████▎     | 363/834 [01:03<01:18,  6.01it/s]\n",
      " 44%|████▎     | 364/834 [01:03<01:16,  6.13it/s]\n",
      " 44%|████▍     | 365/834 [01:03<01:16,  6.12it/s]\n",
      " 44%|████▍     | 366/834 [01:03<01:16,  6.14it/s]\n",
      " 44%|████▍     | 367/834 [01:03<01:15,  6.23it/s]\n",
      " 44%|████▍     | 368/834 [01:04<01:14,  6.27it/s]\n",
      " 44%|████▍     | 369/834 [01:04<01:14,  6.26it/s]\n",
      " 44%|████▍     | 370/834 [01:04<01:13,  6.29it/s]\n",
      "                                                 \n",
      "\n",
      " 44%|████▍     | 370/834 [01:04<01:13,  6.29it/s]\n",
      " 44%|████▍     | 371/834 [01:04<01:14,  6.22it/s]\n",
      " 45%|████▍     | 372/834 [01:04<01:14,  6.18it/s]\n",
      " 45%|████▍     | 373/834 [01:04<01:14,  6.20it/s]\n",
      " 45%|████▍     | 374/834 [01:05<01:15,  6.11it/s]\n",
      " 45%|████▍     | 375/834 [01:05<01:14,  6.19it/s]\n",
      " 45%|████▌     | 376/834 [01:05<01:14,  6.18it/s]\n",
      " 45%|████▌     | 377/834 [01:05<01:13,  6.19it/s]\n",
      " 45%|████▌     | 378/834 [01:05<01:13,  6.23it/s]\n",
      " 45%|████▌     | 379/834 [01:05<01:12,  6.26it/s]\n",
      " 46%|████▌     | 380/834 [01:06<01:15,  6.03it/s]\n",
      "                                                 \n",
      "\n",
      " 46%|████▌     | 380/834 [01:06<01:15,  6.03it/s]\n",
      " 46%|████▌     | 381/834 [01:06<01:16,  5.93it/s]\n",
      " 46%|████▌     | 382/834 [01:06<01:14,  6.07it/s]\n",
      " 46%|████▌     | 383/834 [01:06<01:15,  6.00it/s]\n",
      " 46%|████▌     | 384/834 [01:06<01:14,  6.05it/s]\n",
      " 46%|████▌     | 385/834 [01:06<01:13,  6.07it/s]\n",
      " 46%|████▋     | 386/834 [01:07<01:14,  6.01it/s]\n",
      " 46%|████▋     | 387/834 [01:07<01:13,  6.08it/s]\n",
      " 47%|████▋     | 388/834 [01:07<01:13,  6.09it/s]\n",
      " 47%|████▋     | 389/834 [01:07<01:12,  6.10it/s]\n",
      " 47%|████▋     | 390/834 [01:07<01:11,  6.17it/s]\n",
      "                                                 \n",
      "\n",
      " 47%|████▋     | 390/834 [01:07<01:11,  6.17it/s]\n",
      " 47%|████▋     | 391/834 [01:07<01:13,  6.02it/s]\n",
      " 47%|████▋     | 392/834 [01:08<01:13,  6.05it/s]\n",
      " 47%|████▋     | 393/834 [01:08<01:12,  6.10it/s]\n",
      " 47%|████▋     | 394/834 [01:08<01:12,  6.06it/s]\n",
      " 47%|████▋     | 395/834 [01:08<01:11,  6.13it/s]\n",
      " 47%|████▋     | 396/834 [01:08<01:11,  6.15it/s]\n",
      " 48%|████▊     | 397/834 [01:08<01:10,  6.22it/s]\n",
      " 48%|████▊     | 398/834 [01:09<01:10,  6.20it/s]\n",
      " 48%|████▊     | 399/834 [01:09<01:10,  6.19it/s]\n",
      " 48%|████▊     | 400/834 [01:09<01:09,  6.20it/s]\n",
      "                                                 \n",
      "\n",
      " 48%|████▊     | 400/834 [01:09<01:09,  6.20it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 34.31it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 48%|████▊     | 400/834 [01:09<01:09,  6.20it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 34.31it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 48%|████▊     | 401/834 [01:09<01:34,  4.60it/s]\n",
      " 48%|████▊     | 402/834 [01:09<01:26,  4.99it/s]\n",
      " 48%|████▊     | 403/834 [01:09<01:20,  5.35it/s]\n",
      " 48%|████▊     | 404/834 [01:10<01:18,  5.49it/s]\n",
      " 49%|████▊     | 405/834 [01:10<01:15,  5.65it/s]\n",
      " 49%|████▊     | 406/834 [01:10<01:13,  5.85it/s]\n",
      " 49%|████▉     | 407/834 [01:10<01:11,  5.96it/s]\n",
      " 49%|████▉     | 408/834 [01:10<01:11,  5.99it/s]\n",
      " 49%|████▉     | 409/834 [01:10<01:09,  6.11it/s]\n",
      " 49%|████▉     | 410/834 [01:11<01:09,  6.07it/s]\n",
      "                                                 \n",
      "\n",
      " 49%|████▉     | 410/834 [01:11<01:09,  6.07it/s]\n",
      " 49%|████▉     | 411/834 [01:11<01:11,  5.93it/s]\n",
      " 49%|████▉     | 412/834 [01:11<01:11,  5.92it/s]\n",
      " 50%|████▉     | 413/834 [01:11<01:09,  6.02it/s]\n",
      " 50%|████▉     | 414/834 [01:11<01:08,  6.12it/s]\n",
      " 50%|████▉     | 415/834 [01:11<01:08,  6.11it/s]\n",
      " 50%|████▉     | 416/834 [01:12<01:08,  6.07it/s]\n",
      " 50%|█████     | 417/834 [01:12<01:08,  6.11it/s]\n",
      " 50%|█████     | 418/834 [01:12<01:08,  6.10it/s]\n",
      " 50%|█████     | 419/834 [01:12<01:08,  6.10it/s]\n",
      " 50%|█████     | 420/834 [01:12<01:06,  6.20it/s]\n",
      "                                                 \n",
      "\n",
      " 50%|█████     | 420/834 [01:12<01:06,  6.20it/s]\n",
      " 50%|█████     | 421/834 [01:12<01:06,  6.20it/s]\n",
      " 51%|█████     | 422/834 [01:13<01:06,  6.15it/s]\n",
      " 51%|█████     | 423/834 [01:13<01:06,  6.17it/s]\n",
      " 51%|█████     | 424/834 [01:13<01:05,  6.23it/s]\n",
      " 51%|█████     | 425/834 [01:13<01:06,  6.19it/s]\n",
      " 51%|█████     | 426/834 [01:13<01:04,  6.28it/s]\n",
      " 51%|█████     | 427/834 [01:13<01:05,  6.19it/s]\n",
      " 51%|█████▏    | 428/834 [01:14<01:05,  6.17it/s]\n",
      " 51%|█████▏    | 429/834 [01:14<01:06,  6.13it/s]\n",
      " 52%|█████▏    | 430/834 [01:14<01:05,  6.15it/s]\n",
      "                                                 \n",
      "\n",
      " 52%|█████▏    | 430/834 [01:14<01:05,  6.15it/s]\n",
      " 52%|█████▏    | 431/834 [01:14<01:07,  6.01it/s]\n",
      " 52%|█████▏    | 432/834 [01:14<01:06,  6.07it/s]\n",
      " 52%|█████▏    | 433/834 [01:14<01:06,  5.99it/s]\n",
      " 52%|█████▏    | 434/834 [01:15<01:05,  6.10it/s]\n",
      " 52%|█████▏    | 435/834 [01:15<01:06,  6.01it/s]\n",
      " 52%|█████▏    | 436/834 [01:15<01:05,  6.09it/s]\n",
      " 52%|█████▏    | 437/834 [01:15<01:04,  6.13it/s]\n",
      " 53%|█████▎    | 438/834 [01:15<01:05,  6.06it/s]\n",
      " 53%|█████▎    | 439/834 [01:15<01:04,  6.14it/s]\n",
      " 53%|█████▎    | 440/834 [01:16<01:03,  6.17it/s]\n",
      "                                                 \n",
      "\n",
      " 53%|█████▎    | 440/834 [01:16<01:03,  6.17it/s]\n",
      " 53%|█████▎    | 441/834 [01:16<01:05,  5.97it/s]\n",
      " 53%|█████▎    | 442/834 [01:16<01:04,  6.07it/s]\n",
      " 53%|█████▎    | 443/834 [01:16<01:04,  6.05it/s]\n",
      " 53%|█████▎    | 444/834 [01:16<01:05,  5.98it/s]\n",
      " 53%|█████▎    | 445/834 [01:16<01:04,  5.99it/s]\n",
      " 53%|█████▎    | 446/834 [01:17<01:05,  5.96it/s]\n",
      " 54%|█████▎    | 447/834 [01:17<01:05,  5.87it/s]\n",
      " 54%|█████▎    | 448/834 [01:17<01:04,  5.99it/s]\n",
      " 54%|█████▍    | 449/834 [01:17<01:04,  5.93it/s]\n",
      " 54%|█████▍    | 450/834 [01:17<01:04,  5.99it/s]\n",
      "                                                 \n",
      "\n",
      " 54%|█████▍    | 450/834 [01:17<01:04,  5.99it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 34.33it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 54%|█████▍    | 450/834 [01:17<01:04,  5.99it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 34.33it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 54%|█████▍    | 451/834 [01:18<01:22,  4.66it/s]\n",
      " 54%|█████▍    | 452/834 [01:18<01:16,  4.98it/s]\n",
      " 54%|█████▍    | 453/834 [01:18<01:11,  5.35it/s]\n",
      " 54%|█████▍    | 454/834 [01:18<01:07,  5.59it/s]\n",
      " 55%|█████▍    | 455/834 [01:18<01:06,  5.73it/s]\n",
      " 55%|█████▍    | 456/834 [01:18<01:04,  5.88it/s]\n",
      " 55%|█████▍    | 457/834 [01:19<01:03,  5.98it/s]\n",
      " 55%|█████▍    | 458/834 [01:19<01:03,  5.96it/s]\n",
      " 55%|█████▌    | 459/834 [01:19<01:01,  6.12it/s]\n",
      " 55%|█████▌    | 460/834 [01:19<01:03,  5.90it/s]\n",
      "                                                 \n",
      "\n",
      " 55%|█████▌    | 460/834 [01:19<01:03,  5.90it/s]\n",
      " 55%|█████▌    | 461/834 [01:19<01:03,  5.87it/s]\n",
      " 55%|█████▌    | 462/834 [01:19<01:02,  5.99it/s]\n",
      " 56%|█████▌    | 463/834 [01:20<01:01,  6.01it/s]\n",
      " 56%|█████▌    | 464/834 [01:20<01:01,  6.06it/s]\n",
      " 56%|█████▌    | 465/834 [01:20<01:02,  5.93it/s]\n",
      " 56%|█████▌    | 466/834 [01:20<01:01,  6.01it/s]\n",
      " 56%|█████▌    | 467/834 [01:20<01:00,  6.09it/s]\n",
      " 56%|█████▌    | 468/834 [01:20<00:59,  6.20it/s]\n",
      " 56%|█████▌    | 469/834 [01:20<00:58,  6.23it/s]\n",
      " 56%|█████▋    | 470/834 [01:21<00:59,  6.09it/s]\n",
      "                                                 \n",
      "\n",
      " 56%|█████▋    | 470/834 [01:21<00:59,  6.09it/s]\n",
      " 56%|█████▋    | 471/834 [01:21<01:01,  5.90it/s]\n",
      " 57%|█████▋    | 472/834 [01:21<01:00,  5.98it/s]\n",
      " 57%|█████▋    | 473/834 [01:21<00:59,  6.05it/s]\n",
      " 57%|█████▋    | 474/834 [01:21<00:58,  6.11it/s]\n",
      " 57%|█████▋    | 475/834 [01:21<00:58,  6.17it/s]\n",
      " 57%|█████▋    | 476/834 [01:22<00:58,  6.16it/s]\n",
      " 57%|█████▋    | 477/834 [01:22<00:57,  6.26it/s]\n",
      " 57%|█████▋    | 478/834 [01:22<00:56,  6.33it/s]\n",
      " 57%|█████▋    | 479/834 [01:22<00:56,  6.25it/s]\n",
      " 58%|█████▊    | 480/834 [01:22<00:56,  6.29it/s]\n",
      "                                                 \n",
      "\n",
      " 58%|█████▊    | 480/834 [01:22<00:56,  6.29it/s]\n",
      " 58%|█████▊    | 481/834 [01:22<00:56,  6.20it/s]\n",
      " 58%|█████▊    | 482/834 [01:23<00:56,  6.27it/s]\n",
      " 58%|█████▊    | 483/834 [01:23<00:55,  6.29it/s]\n",
      " 58%|█████▊    | 484/834 [01:23<00:56,  6.22it/s]\n",
      " 58%|█████▊    | 485/834 [01:23<00:56,  6.18it/s]\n",
      " 58%|█████▊    | 486/834 [01:23<00:56,  6.21it/s]\n",
      " 58%|█████▊    | 487/834 [01:23<00:56,  6.15it/s]\n",
      " 59%|█████▊    | 488/834 [01:24<00:56,  6.09it/s]\n",
      " 59%|█████▊    | 489/834 [01:24<00:56,  6.10it/s]\n",
      " 59%|█████▉    | 490/834 [01:24<00:55,  6.18it/s]\n",
      "                                                 \n",
      "\n",
      " 59%|█████▉    | 490/834 [01:24<00:55,  6.18it/s]\n",
      " 59%|█████▉    | 491/834 [01:24<00:56,  6.10it/s]\n",
      " 59%|█████▉    | 492/834 [01:24<00:55,  6.20it/s]\n",
      " 59%|█████▉    | 493/834 [01:24<00:54,  6.24it/s]\n",
      " 59%|█████▉    | 494/834 [01:25<00:54,  6.25it/s]\n",
      " 59%|█████▉    | 495/834 [01:25<00:53,  6.28it/s]\n",
      " 59%|█████▉    | 496/834 [01:25<00:53,  6.27it/s]\n",
      " 60%|█████▉    | 497/834 [01:25<00:54,  6.23it/s]\n",
      " 60%|█████▉    | 498/834 [01:25<00:54,  6.19it/s]\n",
      " 60%|█████▉    | 499/834 [01:25<00:53,  6.24it/s]\n",
      " 60%|█████▉    | 500/834 [01:26<00:54,  6.18it/s]\n",
      "                                                 \n",
      "\n",
      " 60%|█████▉    | 500/834 [01:26<00:54,  6.18it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 35.57it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 60%|█████▉    | 500/834 [01:26<00:54,  6.18it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 35.57it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 60%|██████    | 501/834 [01:26<01:46,  3.11it/s]\n",
      " 60%|██████    | 502/834 [01:26<01:30,  3.66it/s]\n",
      " 60%|██████    | 503/834 [01:27<01:18,  4.19it/s]\n",
      " 60%|██████    | 504/834 [01:27<01:12,  4.54it/s]\n",
      " 61%|██████    | 505/834 [01:27<01:06,  4.98it/s]\n",
      " 61%|██████    | 506/834 [01:27<01:01,  5.34it/s]\n",
      " 61%|██████    | 507/834 [01:27<00:58,  5.61it/s]\n",
      " 61%|██████    | 508/834 [01:27<00:56,  5.80it/s]\n",
      " 61%|██████    | 509/834 [01:27<00:54,  5.95it/s]\n",
      " 61%|██████    | 510/834 [01:28<00:53,  6.05it/s]\n",
      "                                                 \n",
      "\n",
      " 61%|██████    | 510/834 [01:28<00:53,  6.05it/s]\n",
      " 61%|██████▏   | 511/834 [01:28<00:54,  5.95it/s]\n",
      " 61%|██████▏   | 512/834 [01:28<00:52,  6.09it/s]\n",
      " 62%|██████▏   | 513/834 [01:28<00:51,  6.18it/s]\n",
      " 62%|██████▏   | 514/834 [01:28<00:51,  6.16it/s]\n",
      " 62%|██████▏   | 515/834 [01:28<00:51,  6.19it/s]\n",
      " 62%|██████▏   | 516/834 [01:29<00:51,  6.16it/s]\n",
      " 62%|██████▏   | 517/834 [01:29<00:51,  6.17it/s]\n",
      " 62%|██████▏   | 518/834 [01:29<00:50,  6.23it/s]\n",
      " 62%|██████▏   | 519/834 [01:29<00:51,  6.15it/s]\n",
      " 62%|██████▏   | 520/834 [01:29<00:50,  6.25it/s]\n",
      "                                                 \n",
      "\n",
      " 62%|██████▏   | 520/834 [01:29<00:50,  6.25it/s]\n",
      " 62%|██████▏   | 521/834 [01:29<00:51,  6.04it/s]\n",
      " 63%|██████▎   | 522/834 [01:30<00:50,  6.14it/s]\n",
      " 63%|██████▎   | 523/834 [01:30<00:50,  6.13it/s]\n",
      " 63%|██████▎   | 524/834 [01:30<00:50,  6.17it/s]\n",
      " 63%|██████▎   | 525/834 [01:30<00:50,  6.09it/s]\n",
      " 63%|██████▎   | 526/834 [01:30<00:51,  6.02it/s]\n",
      " 63%|██████▎   | 527/834 [01:30<00:49,  6.16it/s]\n",
      " 63%|██████▎   | 528/834 [01:31<00:49,  6.17it/s]\n",
      " 63%|██████▎   | 529/834 [01:31<00:50,  6.06it/s]\n",
      " 64%|██████▎   | 530/834 [01:31<00:50,  6.08it/s]\n",
      "                                                 \n",
      "\n",
      " 64%|██████▎   | 530/834 [01:31<00:50,  6.08it/s]\n",
      " 64%|██████▎   | 531/834 [01:31<00:51,  5.89it/s]\n",
      " 64%|██████▍   | 532/834 [01:31<00:50,  6.02it/s]\n",
      " 64%|██████▍   | 533/834 [01:31<00:49,  6.13it/s]\n",
      " 64%|██████▍   | 534/834 [01:32<00:48,  6.20it/s]\n",
      " 64%|██████▍   | 535/834 [01:32<00:48,  6.21it/s]\n",
      " 64%|██████▍   | 536/834 [01:32<00:48,  6.16it/s]\n",
      " 64%|██████▍   | 537/834 [01:32<00:47,  6.21it/s]\n",
      " 65%|██████▍   | 538/834 [01:32<00:47,  6.29it/s]\n",
      " 65%|██████▍   | 539/834 [01:32<00:47,  6.27it/s]\n",
      " 65%|██████▍   | 540/834 [01:33<00:47,  6.22it/s]\n",
      "                                                 \n",
      "\n",
      " 65%|██████▍   | 540/834 [01:33<00:47,  6.22it/s]\n",
      " 65%|██████▍   | 541/834 [01:33<00:48,  5.99it/s]\n",
      " 65%|██████▍   | 542/834 [01:33<00:47,  6.10it/s]\n",
      " 65%|██████▌   | 543/834 [01:33<00:47,  6.09it/s]\n",
      " 65%|██████▌   | 544/834 [01:33<00:48,  6.00it/s]\n",
      " 65%|██████▌   | 545/834 [01:33<00:47,  6.12it/s]\n",
      " 65%|██████▌   | 546/834 [01:34<00:47,  6.12it/s]\n",
      " 66%|██████▌   | 547/834 [01:34<00:47,  6.04it/s]\n",
      " 66%|██████▌   | 548/834 [01:34<00:47,  5.98it/s]\n",
      " 66%|██████▌   | 549/834 [01:34<00:47,  6.00it/s]\n",
      " 66%|██████▌   | 550/834 [01:34<00:47,  6.03it/s]\n",
      "                                                 \n",
      "\n",
      " 66%|██████▌   | 550/834 [01:34<00:47,  6.03it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 33.87it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 66%|██████▌   | 550/834 [01:34<00:47,  6.03it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 33.87it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 66%|██████▌   | 551/834 [01:35<01:01,  4.60it/s]\n",
      " 66%|██████▌   | 552/834 [01:35<00:56,  4.99it/s]\n",
      " 66%|██████▋   | 553/834 [01:35<00:53,  5.28it/s]\n",
      " 66%|██████▋   | 554/834 [01:35<00:50,  5.54it/s]\n",
      " 67%|██████▋   | 555/834 [01:35<00:48,  5.72it/s]\n",
      " 67%|██████▋   | 556/834 [01:35<00:46,  5.94it/s]\n",
      " 67%|██████▋   | 557/834 [01:35<00:46,  5.92it/s]\n",
      " 67%|██████▋   | 558/834 [01:36<00:46,  6.00it/s]\n",
      " 67%|██████▋   | 559/834 [01:36<00:44,  6.12it/s]\n",
      " 67%|██████▋   | 560/834 [01:36<00:44,  6.11it/s]\n",
      "                                                 \n",
      "\n",
      " 67%|██████▋   | 560/834 [01:36<00:44,  6.11it/s]\n",
      " 67%|██████▋   | 561/834 [01:36<00:45,  6.04it/s]\n",
      " 67%|██████▋   | 562/834 [01:36<00:44,  6.16it/s]\n",
      " 68%|██████▊   | 563/834 [01:36<00:43,  6.23it/s]\n",
      " 68%|██████▊   | 564/834 [01:37<00:43,  6.24it/s]\n",
      " 68%|██████▊   | 565/834 [01:37<00:43,  6.15it/s]\n",
      " 68%|██████▊   | 566/834 [01:37<00:43,  6.22it/s]\n",
      " 68%|██████▊   | 567/834 [01:37<00:42,  6.28it/s]\n",
      " 68%|██████▊   | 568/834 [01:37<00:42,  6.31it/s]\n",
      " 68%|██████▊   | 569/834 [01:37<00:43,  6.15it/s]\n",
      " 68%|██████▊   | 570/834 [01:38<00:42,  6.17it/s]\n",
      "                                                 \n",
      "\n",
      " 68%|██████▊   | 570/834 [01:38<00:42,  6.17it/s]\n",
      " 68%|██████▊   | 571/834 [01:38<00:43,  6.03it/s]\n",
      " 69%|██████▊   | 572/834 [01:38<00:42,  6.10it/s]\n",
      " 69%|██████▊   | 573/834 [01:38<00:42,  6.11it/s]\n",
      " 69%|██████▉   | 574/834 [01:38<00:42,  6.05it/s]\n",
      " 69%|██████▉   | 575/834 [01:38<00:41,  6.18it/s]\n",
      " 69%|██████▉   | 576/834 [01:39<00:42,  6.14it/s]\n",
      " 69%|██████▉   | 577/834 [01:39<00:42,  6.10it/s]\n",
      " 69%|██████▉   | 578/834 [01:39<00:41,  6.16it/s]\n",
      " 69%|██████▉   | 579/834 [01:39<00:41,  6.12it/s]\n",
      " 70%|██████▉   | 580/834 [01:39<00:41,  6.08it/s]\n",
      "                                                 \n",
      "\n",
      " 70%|██████▉   | 580/834 [01:39<00:41,  6.08it/s]\n",
      " 70%|██████▉   | 581/834 [01:39<00:42,  5.99it/s]\n",
      " 70%|██████▉   | 582/834 [01:40<00:41,  6.02it/s]\n",
      " 70%|██████▉   | 583/834 [01:40<00:41,  6.11it/s]\n",
      " 70%|███████   | 584/834 [01:40<00:41,  6.05it/s]\n",
      " 70%|███████   | 585/834 [01:40<00:41,  6.07it/s]\n",
      " 70%|███████   | 586/834 [01:40<00:40,  6.06it/s]\n",
      " 70%|███████   | 587/834 [01:40<00:40,  6.09it/s]\n",
      " 71%|███████   | 588/834 [01:41<00:39,  6.17it/s]\n",
      " 71%|███████   | 589/834 [01:41<00:39,  6.22it/s]\n",
      " 71%|███████   | 590/834 [01:41<00:39,  6.21it/s]\n",
      "                                                 \n",
      "\n",
      " 71%|███████   | 590/834 [01:41<00:39,  6.21it/s]\n",
      " 71%|███████   | 591/834 [01:41<00:39,  6.14it/s]\n",
      " 71%|███████   | 592/834 [01:41<00:39,  6.11it/s]\n",
      " 71%|███████   | 593/834 [01:41<00:38,  6.19it/s]\n",
      " 71%|███████   | 594/834 [01:42<00:38,  6.25it/s]\n",
      " 71%|███████▏  | 595/834 [01:42<00:38,  6.23it/s]\n",
      " 71%|███████▏  | 596/834 [01:42<00:38,  6.23it/s]\n",
      " 72%|███████▏  | 597/834 [01:42<00:38,  6.21it/s]\n",
      " 72%|███████▏  | 598/834 [01:42<00:38,  6.20it/s]\n",
      " 72%|███████▏  | 599/834 [01:42<00:38,  6.17it/s]\n",
      " 72%|███████▏  | 600/834 [01:42<00:38,  6.13it/s]\n",
      "                                                 \n",
      "\n",
      " 72%|███████▏  | 600/834 [01:42<00:38,  6.13it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 35.41it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 72%|███████▏  | 600/834 [01:43<00:38,  6.13it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 35.41it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 72%|███████▏  | 601/834 [01:43<00:49,  4.74it/s]\n",
      " 72%|███████▏  | 602/834 [01:43<00:45,  5.14it/s]\n",
      " 72%|███████▏  | 603/834 [01:43<00:42,  5.45it/s]\n",
      " 72%|███████▏  | 604/834 [01:43<00:40,  5.70it/s]\n",
      " 73%|███████▎  | 605/834 [01:43<00:39,  5.82it/s]\n",
      " 73%|███████▎  | 606/834 [01:44<00:38,  5.98it/s]\n",
      " 73%|███████▎  | 607/834 [01:44<00:37,  6.03it/s]\n",
      " 73%|███████▎  | 608/834 [01:44<00:36,  6.13it/s]\n",
      " 73%|███████▎  | 609/834 [01:44<00:36,  6.19it/s]\n",
      " 73%|███████▎  | 610/834 [01:44<00:35,  6.25it/s]\n",
      "                                                 \n",
      "\n",
      " 73%|███████▎  | 610/834 [01:44<00:35,  6.25it/s]\n",
      " 73%|███████▎  | 611/834 [01:44<00:36,  6.04it/s]\n",
      " 73%|███████▎  | 612/834 [01:45<00:36,  6.06it/s]\n",
      " 74%|███████▎  | 613/834 [01:45<00:36,  6.07it/s]\n",
      " 74%|███████▎  | 614/834 [01:45<00:36,  6.11it/s]\n",
      " 74%|███████▎  | 615/834 [01:45<00:35,  6.15it/s]\n",
      " 74%|███████▍  | 616/834 [01:45<00:35,  6.22it/s]\n",
      " 74%|███████▍  | 617/834 [01:45<00:34,  6.23it/s]\n",
      " 74%|███████▍  | 618/834 [01:46<00:35,  6.13it/s]\n",
      " 74%|███████▍  | 619/834 [01:46<00:35,  6.10it/s]\n",
      " 74%|███████▍  | 620/834 [01:46<00:34,  6.16it/s]\n",
      "                                                 \n",
      "\n",
      " 74%|███████▍  | 620/834 [01:46<00:34,  6.16it/s]\n",
      " 74%|███████▍  | 621/834 [01:46<00:35,  6.02it/s]\n",
      " 75%|███████▍  | 622/834 [01:46<00:35,  5.90it/s]\n",
      " 75%|███████▍  | 623/834 [01:46<00:34,  6.06it/s]\n",
      " 75%|███████▍  | 624/834 [01:47<00:34,  6.07it/s]\n",
      " 75%|███████▍  | 625/834 [01:47<00:33,  6.16it/s]\n",
      " 75%|███████▌  | 626/834 [01:47<00:33,  6.21it/s]\n",
      " 75%|███████▌  | 627/834 [01:47<00:33,  6.18it/s]\n",
      " 75%|███████▌  | 628/834 [01:47<00:33,  6.18it/s]\n",
      " 75%|███████▌  | 629/834 [01:47<00:34,  6.03it/s]\n",
      " 76%|███████▌  | 630/834 [01:48<00:33,  6.11it/s]\n",
      "                                                 \n",
      "\n",
      " 76%|███████▌  | 630/834 [01:48<00:33,  6.11it/s]\n",
      " 76%|███████▌  | 631/834 [01:48<00:33,  6.06it/s]\n",
      " 76%|███████▌  | 632/834 [01:48<00:32,  6.20it/s]\n",
      " 76%|███████▌  | 633/834 [01:48<00:32,  6.16it/s]\n",
      " 76%|███████▌  | 634/834 [01:48<00:32,  6.21it/s]\n",
      " 76%|███████▌  | 635/834 [01:48<00:31,  6.29it/s]\n",
      " 76%|███████▋  | 636/834 [01:48<00:31,  6.28it/s]\n",
      " 76%|███████▋  | 637/834 [01:49<00:32,  6.04it/s]\n",
      " 76%|███████▋  | 638/834 [01:49<00:32,  5.99it/s]\n",
      " 77%|███████▋  | 639/834 [01:49<00:32,  6.07it/s]\n",
      " 77%|███████▋  | 640/834 [01:49<00:31,  6.12it/s]\n",
      "                                                 \n",
      "\n",
      " 77%|███████▋  | 640/834 [01:49<00:31,  6.12it/s]\n",
      " 77%|███████▋  | 641/834 [01:49<00:31,  6.15it/s]\n",
      " 77%|███████▋  | 642/834 [01:49<00:30,  6.23it/s]\n",
      " 77%|███████▋  | 643/834 [01:50<00:30,  6.24it/s]\n",
      " 77%|███████▋  | 644/834 [01:50<00:30,  6.26it/s]\n",
      " 77%|███████▋  | 645/834 [01:50<00:30,  6.12it/s]\n",
      " 77%|███████▋  | 646/834 [01:50<00:30,  6.23it/s]\n",
      " 78%|███████▊  | 647/834 [01:50<00:30,  6.14it/s]\n",
      " 78%|███████▊  | 648/834 [01:50<00:29,  6.22it/s]\n",
      " 78%|███████▊  | 649/834 [01:51<00:29,  6.26it/s]\n",
      " 78%|███████▊  | 650/834 [01:51<00:29,  6.29it/s]\n",
      "                                                 \n",
      "\n",
      " 78%|███████▊  | 650/834 [01:51<00:29,  6.29it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 35.97it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 78%|███████▊  | 650/834 [01:51<00:29,  6.29it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 35.97it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 78%|███████▊  | 651/834 [01:51<00:38,  4.73it/s]\n",
      " 78%|███████▊  | 652/834 [01:51<00:36,  5.06it/s]\n",
      " 78%|███████▊  | 653/834 [01:51<00:34,  5.26it/s]\n",
      " 78%|███████▊  | 654/834 [01:52<00:33,  5.45it/s]\n",
      " 79%|███████▊  | 655/834 [01:52<00:31,  5.65it/s]\n",
      " 79%|███████▊  | 656/834 [01:52<00:30,  5.84it/s]\n",
      " 79%|███████▉  | 657/834 [01:52<00:29,  5.95it/s]\n",
      " 79%|███████▉  | 658/834 [01:52<00:29,  6.04it/s]\n",
      " 79%|███████▉  | 659/834 [01:52<00:28,  6.10it/s]\n",
      " 79%|███████▉  | 660/834 [01:53<00:28,  6.17it/s]\n",
      "                                                 \n",
      "\n",
      " 79%|███████▉  | 660/834 [01:53<00:28,  6.17it/s]\n",
      " 79%|███████▉  | 661/834 [01:53<00:28,  6.07it/s]\n",
      " 79%|███████▉  | 662/834 [01:53<00:28,  6.13it/s]\n",
      " 79%|███████▉  | 663/834 [01:53<00:28,  6.08it/s]\n",
      " 80%|███████▉  | 664/834 [01:53<00:27,  6.16it/s]\n",
      " 80%|███████▉  | 665/834 [01:53<00:27,  6.11it/s]\n",
      " 80%|███████▉  | 666/834 [01:54<00:27,  6.20it/s]\n",
      " 80%|███████▉  | 667/834 [01:54<00:27,  6.17it/s]\n",
      " 80%|████████  | 668/834 [01:54<00:26,  6.16it/s]\n",
      " 80%|████████  | 669/834 [01:54<00:27,  6.04it/s]\n",
      " 80%|████████  | 670/834 [01:54<00:26,  6.10it/s]\n",
      "                                                 \n",
      "\n",
      " 80%|████████  | 670/834 [01:54<00:26,  6.10it/s]\n",
      " 80%|████████  | 671/834 [01:54<00:26,  6.17it/s]\n",
      " 81%|████████  | 672/834 [01:54<00:26,  6.21it/s]\n",
      " 81%|████████  | 673/834 [01:55<00:26,  6.13it/s]\n",
      " 81%|████████  | 674/834 [01:55<00:25,  6.18it/s]\n",
      " 81%|████████  | 675/834 [01:55<00:26,  6.07it/s]\n",
      " 81%|████████  | 676/834 [01:55<00:25,  6.13it/s]\n",
      " 81%|████████  | 677/834 [01:55<00:25,  6.14it/s]\n",
      " 81%|████████▏ | 678/834 [01:55<00:25,  6.18it/s]\n",
      " 81%|████████▏ | 679/834 [01:56<00:24,  6.24it/s]\n",
      " 82%|████████▏ | 680/834 [01:56<00:24,  6.30it/s]\n",
      "                                                 \n",
      "\n",
      " 82%|████████▏ | 680/834 [01:56<00:24,  6.30it/s]\n",
      " 82%|████████▏ | 681/834 [01:56<00:24,  6.13it/s]\n",
      " 82%|████████▏ | 682/834 [01:56<00:24,  6.22it/s]\n",
      " 82%|████████▏ | 683/834 [01:56<00:24,  6.27it/s]\n",
      " 82%|████████▏ | 684/834 [01:56<00:23,  6.28it/s]\n",
      " 82%|████████▏ | 685/834 [01:57<00:23,  6.27it/s]\n",
      " 82%|████████▏ | 686/834 [01:57<00:23,  6.26it/s]\n",
      " 82%|████████▏ | 687/834 [01:57<00:23,  6.32it/s]\n",
      " 82%|████████▏ | 688/834 [01:57<00:23,  6.33it/s]\n",
      " 83%|████████▎ | 689/834 [01:57<00:22,  6.33it/s]\n",
      " 83%|████████▎ | 690/834 [01:57<00:23,  6.15it/s]\n",
      "                                                 \n",
      "\n",
      " 83%|████████▎ | 690/834 [01:57<00:23,  6.15it/s]\n",
      " 83%|████████▎ | 691/834 [01:58<00:23,  6.06it/s]\n",
      " 83%|████████▎ | 692/834 [01:58<00:23,  6.13it/s]\n",
      " 83%|████████▎ | 693/834 [01:58<00:22,  6.23it/s]\n",
      " 83%|████████▎ | 694/834 [01:58<00:22,  6.26it/s]\n",
      " 83%|████████▎ | 695/834 [01:58<00:22,  6.26it/s]\n",
      " 83%|████████▎ | 696/834 [01:58<00:22,  6.20it/s]\n",
      " 84%|████████▎ | 697/834 [01:59<00:22,  6.14it/s]\n",
      " 84%|████████▎ | 698/834 [01:59<00:22,  6.00it/s]\n",
      " 84%|████████▍ | 699/834 [01:59<00:22,  6.04it/s]\n",
      " 84%|████████▍ | 700/834 [01:59<00:22,  6.00it/s]\n",
      "                                                 \n",
      "\n",
      " 84%|████████▍ | 700/834 [01:59<00:22,  6.00it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 35.53it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 84%|████████▍ | 700/834 [01:59<00:22,  6.00it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 35.53it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 84%|████████▍ | 701/834 [01:59<00:28,  4.72it/s]\n",
      " 84%|████████▍ | 702/834 [02:00<00:26,  5.05it/s]\n",
      " 84%|████████▍ | 703/834 [02:00<00:24,  5.34it/s]\n",
      " 84%|████████▍ | 704/834 [02:00<00:23,  5.60it/s]\n",
      " 85%|████████▍ | 705/834 [02:00<00:22,  5.74it/s]\n",
      " 85%|████████▍ | 706/834 [02:00<00:21,  5.87it/s]\n",
      " 85%|████████▍ | 707/834 [02:00<00:21,  5.89it/s]\n",
      " 85%|████████▍ | 708/834 [02:00<00:20,  6.00it/s]\n",
      " 85%|████████▌ | 709/834 [02:01<00:20,  6.02it/s]\n",
      " 85%|████████▌ | 710/834 [02:01<00:20,  6.13it/s]\n",
      "                                                 \n",
      "\n",
      " 85%|████████▌ | 710/834 [02:01<00:20,  6.13it/s]\n",
      " 85%|████████▌ | 711/834 [02:01<00:20,  5.89it/s]\n",
      " 85%|████████▌ | 712/834 [02:01<00:20,  6.00it/s]\n",
      " 85%|████████▌ | 713/834 [02:01<00:19,  6.12it/s]\n",
      " 86%|████████▌ | 714/834 [02:01<00:19,  6.03it/s]\n",
      " 86%|████████▌ | 715/834 [02:02<00:19,  6.00it/s]\n",
      " 86%|████████▌ | 716/834 [02:02<00:19,  6.13it/s]\n",
      " 86%|████████▌ | 717/834 [02:02<00:19,  6.08it/s]\n",
      " 86%|████████▌ | 718/834 [02:02<00:19,  6.03it/s]\n",
      " 86%|████████▌ | 719/834 [02:02<00:18,  6.14it/s]\n",
      " 86%|████████▋ | 720/834 [02:02<00:18,  6.14it/s]\n",
      "                                                 \n",
      "\n",
      " 86%|████████▋ | 720/834 [02:02<00:18,  6.14it/s]\n",
      " 86%|████████▋ | 721/834 [02:03<00:18,  6.04it/s]\n",
      " 87%|████████▋ | 722/834 [02:03<00:18,  6.06it/s]\n",
      " 87%|████████▋ | 723/834 [02:03<00:18,  5.93it/s]\n",
      " 87%|████████▋ | 724/834 [02:03<00:18,  5.95it/s]\n",
      " 87%|████████▋ | 725/834 [02:03<00:18,  5.87it/s]\n",
      " 87%|████████▋ | 726/834 [02:03<00:18,  5.79it/s]\n",
      " 87%|████████▋ | 727/834 [02:04<00:18,  5.92it/s]\n",
      " 87%|████████▋ | 728/834 [02:04<00:17,  6.00it/s]\n",
      " 87%|████████▋ | 729/834 [02:04<00:17,  6.04it/s]\n",
      " 88%|████████▊ | 730/834 [02:04<00:17,  6.08it/s]\n",
      "                                                 \n",
      "\n",
      " 88%|████████▊ | 730/834 [02:04<00:17,  6.08it/s]\n",
      " 88%|████████▊ | 731/834 [02:04<00:17,  6.03it/s]\n",
      " 88%|████████▊ | 732/834 [02:04<00:16,  6.12it/s]\n",
      " 88%|████████▊ | 733/834 [02:05<00:16,  6.03it/s]\n",
      " 88%|████████▊ | 734/834 [02:05<00:16,  6.06it/s]\n",
      " 88%|████████▊ | 735/834 [02:05<00:16,  6.02it/s]\n",
      " 88%|████████▊ | 736/834 [02:05<00:16,  6.03it/s]\n",
      " 88%|████████▊ | 737/834 [02:05<00:16,  6.06it/s]\n",
      " 88%|████████▊ | 738/834 [02:05<00:15,  6.11it/s]\n",
      " 89%|████████▊ | 739/834 [02:06<00:15,  6.05it/s]\n",
      " 89%|████████▊ | 740/834 [02:06<00:15,  5.93it/s]\n",
      "                                                 \n",
      "\n",
      " 89%|████████▊ | 740/834 [02:06<00:15,  5.93it/s]\n",
      " 89%|████████▉ | 741/834 [02:06<00:15,  5.89it/s]\n",
      " 89%|████████▉ | 742/834 [02:06<00:15,  6.00it/s]\n",
      " 89%|████████▉ | 743/834 [02:06<00:15,  6.05it/s]\n",
      " 89%|████████▉ | 744/834 [02:06<00:14,  6.16it/s]\n",
      " 89%|████████▉ | 745/834 [02:07<00:14,  6.20it/s]\n",
      " 89%|████████▉ | 746/834 [02:07<00:14,  6.27it/s]\n",
      " 90%|████████▉ | 747/834 [02:07<00:13,  6.22it/s]\n",
      " 90%|████████▉ | 748/834 [02:07<00:13,  6.19it/s]\n",
      " 90%|████████▉ | 749/834 [02:07<00:13,  6.15it/s]\n",
      " 90%|████████▉ | 750/834 [02:07<00:13,  6.21it/s]\n",
      "                                                 \n",
      "\n",
      " 90%|████████▉ | 750/834 [02:07<00:13,  6.21it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 34.77it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 90%|████████▉ | 750/834 [02:08<00:13,  6.21it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 34.77it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 90%|█████████ | 751/834 [02:08<00:17,  4.76it/s]\n",
      " 90%|█████████ | 752/834 [02:08<00:16,  5.09it/s]\n",
      " 90%|█████████ | 753/834 [02:08<00:14,  5.43it/s]\n",
      " 90%|█████████ | 754/834 [02:08<00:14,  5.67it/s]\n",
      " 91%|█████████ | 755/834 [02:08<00:13,  5.88it/s]\n",
      " 91%|█████████ | 756/834 [02:09<00:12,  6.06it/s]\n",
      " 91%|█████████ | 757/834 [02:09<00:12,  6.15it/s]\n",
      " 91%|█████████ | 758/834 [02:09<00:12,  6.22it/s]\n",
      " 91%|█████████ | 759/834 [02:09<00:12,  6.24it/s]\n",
      " 91%|█████████ | 760/834 [02:09<00:11,  6.28it/s]\n",
      "                                                 \n",
      "\n",
      " 91%|█████████ | 760/834 [02:09<00:11,  6.28it/s]\n",
      " 91%|█████████ | 761/834 [02:09<00:11,  6.23it/s]\n",
      " 91%|█████████▏| 762/834 [02:09<00:11,  6.27it/s]\n",
      " 91%|█████████▏| 763/834 [02:10<00:11,  6.30it/s]\n",
      " 92%|█████████▏| 764/834 [02:10<00:11,  6.24it/s]\n",
      " 92%|█████████▏| 765/834 [02:10<00:11,  6.12it/s]\n",
      " 92%|█████████▏| 766/834 [02:10<00:11,  6.08it/s]\n",
      " 92%|█████████▏| 767/834 [02:10<00:10,  6.14it/s]\n",
      " 92%|█████████▏| 768/834 [02:10<00:10,  6.11it/s]\n",
      " 92%|█████████▏| 769/834 [02:11<00:10,  6.12it/s]\n",
      " 92%|█████████▏| 770/834 [02:11<00:10,  6.03it/s]\n",
      "                                                 \n",
      "\n",
      " 92%|█████████▏| 770/834 [02:11<00:10,  6.03it/s]\n",
      " 92%|█████████▏| 771/834 [02:11<00:10,  6.01it/s]\n",
      " 93%|█████████▎| 772/834 [02:11<00:10,  6.12it/s]\n",
      " 93%|█████████▎| 773/834 [02:11<00:09,  6.16it/s]\n",
      " 93%|█████████▎| 774/834 [02:11<00:09,  6.14it/s]\n",
      " 93%|█████████▎| 775/834 [02:12<00:09,  6.14it/s]\n",
      " 93%|█████████▎| 776/834 [02:12<00:09,  6.10it/s]\n",
      " 93%|█████████▎| 777/834 [02:12<00:09,  6.18it/s]\n",
      " 93%|█████████▎| 778/834 [02:12<00:08,  6.24it/s]\n",
      " 93%|█████████▎| 779/834 [02:12<00:08,  6.29it/s]\n",
      " 94%|█████████▎| 780/834 [02:12<00:08,  6.35it/s]\n",
      "                                                 \n",
      "\n",
      " 94%|█████████▎| 780/834 [02:12<00:08,  6.35it/s]\n",
      " 94%|█████████▎| 781/834 [02:13<00:08,  6.26it/s]\n",
      " 94%|█████████▍| 782/834 [02:13<00:08,  6.20it/s]\n",
      " 94%|█████████▍| 783/834 [02:13<00:08,  6.17it/s]\n",
      " 94%|█████████▍| 784/834 [02:13<00:08,  6.22it/s]\n",
      " 94%|█████████▍| 785/834 [02:13<00:07,  6.20it/s]\n",
      " 94%|█████████▍| 786/834 [02:13<00:07,  6.31it/s]\n",
      " 94%|█████████▍| 787/834 [02:14<00:07,  6.21it/s]\n",
      " 94%|█████████▍| 788/834 [02:14<00:07,  6.29it/s]\n",
      " 95%|█████████▍| 789/834 [02:14<00:07,  6.26it/s]\n",
      " 95%|█████████▍| 790/834 [02:14<00:07,  6.22it/s]\n",
      "                                                 \n",
      "\n",
      " 95%|█████████▍| 790/834 [02:14<00:07,  6.22it/s]\n",
      " 95%|█████████▍| 791/834 [02:14<00:06,  6.15it/s]\n",
      " 95%|█████████▍| 792/834 [02:14<00:06,  6.19it/s]\n",
      " 95%|█████████▌| 793/834 [02:14<00:06,  6.10it/s]\n",
      " 95%|█████████▌| 794/834 [02:15<00:06,  6.21it/s]\n",
      " 95%|█████████▌| 795/834 [02:15<00:06,  6.15it/s]\n",
      " 95%|█████████▌| 796/834 [02:15<00:06,  6.19it/s]\n",
      " 96%|█████████▌| 797/834 [02:15<00:05,  6.29it/s]\n",
      " 96%|█████████▌| 798/834 [02:15<00:05,  6.32it/s]\n",
      " 96%|█████████▌| 799/834 [02:15<00:05,  6.29it/s]\n",
      " 96%|█████████▌| 800/834 [02:16<00:05,  6.34it/s]\n",
      "                                                 \n",
      "\n",
      " 96%|█████████▌| 800/834 [02:16<00:05,  6.34it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 35.55it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 96%|█████████▌| 800/834 [02:16<00:05,  6.34it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 35.55it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 96%|█████████▌| 801/834 [02:16<00:06,  4.79it/s]\n",
      " 96%|█████████▌| 802/834 [02:16<00:06,  5.09it/s]\n",
      " 96%|█████████▋| 803/834 [02:16<00:05,  5.38it/s]\n",
      " 96%|█████████▋| 804/834 [02:16<00:05,  5.53it/s]\n",
      " 97%|█████████▋| 805/834 [02:17<00:05,  5.59it/s]\n",
      " 97%|█████████▋| 806/834 [02:17<00:05,  5.57it/s]\n",
      " 97%|█████████▋| 807/834 [02:17<00:04,  5.69it/s]\n",
      " 97%|█████████▋| 808/834 [02:17<00:04,  5.86it/s]\n",
      " 97%|█████████▋| 809/834 [02:17<00:04,  5.86it/s]\n",
      " 97%|█████████▋| 810/834 [02:17<00:04,  5.94it/s]\n",
      "                                                 \n",
      "\n",
      " 97%|█████████▋| 810/834 [02:17<00:04,  5.94it/s]\n",
      " 97%|█████████▋| 811/834 [02:18<00:03,  5.86it/s]\n",
      " 97%|█████████▋| 812/834 [02:18<00:03,  5.99it/s]\n",
      " 97%|█████████▋| 813/834 [02:18<00:03,  6.06it/s]\n",
      " 98%|█████████▊| 814/834 [02:18<00:03,  6.05it/s]\n",
      " 98%|█████████▊| 815/834 [02:18<00:03,  6.13it/s]\n",
      " 98%|█████████▊| 816/834 [02:18<00:02,  6.13it/s]\n",
      " 98%|█████████▊| 817/834 [02:19<00:02,  6.08it/s]\n",
      " 98%|█████████▊| 818/834 [02:19<00:02,  6.10it/s]\n",
      " 98%|█████████▊| 819/834 [02:19<00:02,  6.18it/s]\n",
      " 98%|█████████▊| 820/834 [02:19<00:02,  6.21it/s]\n",
      "                                                 \n",
      "\n",
      " 98%|█████████▊| 820/834 [02:19<00:02,  6.21it/s]\n",
      " 98%|█████████▊| 821/834 [02:19<00:02,  6.10it/s]\n",
      " 99%|█████████▊| 822/834 [02:19<00:01,  6.16it/s]\n",
      " 99%|█████████▊| 823/834 [02:20<00:01,  6.19it/s]\n",
      " 99%|█████████▉| 824/834 [02:20<00:01,  6.19it/s]\n",
      " 99%|█████████▉| 825/834 [02:20<00:01,  6.25it/s]\n",
      " 99%|█████████▉| 826/834 [02:20<00:01,  6.24it/s]\n",
      " 99%|█████████▉| 827/834 [02:20<00:01,  6.27it/s]\n",
      " 99%|█████████▉| 828/834 [02:20<00:00,  6.27it/s]\n",
      " 99%|█████████▉| 829/834 [02:21<00:00,  6.33it/s]\n",
      "100%|█████████▉| 830/834 [02:21<00:00,  6.31it/s]\n",
      "                                                 \n",
      "\n",
      "100%|█████████▉| 830/834 [02:21<00:00,  6.31it/s]\n",
      "100%|█████████▉| 831/834 [02:21<00:00,  6.17it/s]\n",
      "100%|█████████▉| 832/834 [02:21<00:00,  6.11it/s]\n",
      "100%|█████████▉| 833/834 [02:21<00:00,  6.16it/s]\n",
      "100%|██████████| 834/834 [02:21<00:00,  6.20it/s]\n",
      "                                                 \n",
      "\n",
      "100%|██████████| 834/834 [02:21<00:00,  6.20it/s]\n",
      "100%|██████████| 834/834 [02:21<00:00,  5.88it/s]\n",
      "\u001b[32m2024-08-21 20:28:37.970\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m753\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 141.8365, 'train_samples_per_second': 17.64, 'train_steps_per_second': 5.88, 'train_loss': 3.2034788028799372, 'epoch': 1.0, 'train_samples': 2502}\u001b[0m\n",
      "\u001b[32m2024-08-21 20:28:37.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m754\u001b[0m - \u001b[1mSaving model checkpoint to outputs-pt-v1\u001b[0m\n",
      "\u001b[32m2024-08-21 20:28:38.243\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m762\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 32.29it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 28.95it/s]\n",
      "\u001b[32m2024-08-21 20:28:38.428\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m775\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': 3.1834299564361572, 'eval_accuracy': 0.40551181102362205, 'eval_runtime': 0.1678, 'eval_samples_per_second': 59.594, 'eval_steps_per_second': 23.837, 'epoch': 1.0, 'eval_samples': 10, 'perplexity': 24.129374481272862}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python pretraining.py \\\n",
    "    --model_type auto \\\n",
    "    --model_name_or_path Qwen1.5-1.8B-Chat\\\n",
    "    --train_file_dir data\\pretrain \\\n",
    "    --validation_file_dir data\\pretrain \\\n",
    "    --per_device_train_batch_size 3 \\\n",
    "    --per_device_eval_batch_size 3 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --use_peft True \\\n",
    "    --seed 42 \\\n",
    "    --fp16 \\\n",
    "    --max_train_samples 20000 \\\n",
    "    --max_eval_samples 10 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --learning_rate 2e-4 \\\n",
    "    --warmup_ratio 0.05 \\\n",
    "    --weight_decay 0.01 \\\n",
    "    --logging_strategy steps \\\n",
    "    --logging_steps 10 \\\n",
    "    --eval_steps 50 \\\n",
    "    --evaluation_strategy steps \\\n",
    "    --save_steps 500 \\\n",
    "    --save_strategy steps \\\n",
    "    --save_total_limit 3 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --preprocessing_num_workers 1 \\\n",
    "    --block_size 128 \\\n",
    "    --group_by_length True \\\n",
    "    --output_dir outputs-pt-v1 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --ddp_timeout 30000 \\\n",
    "    --logging_first_step True \\\n",
    "    --target_modules all \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --torch_dtype float16 \\\n",
    "    --device_map auto \\\n",
    "    --report_to tensorboard \\\n",
    "    --ddp_find_unused_parameters False \\\n",
    "    --gradient_checkpointing True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,784,704 || all params: 467,772,416 || trainable%: 0.8091\n",
      "{'loss': 5.1886, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 0.0}\n",
      "{'loss': 4.7897, 'grad_norm': 2.5664875507354736, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.01}\n",
      "{'loss': 4.5736, 'grad_norm': 1.6772146224975586, 'learning_rate': 7.619047619047618e-05, 'epoch': 0.02}\n",
      "{'loss': 4.5652, 'grad_norm': 1.6379257440567017, 'learning_rate': 0.0001238095238095238, 'epoch': 0.04}\n",
      "{'loss': 4.2609, 'grad_norm': 1.7567120790481567, 'learning_rate': 0.00015714285714285716, 'epoch': 0.05}\n",
      "{'loss': 4.2746, 'grad_norm': 1.9959819316864014, 'learning_rate': 0.00019974747474747474, 'epoch': 0.06}\n",
      "{'eval_loss': 4.292125225067139, 'eval_accuracy': 0.3094488188976378, 'eval_runtime': 0.1572, 'eval_samples_per_second': 63.623, 'eval_steps_per_second': 25.449, 'epoch': 0.06}\n",
      "{'loss': 4.0705, 'grad_norm': 2.0355987548828125, 'learning_rate': 0.00019722222222222225, 'epoch': 0.07}\n",
      "{'loss': 4.0627, 'grad_norm': 2.772348165512085, 'learning_rate': 0.0001946969696969697, 'epoch': 0.08}\n",
      "{'loss': 4.0242, 'grad_norm': 2.1388866901397705, 'learning_rate': 0.00019217171717171718, 'epoch': 0.1}\n",
      "{'loss': 3.9681, 'grad_norm': 2.165433883666992, 'learning_rate': 0.00018964646464646466, 'epoch': 0.11}\n",
      "{'loss': 3.8576, 'grad_norm': 2.060882568359375, 'learning_rate': 0.00018712121212121212, 'epoch': 0.12}\n",
      "{'eval_loss': 4.183043479919434, 'eval_accuracy': 0.31338582677165355, 'eval_runtime': 0.1474, 'eval_samples_per_second': 67.865, 'eval_steps_per_second': 27.146, 'epoch': 0.12}\n",
      "{'loss': 4.0082, 'grad_norm': 2.064817428588867, 'learning_rate': 0.00018459595959595962, 'epoch': 0.13}\n",
      "{'loss': 3.8707, 'grad_norm': 9.34433364868164, 'learning_rate': 0.00018207070707070708, 'epoch': 0.14}\n",
      "{'loss': 4.0406, 'grad_norm': 2.9807217121124268, 'learning_rate': 0.00017954545454545456, 'epoch': 0.16}\n",
      "{'loss': 3.8575, 'grad_norm': 2.509052276611328, 'learning_rate': 0.000177020202020202, 'epoch': 0.17}\n",
      "{'loss': 3.9139, 'grad_norm': 2.3931827545166016, 'learning_rate': 0.00017449494949494952, 'epoch': 0.18}\n",
      "{'eval_loss': 4.1476335525512695, 'eval_accuracy': 0.3251968503937008, 'eval_runtime': 0.1491, 'eval_samples_per_second': 67.086, 'eval_steps_per_second': 26.835, 'epoch': 0.18}\n",
      "{'loss': 3.7348, 'grad_norm': 2.4114646911621094, 'learning_rate': 0.00017196969696969697, 'epoch': 0.19}\n",
      "{'loss': 3.718, 'grad_norm': 2.388174295425415, 'learning_rate': 0.00016944444444444445, 'epoch': 0.2}\n",
      "{'loss': 3.796, 'grad_norm': 2.5871808528900146, 'learning_rate': 0.00016691919191919193, 'epoch': 0.22}\n",
      "{'loss': 3.5708, 'grad_norm': 2.3482043743133545, 'learning_rate': 0.0001643939393939394, 'epoch': 0.23}\n",
      "{'loss': 3.8371, 'grad_norm': 2.2268550395965576, 'learning_rate': 0.0001618686868686869, 'epoch': 0.24}\n",
      "{'eval_loss': 4.176446437835693, 'eval_accuracy': 0.3220472440944882, 'eval_runtime': 0.1502, 'eval_samples_per_second': 66.569, 'eval_steps_per_second': 26.628, 'epoch': 0.24}\n",
      "{'loss': 3.6368, 'grad_norm': 2.303769111633301, 'learning_rate': 0.00015934343434343434, 'epoch': 0.25}\n",
      "{'loss': 3.6016, 'grad_norm': 2.389505624771118, 'learning_rate': 0.00015681818181818182, 'epoch': 0.26}\n",
      "{'loss': 3.736, 'grad_norm': 2.5528178215026855, 'learning_rate': 0.0001542929292929293, 'epoch': 0.28}\n",
      "{'loss': 3.6605, 'grad_norm': 2.491393804550171, 'learning_rate': 0.00015176767676767678, 'epoch': 0.29}\n",
      "{'loss': 3.7988, 'grad_norm': 2.3330554962158203, 'learning_rate': 0.00014924242424242426, 'epoch': 0.3}\n",
      "{'eval_loss': 4.107182502746582, 'eval_accuracy': 0.33070866141732286, 'eval_runtime': 0.154, 'eval_samples_per_second': 64.935, 'eval_steps_per_second': 25.974, 'epoch': 0.3}\n",
      "{'loss': 3.6539, 'grad_norm': 2.3333563804626465, 'learning_rate': 0.00014671717171717172, 'epoch': 0.31}\n",
      "{'loss': 3.8048, 'grad_norm': 2.30647349357605, 'learning_rate': 0.0001441919191919192, 'epoch': 0.32}\n",
      "{'loss': 3.5147, 'grad_norm': 2.1167407035827637, 'learning_rate': 0.00014166666666666668, 'epoch': 0.34}\n",
      "{'loss': 3.5883, 'grad_norm': 2.6411025524139404, 'learning_rate': 0.00013914141414141416, 'epoch': 0.35}\n",
      "{'loss': 3.7024, 'grad_norm': 2.699122428894043, 'learning_rate': 0.0001366161616161616, 'epoch': 0.36}\n",
      "{'eval_loss': 4.069320201873779, 'eval_accuracy': 0.33307086614173226, 'eval_runtime': 0.1811, 'eval_samples_per_second': 55.219, 'eval_steps_per_second': 22.087, 'epoch': 0.36}\n",
      "{'loss': 3.6262, 'grad_norm': 2.579465866088867, 'learning_rate': 0.0001340909090909091, 'epoch': 0.37}\n",
      "{'loss': 3.6973, 'grad_norm': 2.577077865600586, 'learning_rate': 0.00013156565656565657, 'epoch': 0.38}\n",
      "{'loss': 3.6339, 'grad_norm': 2.748687744140625, 'learning_rate': 0.00012904040404040405, 'epoch': 0.4}\n",
      "{'loss': 3.7358, 'grad_norm': 2.2963509559631348, 'learning_rate': 0.00012651515151515153, 'epoch': 0.41}\n",
      "{'loss': 3.6893, 'grad_norm': 2.530587911605835, 'learning_rate': 0.00012398989898989898, 'epoch': 0.42}\n",
      "{'eval_loss': 4.068427085876465, 'eval_accuracy': 0.33228346456692914, 'eval_runtime': 0.1505, 'eval_samples_per_second': 66.443, 'eval_steps_per_second': 26.577, 'epoch': 0.42}\n",
      "{'loss': 3.6897, 'grad_norm': 2.1269891262054443, 'learning_rate': 0.00012146464646464648, 'epoch': 0.43}\n",
      "{'loss': 3.6534, 'grad_norm': 2.416518449783325, 'learning_rate': 0.00011893939393939394, 'epoch': 0.44}\n",
      "{'loss': 3.6926, 'grad_norm': 2.67549729347229, 'learning_rate': 0.00011641414141414142, 'epoch': 0.46}\n",
      "{'loss': 3.6608, 'grad_norm': 2.4446194171905518, 'learning_rate': 0.00011388888888888889, 'epoch': 0.47}\n",
      "{'loss': 3.722, 'grad_norm': 2.6661150455474854, 'learning_rate': 0.00011136363636363636, 'epoch': 0.48}\n",
      "{'eval_loss': 4.059205055236816, 'eval_accuracy': 0.33307086614173226, 'eval_runtime': 0.149, 'eval_samples_per_second': 67.114, 'eval_steps_per_second': 26.846, 'epoch': 0.48}\n",
      "{'loss': 3.7726, 'grad_norm': 2.5570340156555176, 'learning_rate': 0.00010883838383838385, 'epoch': 0.49}\n",
      "{'loss': 3.684, 'grad_norm': 2.4503674507141113, 'learning_rate': 0.00010631313131313132, 'epoch': 0.5}\n",
      "{'loss': 3.6647, 'grad_norm': 2.908914566040039, 'learning_rate': 0.00010378787878787878, 'epoch': 0.52}\n",
      "{'loss': 3.4996, 'grad_norm': 2.2641284465789795, 'learning_rate': 0.00010126262626262626, 'epoch': 0.53}\n",
      "{'loss': 3.7692, 'grad_norm': 2.5097250938415527, 'learning_rate': 9.873737373737374e-05, 'epoch': 0.54}\n",
      "{'eval_loss': 3.995600938796997, 'eval_accuracy': 0.33937007874015745, 'eval_runtime': 0.1502, 'eval_samples_per_second': 66.599, 'eval_steps_per_second': 26.64, 'epoch': 0.54}\n",
      "{'loss': 3.5631, 'grad_norm': 2.5918171405792236, 'learning_rate': 9.621212121212123e-05, 'epoch': 0.55}\n",
      "{'loss': 3.6379, 'grad_norm': 2.3489272594451904, 'learning_rate': 9.368686868686869e-05, 'epoch': 0.56}\n",
      "{'loss': 3.6827, 'grad_norm': 3.469151258468628, 'learning_rate': 9.116161616161617e-05, 'epoch': 0.58}\n",
      "{'loss': 3.5553, 'grad_norm': 2.5181963443756104, 'learning_rate': 8.863636363636364e-05, 'epoch': 0.59}\n",
      "{'loss': 3.6498, 'grad_norm': 2.5407044887542725, 'learning_rate': 8.611111111111112e-05, 'epoch': 0.6}\n",
      "{'eval_loss': 3.9794743061065674, 'eval_accuracy': 0.34330708661417325, 'eval_runtime': 0.1786, 'eval_samples_per_second': 55.979, 'eval_steps_per_second': 22.392, 'epoch': 0.6}\n",
      "{'loss': 3.5576, 'grad_norm': 2.5209524631500244, 'learning_rate': 8.358585858585859e-05, 'epoch': 0.61}\n",
      "{'loss': 3.6338, 'grad_norm': 2.341318130493164, 'learning_rate': 8.106060606060607e-05, 'epoch': 0.62}\n",
      "{'loss': 3.7914, 'grad_norm': 2.3103318214416504, 'learning_rate': 7.853535353535355e-05, 'epoch': 0.64}\n",
      "{'loss': 3.8144, 'grad_norm': 2.684532642364502, 'learning_rate': 7.601010101010101e-05, 'epoch': 0.65}\n",
      "{'loss': 3.691, 'grad_norm': 2.390098810195923, 'learning_rate': 7.348484848484849e-05, 'epoch': 0.66}\n",
      "{'eval_loss': 3.9554882049560547, 'eval_accuracy': 0.3425196850393701, 'eval_runtime': 0.193, 'eval_samples_per_second': 51.814, 'eval_steps_per_second': 20.726, 'epoch': 0.66}\n",
      "{'loss': 3.5882, 'grad_norm': 2.3099446296691895, 'learning_rate': 7.095959595959596e-05, 'epoch': 0.67}\n",
      "{'loss': 3.5377, 'grad_norm': 2.2839465141296387, 'learning_rate': 6.843434343434344e-05, 'epoch': 0.68}\n",
      "{'loss': 3.6744, 'grad_norm': 2.604222059249878, 'learning_rate': 6.59090909090909e-05, 'epoch': 0.7}\n",
      "{'loss': 3.4096, 'grad_norm': 2.4986820220947266, 'learning_rate': 6.338383838383839e-05, 'epoch': 0.71}\n",
      "{'loss': 3.6492, 'grad_norm': 2.736905813217163, 'learning_rate': 6.085858585858586e-05, 'epoch': 0.72}\n",
      "{'eval_loss': 3.9516844749450684, 'eval_accuracy': 0.3417322834645669, 'eval_runtime': 0.15, 'eval_samples_per_second': 66.668, 'eval_steps_per_second': 26.667, 'epoch': 0.72}\n",
      "{'loss': 3.4984, 'grad_norm': 2.5098934173583984, 'learning_rate': 5.833333333333334e-05, 'epoch': 0.73}\n",
      "{'loss': 3.5297, 'grad_norm': 2.9276559352874756, 'learning_rate': 5.5808080808080806e-05, 'epoch': 0.74}\n",
      "{'loss': 3.6297, 'grad_norm': 2.2746989727020264, 'learning_rate': 5.328282828282829e-05, 'epoch': 0.76}\n",
      "{'loss': 3.5118, 'grad_norm': 2.6223466396331787, 'learning_rate': 5.075757575757576e-05, 'epoch': 0.77}\n",
      "{'loss': 3.5562, 'grad_norm': 2.4418792724609375, 'learning_rate': 4.823232323232323e-05, 'epoch': 0.78}\n",
      "{'eval_loss': 3.9281036853790283, 'eval_accuracy': 0.34488188976377954, 'eval_runtime': 0.1491, 'eval_samples_per_second': 67.056, 'eval_steps_per_second': 26.822, 'epoch': 0.78}\n",
      "{'loss': 3.6284, 'grad_norm': 2.567542314529419, 'learning_rate': 4.5707070707070706e-05, 'epoch': 0.79}\n",
      "{'loss': 3.5014, 'grad_norm': 2.580092430114746, 'learning_rate': 4.318181818181819e-05, 'epoch': 0.8}\n",
      "{'loss': 3.5765, 'grad_norm': 2.747840404510498, 'learning_rate': 4.065656565656566e-05, 'epoch': 0.82}\n",
      "{'loss': 3.6216, 'grad_norm': 2.5384390354156494, 'learning_rate': 3.8131313131313133e-05, 'epoch': 0.83}\n",
      "{'loss': 3.5052, 'grad_norm': 2.29030179977417, 'learning_rate': 3.560606060606061e-05, 'epoch': 0.84}\n",
      "{'eval_loss': 3.9346415996551514, 'eval_accuracy': 0.34566929133858265, 'eval_runtime': 0.156, 'eval_samples_per_second': 64.101, 'eval_steps_per_second': 25.641, 'epoch': 0.84}\n",
      "{'loss': 3.5722, 'grad_norm': 2.6093695163726807, 'learning_rate': 3.308080808080809e-05, 'epoch': 0.85}\n",
      "{'loss': 3.6288, 'grad_norm': 2.7237982749938965, 'learning_rate': 3.055555555555556e-05, 'epoch': 0.86}\n",
      "{'loss': 3.6274, 'grad_norm': 2.4693193435668945, 'learning_rate': 2.803030303030303e-05, 'epoch': 0.88}\n",
      "{'loss': 3.4447, 'grad_norm': 2.483731746673584, 'learning_rate': 2.5505050505050504e-05, 'epoch': 0.89}\n",
      "{'loss': 3.7093, 'grad_norm': 2.504236936569214, 'learning_rate': 2.297979797979798e-05, 'epoch': 0.9}\n",
      "{'eval_loss': 3.929828643798828, 'eval_accuracy': 0.34409448818897637, 'eval_runtime': 0.1601, 'eval_samples_per_second': 62.458, 'eval_steps_per_second': 24.983, 'epoch': 0.9}\n",
      "{'loss': 3.6875, 'grad_norm': 2.5310535430908203, 'learning_rate': 2.0454545454545457e-05, 'epoch': 0.91}\n",
      "{'loss': 3.487, 'grad_norm': 2.733259677886963, 'learning_rate': 1.7929292929292927e-05, 'epoch': 0.92}\n",
      "{'loss': 3.3167, 'grad_norm': 2.517195224761963, 'learning_rate': 1.5404040404040404e-05, 'epoch': 0.94}\n",
      "{'loss': 3.6802, 'grad_norm': 2.666377305984497, 'learning_rate': 1.287878787878788e-05, 'epoch': 0.95}\n",
      "{'loss': 3.6935, 'grad_norm': 2.319779634475708, 'learning_rate': 1.0353535353535354e-05, 'epoch': 0.96}\n",
      "{'eval_loss': 3.927090883255005, 'eval_accuracy': 0.34566929133858265, 'eval_runtime': 0.1662, 'eval_samples_per_second': 60.154, 'eval_steps_per_second': 24.062, 'epoch': 0.96}\n",
      "{'loss': 3.6343, 'grad_norm': 2.2821671962738037, 'learning_rate': 7.82828282828283e-06, 'epoch': 0.97}\n",
      "{'loss': 3.6543, 'grad_norm': 2.40665602684021, 'learning_rate': 5.303030303030304e-06, 'epoch': 0.98}\n",
      "{'loss': 3.6087, 'grad_norm': 2.455580472946167, 'learning_rate': 2.777777777777778e-06, 'epoch': 1.0}\n",
      "{'train_runtime': 144.2397, 'train_samples_per_second': 17.346, 'train_steps_per_second': 5.782, 'train_loss': 3.7271630546743637, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  total_flos               =   558685GF\n",
      "  train_loss               =     3.7272\n",
      "  train_runtime            = 0:02:24.23\n",
      "  train_samples            =       2502\n",
      "  train_samples_per_second =     17.346\n",
      "  train_steps_per_second   =      5.782\n",
      "***** eval metrics *****\n",
      "  epoch                   =        1.0\n",
      "  eval_accuracy           =     0.3465\n",
      "  eval_loss               =      3.924\n",
      "  eval_runtime            = 0:00:00.15\n",
      "  eval_samples            =         10\n",
      "  eval_samples_per_second =     64.019\n",
      "  eval_steps_per_second   =     25.608\n",
      "  perplexity              =    50.6004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 13:28:13.221214: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-13 13:28:13.621274: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "d:\\anaconda\\envs\\py39\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-06-13 13:28:14.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m377\u001b[0m - \u001b[1mModel args: ModelArguments(model_type='auto', model_name_or_path='Qwen/Qwen1.5-0.5B-Chat', tokenizer_name_or_path=None, load_in_8bit=False, load_in_4bit=False, cache_dir=None, model_revision='main', hf_hub_token=None, use_fast_tokenizer=False, torch_dtype='float16', device_map='auto', trust_remote_code=True)\u001b[0m\n",
      "\u001b[32m2024-06-13 13:28:14.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m378\u001b[0m - \u001b[1mData args: DataArguments(dataset_name=None, dataset_config_name=None, train_file_dir='data\\\\pretrain', validation_file_dir='data\\\\pretrain', max_train_samples=20000, max_eval_samples=10, streaming=False, block_size=128, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=1, keep_linebreaks=True)\u001b[0m\n",
      "\u001b[32m2024-06-13 13:28:14.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m379\u001b[0m - \u001b[1mTraining args: Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=30000,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_steps=50,\n",
      "eval_strategy=steps,\n",
      "evaluation_strategy=steps,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=True,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0002,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=outputs-pt-v1\\runs\\Jun13_13-28-14_dpg,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=outputs-pt-v1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=3,\n",
      "per_device_train_batch_size=3,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=outputs-pt-v1,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=3,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      ")\u001b[0m\n",
      "\u001b[32m2024-06-13 13:28:14.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m380\u001b[0m - \u001b[1mScript args: ScriptArguments(use_peft=True, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, qlora=False)\u001b[0m\n",
      "\u001b[32m2024-06-13 13:28:14.327\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m381\u001b[0m - \u001b[1mProcess rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: True\u001b[0m\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m2024-06-13 13:28:14.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m491\u001b[0m - \u001b[1mtrain files: ['data\\\\pretrain\\\\en_article_tail500.txt', 'data\\\\pretrain\\\\fever.txt', 'data\\\\pretrain\\\\tianlongbabu.txt']\u001b[0m\n",
      "\u001b[32m2024-06-13 13:28:14.442\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m501\u001b[0m - \u001b[1meval files: ['data\\\\pretrain\\\\en_article_tail500.txt', 'data\\\\pretrain\\\\fever.txt', 'data\\\\pretrain\\\\tianlongbabu.txt']\u001b[0m\n",
      "\n",
      "Generating train split: 0 examples [00:00, ? examples/s]\n",
      "Generating train split: 3876 examples [00:00, 155039.41 examples/s]\n",
      "\n",
      "Generating validation split: 0 examples [00:00, ? examples/s]\n",
      "Generating validation split: 3876 examples [00:00, 176181.22 examples/s]\n",
      "\u001b[32m2024-06-13 13:28:14.824\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m533\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3876\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3876\n",
      "    })\n",
      "})\u001b[0m\n",
      "\n",
      "Running tokenizer on dataset:   0%|          | 0/3876 [00:00<?, ? examples/s]\n",
      "Running tokenizer on dataset:  26%|██▌       | 1000/3876 [00:01<00:03, 929.93 examples/s]\n",
      "Running tokenizer on dataset:  52%|█████▏    | 2000/3876 [00:02<00:02, 718.03 examples/s]\n",
      "Running tokenizer on dataset:  77%|███████▋  | 3000/3876 [00:04<00:01, 714.98 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 3876/3876 [00:05<00:00, 688.74 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 3876/3876 [00:05<00:00, 710.09 examples/s]\n",
      "\n",
      "Running tokenizer on dataset:   0%|          | 0/3876 [00:00<?, ? examples/s]\n",
      "Running tokenizer on dataset:  26%|██▌       | 1000/3876 [00:01<00:03, 927.43 examples/s]\n",
      "Running tokenizer on dataset:  52%|█████▏    | 2000/3876 [00:02<00:02, 719.24 examples/s]\n",
      "Running tokenizer on dataset:  77%|███████▋  | 3000/3876 [00:04<00:01, 717.66 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 3876/3876 [00:05<00:00, 688.14 examples/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 3876/3876 [00:05<00:00, 710.57 examples/s]\n",
      "\n",
      "Grouping texts in chunks of 128:   0%|          | 0/3876 [00:00<?, ? examples/s]\n",
      "Grouping texts in chunks of 128:  77%|███████▋  | 3000/3876 [00:00<00:00, 20546.58 examples/s]\n",
      "Grouping texts in chunks of 128: 100%|██████████| 3876/3876 [00:00<00:00, 19089.51 examples/s]\n",
      "\n",
      "Grouping texts in chunks of 128:   0%|          | 0/3876 [00:00<?, ? examples/s]\n",
      "Grouping texts in chunks of 128:  77%|███████▋  | 3000/3876 [00:00<00:00, 21729.48 examples/s]\n",
      "Grouping texts in chunks of 128: 100%|██████████| 3876/3876 [00:00<00:00, 19770.07 examples/s]\n",
      "\u001b[32m2024-06-13 13:28:29.437\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m596\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 2502\u001b[0m\n",
      "\u001b[32m2024-06-13 13:28:29.437\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m597\u001b[0m - \u001b[34m\u001b[1mTokenized training example:\u001b[0m\n",
      "\u001b[32m2024-06-13 13:28:29.438\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m598\u001b[0m - \u001b[34m\u001b[1mcontract to work in specified mines and mills. There seemed to be no\n",
      "limit to the factories, forges, refineries, and railways that could be\n",
      "built, to the multitudes that could be employed in conquering a\n",
      "continent. As for the future, that was in the hands of Providence!\n",
      "\n",
      "=Business Theories of Politics.=--As the statesmen of Hamilton's school\n",
      "and the planters of Calhoun's had their theories of government and\n",
      "politics, so the leaders in business enterprise had theirs. It was\n",
      "simple and easily stated. \"It is the duty of the government,\" they\n",
      "ur\u001b[0m\n",
      "\u001b[32m2024-06-13 13:28:29.439\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m610\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
      "\u001b[32m2024-06-13 13:28:29.439\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m611\u001b[0m - \u001b[34m\u001b[1mTokenized eval example:\u001b[0m\n",
      "\u001b[32m2024-06-13 13:28:29.439\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m612\u001b[0m - \u001b[34m\u001b[1mcontract to work in specified mines and mills. There seemed to be no\n",
      "limit to the factories, forges, refineries, and railways that could be\n",
      "built, to the multitudes that could be employed in conquering a\n",
      "continent. As for the future, that was in the hands of Providence!\n",
      "\n",
      "=Business Theories of Politics.=--As the statesmen of Hamilton's school\n",
      "and the planters of Calhoun's had their theories of government and\n",
      "politics, so the leaders in business enterprise had theirs. It was\n",
      "simple and easily stated. \"It is the duty of the government,\" they\n",
      "ur\u001b[0m\n",
      "\u001b[32m2024-06-13 13:28:30.384\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m671\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
      "\u001b[32m2024-06-13 13:28:30.384\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m676\u001b[0m - \u001b[1mInit new peft model\u001b[0m\n",
      "\u001b[32m2024-06-13 13:28:30.385\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m689\u001b[0m - \u001b[1mPeft target_modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[0m\n",
      "\u001b[32m2024-06-13 13:28:30.385\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m690\u001b[0m - \u001b[1mPeft lora_rank: 8\u001b[0m\n",
      "d:\\anaconda\\envs\\py39\\lib\\site-packages\\transformers\\utils\\import_utils.py:533: FutureWarning: `is_torch_tpu_available` is deprecated and will be removed in 4.41.0. Please use the `is_torch_xla_available` instead.\n",
      "  warnings.warn(\n",
      "\u001b[32m2024-06-13 13:28:30.535\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m735\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
      "\u001b[32m2024-06-13 13:28:30.769\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m736\u001b[0m - \u001b[34m\u001b[1mTrain dataloader example: {'input_ids': tensor([[ 99395,  99316,  68065,  49567,   1773, 100270,  13343,  18493,  31843,\n",
      "         100811,  16872, 101041,  81433,  99389,   9909, 102890,  99619, 100506,\n",
      "           3837,  38212, 100898,  81433,  99389,   3837, 107833, 111328, 100067,\n",
      "          49567,  74276, 101364, 111328,  64689, 100297, 105897, 100771, 107429,\n",
      "          97706, 107769, 107174,  81217, 104160, 101899,   1773, 107429, 110305,\n",
      "          13343,  50511, 105109, 101924, 104432, 104405,  90395,  89012, 100316,\n",
      "           8997, 100016,  28291, 101304,   5122, 114380, 101364, 100439, 101924,\n",
      "         118670, 106080, 101304,   3837, 100645, 101046, 105110,   3837, 100470,\n",
      "         105202, 105604,   1773, 101304,  42140,  99726,  34204, 101335,  44793,\n",
      "           3837, 101317, 102504,   3837, 115610,   3837, 101690, 102395,  44793,\n",
      "          49567,   1773, 104110, 100347,   3837, 100345, 107368, 101112,  59151,\n",
      "          81217, 104595, 100034,  50404, 115340,   1773, 101335,  38176,  81217,\n",
      "         101317, 101232, 101304,  23031,  99575,  99533, 100205,  33071, 118039,\n",
      "          42140,  88970,   3837,  30440, 106423,  64355,  99963, 100517,  71138,\n",
      "          21515,  57191],\n",
      "        [  3837,  94443,  44934,  14777,  99933,   3837, 108468, 106677,  42411,\n",
      "          18947, 100229,  99225,   3837, 108233,  99786,  99754, 103301,  42411,\n",
      "         112188,   3837, 106006, 107856, 110197, 100560,  71618,   8997,  37474,\n",
      "          99783, 100929,  99164,  99283, 117746,   3837, 106052,  36987,  56568,\n",
      "          27733, 102933,   9370,  99364,  75437,  17340,   3837,  88051,  99521,\n",
      "         101209, 106156,   9370,  34187,  75758, 106116,  36987, 102903, 119516,\n",
      "          99315, 101036,  11319,  42411, 102972,  99817,  49567,  35946,  81596,\n",
      "          81264,  75405, 106783,  79766,  44793,  36987, 104389,  36667,  49567,\n",
      "          34187,  56568,  99612,   8903,  99612,  99530,   3837, 113364,  81596,\n",
      "          11319,  42411,  99314, 104060,  32945,  37474,  99783,  28641,  13343,\n",
      "          99315,  99433, 114831,   3837,  99370,  44793,  36987, 100279,  99226,\n",
      "           3837, 100279,  99226,   6313,  35946, 105872,  21287, 102142,   1773,\n",
      "          42411, 100155,  99927,  30534, 102328,  35946, 100090,  42411,  17714,\n",
      "          99235,   3837,  30440, 102085, 100007,  20412, 104334,  32945,    198,\n",
      "          75405, 106783],\n",
      "        [    20,     11,  10778,    315,  12066,    594,    198,     82,    843,\n",
      "            900,     11,    566,   1030,  79508,     25,    330,   3862,    374,\n",
      "           2494,  40692,    304,   3432,    264,    198,  11141,    311,   2948,\n",
      "           8958,  17695,   1635,   2937,     11,    979,   4588,    311,   3270,\n",
      "            458,  72894,    369,    279,    198,   1168,  31231,    518,  19335,\n",
      "            304,    220,     16,     23,     22,     21,     11,    566,   1410,\n",
      "           1744,   1172,    315,    264,  67096,    198,  36468,    554,    389,\n",
      "            279,   6995,    510,    198,    262,    330,   7812,    697,   1584,\n",
      "          14963,   2789,     26,   1473,    697,  46280,    280,    257,   1597,\n",
      "           8645,   4505,    311,   8193,   1741,   2513,    198,    257,   1634,\n",
      "           1550,   7359,  11699,   4279,    304,  13929,    198,    257,   2014,\n",
      "           4332,    279,  87702,    323,   5046,   2513,   1290,    624,    257,\n",
      "           1416,    429,   1513,    944,   7807,   1059,     11,   3170,     11,\n",
      "            498,   1184,   1172,    198,    257,   2014,   1473,    697,   5535,\n",
      "           1707,    304]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'labels': tensor([[ 99395,  99316,  68065,  49567,   1773, 100270,  13343,  18493,  31843,\n",
      "         100811,  16872, 101041,  81433,  99389,   9909, 102890,  99619, 100506,\n",
      "           3837,  38212, 100898,  81433,  99389,   3837, 107833, 111328, 100067,\n",
      "          49567,  74276, 101364, 111328,  64689, 100297, 105897, 100771, 107429,\n",
      "          97706, 107769, 107174,  81217, 104160, 101899,   1773, 107429, 110305,\n",
      "          13343,  50511, 105109, 101924, 104432, 104405,  90395,  89012, 100316,\n",
      "           8997, 100016,  28291, 101304,   5122, 114380, 101364, 100439, 101924,\n",
      "         118670, 106080, 101304,   3837, 100645, 101046, 105110,   3837, 100470,\n",
      "         105202, 105604,   1773, 101304,  42140,  99726,  34204, 101335,  44793,\n",
      "           3837, 101317, 102504,   3837, 115610,   3837, 101690, 102395,  44793,\n",
      "          49567,   1773, 104110, 100347,   3837, 100345, 107368, 101112,  59151,\n",
      "          81217, 104595, 100034,  50404, 115340,   1773, 101335,  38176,  81217,\n",
      "         101317, 101232, 101304,  23031,  99575,  99533, 100205,  33071, 118039,\n",
      "          42140,  88970,   3837,  30440, 106423,  64355,  99963, 100517,  71138,\n",
      "          21515,  57191],\n",
      "        [  3837,  94443,  44934,  14777,  99933,   3837, 108468, 106677,  42411,\n",
      "          18947, 100229,  99225,   3837, 108233,  99786,  99754, 103301,  42411,\n",
      "         112188,   3837, 106006, 107856, 110197, 100560,  71618,   8997,  37474,\n",
      "          99783, 100929,  99164,  99283, 117746,   3837, 106052,  36987,  56568,\n",
      "          27733, 102933,   9370,  99364,  75437,  17340,   3837,  88051,  99521,\n",
      "         101209, 106156,   9370,  34187,  75758, 106116,  36987, 102903, 119516,\n",
      "          99315, 101036,  11319,  42411, 102972,  99817,  49567,  35946,  81596,\n",
      "          81264,  75405, 106783,  79766,  44793,  36987, 104389,  36667,  49567,\n",
      "          34187,  56568,  99612,   8903,  99612,  99530,   3837, 113364,  81596,\n",
      "          11319,  42411,  99314, 104060,  32945,  37474,  99783,  28641,  13343,\n",
      "          99315,  99433, 114831,   3837,  99370,  44793,  36987, 100279,  99226,\n",
      "           3837, 100279,  99226,   6313,  35946, 105872,  21287, 102142,   1773,\n",
      "          42411, 100155,  99927,  30534, 102328,  35946, 100090,  42411,  17714,\n",
      "          99235,   3837,  30440, 102085, 100007,  20412, 104334,  32945,    198,\n",
      "          75405, 106783],\n",
      "        [    20,     11,  10778,    315,  12066,    594,    198,     82,    843,\n",
      "            900,     11,    566,   1030,  79508,     25,    330,   3862,    374,\n",
      "           2494,  40692,    304,   3432,    264,    198,  11141,    311,   2948,\n",
      "           8958,  17695,   1635,   2937,     11,    979,   4588,    311,   3270,\n",
      "            458,  72894,    369,    279,    198,   1168,  31231,    518,  19335,\n",
      "            304,    220,     16,     23,     22,     21,     11,    566,   1410,\n",
      "           1744,   1172,    315,    264,  67096,    198,  36468,    554,    389,\n",
      "            279,   6995,    510,    198,    262,    330,   7812,    697,   1584,\n",
      "          14963,   2789,     26,   1473,    697,  46280,    280,    257,   1597,\n",
      "           8645,   4505,    311,   8193,   1741,   2513,    198,    257,   1634,\n",
      "           1550,   7359,  11699,   4279,    304,  13929,    198,    257,   2014,\n",
      "           4332,    279,  87702,    323,   5046,   2513,   1290,    624,    257,\n",
      "           1416,    429,   1513,    944,   7807,   1059,     11,   3170,     11,\n",
      "            498,   1184,   1172,    198,    257,   2014,   1473,    697,   5535,\n",
      "           1707,    304]], device='cuda:0')}\u001b[0m\n",
      "\n",
      "  0%|          | 0/834 [00:00<?, ?it/s]d:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\py39\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:679: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "\n",
      "  0%|          | 1/834 [00:00<05:50,  2.38it/s]\n",
      "                                               \n",
      "\n",
      "  0%|          | 1/834 [00:00<05:50,  2.38it/s]\n",
      "  0%|          | 2/834 [00:00<03:42,  3.74it/s]\n",
      "  0%|          | 3/834 [00:00<03:25,  4.03it/s]\n",
      "  0%|          | 4/834 [00:00<02:59,  4.62it/s]\n",
      "  1%|          | 5/834 [00:01<02:44,  5.03it/s]\n",
      "  1%|          | 6/834 [00:01<02:36,  5.27it/s]\n",
      "  1%|          | 7/834 [00:01<02:28,  5.55it/s]\n",
      "  1%|          | 8/834 [00:01<02:24,  5.72it/s]\n",
      "  1%|          | 9/834 [00:01<02:19,  5.90it/s]\n",
      "  1%|          | 10/834 [00:01<02:18,  5.96it/s]\n",
      "                                                \n",
      "\n",
      "  1%|          | 10/834 [00:01<02:18,  5.96it/s]\n",
      "  1%|▏         | 11/834 [00:02<02:19,  5.92it/s]\n",
      "  1%|▏         | 12/834 [00:02<02:19,  5.91it/s]\n",
      "  2%|▏         | 13/834 [00:02<02:15,  6.04it/s]\n",
      "  2%|▏         | 14/834 [00:02<02:16,  6.02it/s]\n",
      "  2%|▏         | 15/834 [00:02<02:15,  6.04it/s]\n",
      "  2%|▏         | 16/834 [00:02<02:16,  6.00it/s]\n",
      "  2%|▏         | 17/834 [00:03<02:15,  6.01it/s]\n",
      "  2%|▏         | 18/834 [00:03<02:14,  6.07it/s]\n",
      "  2%|▏         | 19/834 [00:03<02:15,  6.00it/s]\n",
      "  2%|▏         | 20/834 [00:03<02:13,  6.08it/s]\n",
      "                                                \n",
      "\n",
      "  2%|▏         | 20/834 [00:03<02:13,  6.08it/s]\n",
      "  3%|▎         | 21/834 [00:03<02:14,  6.04it/s]\n",
      "  3%|▎         | 22/834 [00:03<02:12,  6.12it/s]\n",
      "  3%|▎         | 23/834 [00:04<02:12,  6.13it/s]\n",
      "  3%|▎         | 24/834 [00:04<02:10,  6.18it/s]\n",
      "  3%|▎         | 25/834 [00:04<02:09,  6.24it/s]\n",
      "  3%|▎         | 26/834 [00:04<02:08,  6.27it/s]\n",
      "  3%|▎         | 27/834 [00:04<02:08,  6.27it/s]\n",
      "  3%|▎         | 28/834 [00:04<02:08,  6.29it/s]\n",
      "  3%|▎         | 29/834 [00:05<02:10,  6.17it/s]\n",
      "  4%|▎         | 30/834 [00:05<02:09,  6.19it/s]\n",
      "                                                \n",
      "\n",
      "  4%|▎         | 30/834 [00:05<02:09,  6.19it/s]\n",
      "  4%|▎         | 31/834 [00:05<02:11,  6.10it/s]\n",
      "  4%|▍         | 32/834 [00:05<02:13,  6.03it/s]\n",
      "  4%|▍         | 33/834 [00:05<02:11,  6.07it/s]\n",
      "  4%|▍         | 34/834 [00:05<02:10,  6.14it/s]\n",
      "  4%|▍         | 35/834 [00:06<02:09,  6.18it/s]\n",
      "  4%|▍         | 36/834 [00:06<02:12,  6.04it/s]\n",
      "  4%|▍         | 37/834 [00:06<02:16,  5.83it/s]\n",
      "  5%|▍         | 38/834 [00:06<02:18,  5.75it/s]\n",
      "  5%|▍         | 39/834 [00:06<02:15,  5.86it/s]\n",
      "  5%|▍         | 40/834 [00:06<02:14,  5.90it/s]\n",
      "                                                \n",
      "\n",
      "  5%|▍         | 40/834 [00:06<02:14,  5.90it/s]\n",
      "  5%|▍         | 41/834 [00:07<02:16,  5.79it/s]\n",
      "  5%|▌         | 42/834 [00:07<02:16,  5.80it/s]\n",
      "  5%|▌         | 43/834 [00:07<02:15,  5.85it/s]\n",
      "  5%|▌         | 44/834 [00:07<02:13,  5.91it/s]\n",
      "  5%|▌         | 45/834 [00:07<02:17,  5.76it/s]\n",
      "  6%|▌         | 46/834 [00:07<02:15,  5.82it/s]\n",
      "  6%|▌         | 47/834 [00:08<02:14,  5.86it/s]\n",
      "  6%|▌         | 48/834 [00:08<02:12,  5.95it/s]\n",
      "  6%|▌         | 49/834 [00:08<02:10,  6.00it/s]\n",
      "  6%|▌         | 50/834 [00:08<02:08,  6.09it/s]\n",
      "                                                \n",
      "\n",
      "  6%|▌         | 50/834 [00:08<02:08,  6.09it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 36.36it/s]\u001b[A\n",
      "                                                \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      "  6%|▌         | 50/834 [00:08<02:08,  6.09it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 36.36it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      "  6%|▌         | 51/834 [00:08<02:47,  4.69it/s]\n",
      "  6%|▌         | 52/834 [00:09<02:35,  5.03it/s]\n",
      "  6%|▋         | 53/834 [00:09<02:26,  5.34it/s]\n",
      "  6%|▋         | 54/834 [00:09<02:19,  5.58it/s]\n",
      "  7%|▋         | 55/834 [00:09<02:18,  5.64it/s]\n",
      "  7%|▋         | 56/834 [00:09<02:14,  5.79it/s]\n",
      "  7%|▋         | 57/834 [00:09<02:12,  5.86it/s]\n",
      "  7%|▋         | 58/834 [00:10<02:10,  5.96it/s]\n",
      "  7%|▋         | 59/834 [00:10<02:11,  5.91it/s]\n",
      "  7%|▋         | 60/834 [00:10<02:09,  5.98it/s]\n",
      "                                                \n",
      "\n",
      "  7%|▋         | 60/834 [00:10<02:09,  5.98it/s]\n",
      "  7%|▋         | 61/834 [00:10<02:12,  5.84it/s]\n",
      "  7%|▋         | 62/834 [00:10<02:15,  5.68it/s]\n",
      "  8%|▊         | 63/834 [00:10<02:11,  5.87it/s]\n",
      "  8%|▊         | 64/834 [00:11<02:07,  6.02it/s]\n",
      "  8%|▊         | 65/834 [00:11<02:06,  6.07it/s]\n",
      "  8%|▊         | 66/834 [00:11<02:07,  6.03it/s]\n",
      "  8%|▊         | 67/834 [00:11<02:08,  5.95it/s]\n",
      "  8%|▊         | 68/834 [00:11<02:07,  5.99it/s]\n",
      "  8%|▊         | 69/834 [00:11<02:06,  6.05it/s]\n",
      "  8%|▊         | 70/834 [00:12<02:07,  5.99it/s]\n",
      "                                                \n",
      "\n",
      "  8%|▊         | 70/834 [00:12<02:07,  5.99it/s]\n",
      "  9%|▊         | 71/834 [00:12<02:08,  5.96it/s]\n",
      "  9%|▊         | 72/834 [00:12<02:06,  6.02it/s]\n",
      "  9%|▉         | 73/834 [00:12<02:04,  6.12it/s]\n",
      "  9%|▉         | 74/834 [00:12<02:03,  6.14it/s]\n",
      "  9%|▉         | 75/834 [00:12<02:05,  6.07it/s]\n",
      "  9%|▉         | 76/834 [00:13<02:04,  6.11it/s]\n",
      "  9%|▉         | 77/834 [00:13<02:06,  5.96it/s]\n",
      "  9%|▉         | 78/834 [00:13<02:06,  5.98it/s]\n",
      "  9%|▉         | 79/834 [00:13<02:05,  6.04it/s]\n",
      " 10%|▉         | 80/834 [00:13<02:03,  6.13it/s]\n",
      "                                                \n",
      "\n",
      " 10%|▉         | 80/834 [00:13<02:03,  6.13it/s]\n",
      " 10%|▉         | 81/834 [00:13<02:04,  6.06it/s]\n",
      " 10%|▉         | 82/834 [00:14<02:04,  6.05it/s]\n",
      " 10%|▉         | 83/834 [00:14<02:01,  6.16it/s]\n",
      " 10%|█         | 84/834 [00:14<02:04,  6.04it/s]\n",
      " 10%|█         | 85/834 [00:14<02:02,  6.11it/s]\n",
      " 10%|█         | 86/834 [00:14<02:02,  6.12it/s]\n",
      " 10%|█         | 87/834 [00:14<02:00,  6.18it/s]\n",
      " 11%|█         | 88/834 [00:15<01:59,  6.23it/s]\n",
      " 11%|█         | 89/834 [00:15<01:59,  6.24it/s]\n",
      " 11%|█         | 90/834 [00:15<01:58,  6.25it/s]\n",
      "                                                \n",
      "\n",
      " 11%|█         | 90/834 [00:15<01:58,  6.25it/s]\n",
      " 11%|█         | 91/834 [00:15<01:58,  6.27it/s]\n",
      " 11%|█         | 92/834 [00:15<01:57,  6.29it/s]\n",
      " 11%|█         | 93/834 [00:15<01:57,  6.30it/s]\n",
      " 11%|█▏        | 94/834 [00:16<02:00,  6.13it/s]\n",
      " 11%|█▏        | 95/834 [00:16<01:59,  6.18it/s]\n",
      " 12%|█▏        | 96/834 [00:16<01:59,  6.19it/s]\n",
      " 12%|█▏        | 97/834 [00:16<01:59,  6.19it/s]\n",
      " 12%|█▏        | 98/834 [00:16<01:58,  6.23it/s]\n",
      " 12%|█▏        | 99/834 [00:16<01:57,  6.27it/s]\n",
      " 12%|█▏        | 100/834 [00:16<01:56,  6.28it/s]\n",
      "                                                 \n",
      "\n",
      " 12%|█▏        | 100/834 [00:16<01:56,  6.28it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 37.61it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 12%|█▏        | 100/834 [00:17<01:56,  6.28it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 37.61it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 12%|█▏        | 101/834 [00:17<02:33,  4.78it/s]\n",
      " 12%|█▏        | 102/834 [00:17<02:20,  5.19it/s]\n",
      " 12%|█▏        | 103/834 [00:17<02:14,  5.44it/s]\n",
      " 12%|█▏        | 104/834 [00:17<02:09,  5.64it/s]\n",
      " 13%|█▎        | 105/834 [00:17<02:06,  5.77it/s]\n",
      " 13%|█▎        | 106/834 [00:18<02:04,  5.83it/s]\n",
      " 13%|█▎        | 107/834 [00:18<02:01,  5.97it/s]\n",
      " 13%|█▎        | 108/834 [00:18<01:59,  6.10it/s]\n",
      " 13%|█▎        | 109/834 [00:18<01:58,  6.12it/s]\n",
      " 13%|█▎        | 110/834 [00:18<01:58,  6.12it/s]\n",
      "                                                 \n",
      "\n",
      " 13%|█▎        | 110/834 [00:18<01:58,  6.12it/s]\n",
      " 13%|█▎        | 111/834 [00:18<01:59,  6.04it/s]\n",
      " 13%|█▎        | 112/834 [00:19<02:00,  6.01it/s]\n",
      " 14%|█▎        | 113/834 [00:19<01:57,  6.12it/s]\n",
      " 14%|█▎        | 114/834 [00:19<01:57,  6.11it/s]\n",
      " 14%|█▍        | 115/834 [00:19<01:59,  6.04it/s]\n",
      " 14%|█▍        | 116/834 [00:19<01:59,  6.01it/s]\n",
      " 14%|█▍        | 117/834 [00:19<01:58,  6.05it/s]\n",
      " 14%|█▍        | 118/834 [00:20<01:58,  6.03it/s]\n",
      " 14%|█▍        | 119/834 [00:20<01:57,  6.06it/s]\n",
      " 14%|█▍        | 120/834 [00:20<01:56,  6.10it/s]\n",
      "                                                 \n",
      "\n",
      " 14%|█▍        | 120/834 [00:20<01:56,  6.10it/s]\n",
      " 15%|█▍        | 121/834 [00:20<02:01,  5.89it/s]\n",
      " 15%|█▍        | 122/834 [00:20<02:00,  5.91it/s]\n",
      " 15%|█▍        | 123/834 [00:20<01:57,  6.05it/s]\n",
      " 15%|█▍        | 124/834 [00:21<01:56,  6.09it/s]\n",
      " 15%|█▍        | 125/834 [00:21<02:00,  5.89it/s]\n",
      " 15%|█▌        | 126/834 [00:21<01:57,  6.04it/s]\n",
      " 15%|█▌        | 127/834 [00:21<01:56,  6.05it/s]\n",
      " 15%|█▌        | 128/834 [00:21<01:55,  6.10it/s]\n",
      " 15%|█▌        | 129/834 [00:21<01:56,  6.06it/s]\n",
      " 16%|█▌        | 130/834 [00:22<01:57,  6.01it/s]\n",
      "                                                 \n",
      "\n",
      " 16%|█▌        | 130/834 [00:22<01:57,  6.01it/s]\n",
      " 16%|█▌        | 131/834 [00:22<02:01,  5.78it/s]\n",
      " 16%|█▌        | 132/834 [00:22<01:58,  5.95it/s]\n",
      " 16%|█▌        | 133/834 [00:22<01:55,  6.04it/s]\n",
      " 16%|█▌        | 134/834 [00:22<01:56,  6.02it/s]\n",
      " 16%|█▌        | 135/834 [00:22<01:54,  6.11it/s]\n",
      " 16%|█▋        | 136/834 [00:23<01:54,  6.10it/s]\n",
      " 16%|█▋        | 137/834 [00:23<01:54,  6.08it/s]\n",
      " 17%|█▋        | 138/834 [00:23<01:52,  6.17it/s]\n",
      " 17%|█▋        | 139/834 [00:23<01:52,  6.16it/s]\n",
      " 17%|█▋        | 140/834 [00:23<01:54,  6.06it/s]\n",
      "                                                 \n",
      "\n",
      " 17%|█▋        | 140/834 [00:23<01:54,  6.06it/s]\n",
      " 17%|█▋        | 141/834 [00:23<01:58,  5.84it/s]\n",
      " 17%|█▋        | 142/834 [00:24<01:57,  5.86it/s]\n",
      " 17%|█▋        | 143/834 [00:24<01:59,  5.78it/s]\n",
      " 17%|█▋        | 144/834 [00:24<02:06,  5.47it/s]\n",
      " 17%|█▋        | 145/834 [00:24<02:03,  5.59it/s]\n",
      " 18%|█▊        | 146/834 [00:24<02:01,  5.67it/s]\n",
      " 18%|█▊        | 147/834 [00:24<01:59,  5.73it/s]\n",
      " 18%|█▊        | 148/834 [00:25<01:57,  5.83it/s]\n",
      " 18%|█▊        | 149/834 [00:25<01:56,  5.86it/s]\n",
      " 18%|█▊        | 150/834 [00:25<01:57,  5.81it/s]\n",
      "                                                 \n",
      "\n",
      " 18%|█▊        | 150/834 [00:25<01:57,  5.81it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 36.34it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 18%|█▊        | 150/834 [00:25<01:57,  5.81it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 36.34it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 18%|█▊        | 151/834 [00:25<02:29,  4.56it/s]\n",
      " 18%|█▊        | 152/834 [00:25<02:18,  4.94it/s]\n",
      " 18%|█▊        | 153/834 [00:26<02:09,  5.26it/s]\n",
      " 18%|█▊        | 154/834 [00:26<02:03,  5.49it/s]\n",
      " 19%|█▊        | 155/834 [00:26<02:00,  5.65it/s]\n",
      " 19%|█▊        | 156/834 [00:26<01:58,  5.70it/s]\n",
      " 19%|█▉        | 157/834 [00:26<01:56,  5.79it/s]\n",
      " 19%|█▉        | 158/834 [00:26<01:54,  5.90it/s]\n",
      " 19%|█▉        | 159/834 [00:27<01:54,  5.89it/s]\n",
      " 19%|█▉        | 160/834 [00:27<01:52,  6.01it/s]\n",
      "                                                 \n",
      "\n",
      " 19%|█▉        | 160/834 [00:27<01:52,  6.01it/s]\n",
      " 19%|█▉        | 161/834 [00:27<01:57,  5.73it/s]\n",
      " 19%|█▉        | 162/834 [00:27<01:58,  5.65it/s]\n",
      " 20%|█▉        | 163/834 [00:27<01:57,  5.70it/s]\n",
      " 20%|█▉        | 164/834 [00:28<02:00,  5.55it/s]\n",
      " 20%|█▉        | 165/834 [00:28<01:58,  5.65it/s]\n",
      " 20%|█▉        | 166/834 [00:28<01:56,  5.72it/s]\n",
      " 20%|██        | 167/834 [00:28<01:54,  5.84it/s]\n",
      " 20%|██        | 168/834 [00:28<01:58,  5.63it/s]\n",
      " 20%|██        | 169/834 [00:28<01:59,  5.57it/s]\n",
      " 20%|██        | 170/834 [00:29<01:55,  5.73it/s]\n",
      "                                                 \n",
      "\n",
      " 20%|██        | 170/834 [00:29<01:55,  5.73it/s]\n",
      " 21%|██        | 171/834 [00:29<01:53,  5.86it/s]\n",
      " 21%|██        | 172/834 [00:29<01:52,  5.89it/s]\n",
      " 21%|██        | 173/834 [00:29<01:50,  6.00it/s]\n",
      " 21%|██        | 174/834 [00:29<01:48,  6.08it/s]\n",
      " 21%|██        | 175/834 [00:29<01:48,  6.06it/s]\n",
      " 21%|██        | 176/834 [00:30<01:48,  6.04it/s]\n",
      " 21%|██        | 177/834 [00:30<01:49,  6.01it/s]\n",
      " 21%|██▏       | 178/834 [00:30<01:49,  5.98it/s]\n",
      " 21%|██▏       | 179/834 [00:30<01:49,  5.99it/s]\n",
      " 22%|██▏       | 180/834 [00:30<01:49,  5.96it/s]\n",
      "                                                 \n",
      "\n",
      " 22%|██▏       | 180/834 [00:30<01:49,  5.96it/s]\n",
      " 22%|██▏       | 181/834 [00:30<01:54,  5.72it/s]\n",
      " 22%|██▏       | 182/834 [00:31<01:52,  5.81it/s]\n",
      " 22%|██▏       | 183/834 [00:31<01:51,  5.82it/s]\n",
      " 22%|██▏       | 184/834 [00:31<01:49,  5.94it/s]\n",
      " 22%|██▏       | 185/834 [00:31<01:51,  5.84it/s]\n",
      " 22%|██▏       | 186/834 [00:31<01:51,  5.82it/s]\n",
      " 22%|██▏       | 187/834 [00:31<01:57,  5.53it/s]\n",
      " 23%|██▎       | 188/834 [00:32<01:55,  5.57it/s]\n",
      " 23%|██▎       | 189/834 [00:32<01:54,  5.65it/s]\n",
      " 23%|██▎       | 190/834 [00:32<01:53,  5.66it/s]\n",
      "                                                 \n",
      "\n",
      " 23%|██▎       | 190/834 [00:32<01:53,  5.66it/s]\n",
      " 23%|██▎       | 191/834 [00:32<01:52,  5.72it/s]\n",
      " 23%|██▎       | 192/834 [00:32<01:49,  5.88it/s]\n",
      " 23%|██▎       | 193/834 [00:32<01:49,  5.87it/s]\n",
      " 23%|██▎       | 194/834 [00:33<01:48,  5.89it/s]\n",
      " 23%|██▎       | 195/834 [00:33<01:46,  6.01it/s]\n",
      " 24%|██▎       | 196/834 [00:33<01:45,  6.02it/s]\n",
      " 24%|██▎       | 197/834 [00:33<01:44,  6.08it/s]\n",
      " 24%|██▎       | 198/834 [00:33<01:43,  6.13it/s]\n",
      " 24%|██▍       | 199/834 [00:33<01:43,  6.12it/s]\n",
      " 24%|██▍       | 200/834 [00:34<01:42,  6.19it/s]\n",
      "                                                 \n",
      "\n",
      " 24%|██▍       | 200/834 [00:34<01:42,  6.19it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 37.03it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 24%|██▍       | 200/834 [00:34<01:42,  6.19it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 37.03it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 24%|██▍       | 201/834 [00:34<02:12,  4.79it/s]\n",
      " 24%|██▍       | 202/834 [00:34<02:03,  5.12it/s]\n",
      " 24%|██▍       | 203/834 [00:34<01:56,  5.44it/s]\n",
      " 24%|██▍       | 204/834 [00:34<01:50,  5.68it/s]\n",
      " 25%|██▍       | 205/834 [00:35<01:48,  5.81it/s]\n",
      " 25%|██▍       | 206/834 [00:35<01:46,  5.92it/s]\n",
      " 25%|██▍       | 207/834 [00:35<01:43,  6.05it/s]\n",
      " 25%|██▍       | 208/834 [00:35<01:44,  6.01it/s]\n",
      " 25%|██▌       | 209/834 [00:35<01:42,  6.08it/s]\n",
      " 25%|██▌       | 210/834 [00:35<01:41,  6.17it/s]\n",
      "                                                 \n",
      "\n",
      " 25%|██▌       | 210/834 [00:35<01:41,  6.17it/s]\n",
      " 25%|██▌       | 211/834 [00:36<01:43,  6.00it/s]\n",
      " 25%|██▌       | 212/834 [00:36<01:44,  5.97it/s]\n",
      " 26%|██▌       | 213/834 [00:36<01:41,  6.10it/s]\n",
      " 26%|██▌       | 214/834 [00:36<01:41,  6.11it/s]\n",
      " 26%|██▌       | 215/834 [00:36<01:40,  6.16it/s]\n",
      " 26%|██▌       | 216/834 [00:36<01:40,  6.16it/s]\n",
      " 26%|██▌       | 217/834 [00:37<01:39,  6.22it/s]\n",
      " 26%|██▌       | 218/834 [00:37<01:39,  6.19it/s]\n",
      " 26%|██▋       | 219/834 [00:37<01:38,  6.23it/s]\n",
      " 26%|██▋       | 220/834 [00:37<01:38,  6.22it/s]\n",
      "                                                 \n",
      "\n",
      " 26%|██▋       | 220/834 [00:37<01:38,  6.22it/s]\n",
      " 26%|██▋       | 221/834 [00:37<01:41,  6.04it/s]\n",
      " 27%|██▋       | 222/834 [00:37<01:40,  6.08it/s]\n",
      " 27%|██▋       | 223/834 [00:38<01:40,  6.11it/s]\n",
      " 27%|██▋       | 224/834 [00:38<01:38,  6.19it/s]\n",
      " 27%|██▋       | 225/834 [00:38<01:37,  6.23it/s]\n",
      " 27%|██▋       | 226/834 [00:38<01:37,  6.27it/s]\n",
      " 27%|██▋       | 227/834 [00:38<01:36,  6.31it/s]\n",
      " 27%|██▋       | 228/834 [00:38<01:35,  6.33it/s]\n",
      " 27%|██▋       | 229/834 [00:38<01:36,  6.29it/s]\n",
      " 28%|██▊       | 230/834 [00:39<01:35,  6.29it/s]\n",
      "                                                 \n",
      "\n",
      " 28%|██▊       | 230/834 [00:39<01:35,  6.29it/s]\n",
      " 28%|██▊       | 231/834 [00:39<01:39,  6.06it/s]\n",
      " 28%|██▊       | 232/834 [00:39<01:41,  5.93it/s]\n",
      " 28%|██▊       | 233/834 [00:39<01:41,  5.92it/s]\n",
      " 28%|██▊       | 234/834 [00:39<01:40,  5.94it/s]\n",
      " 28%|██▊       | 235/834 [00:39<01:43,  5.80it/s]\n",
      " 28%|██▊       | 236/834 [00:40<01:43,  5.79it/s]\n",
      " 28%|██▊       | 237/834 [00:40<01:40,  5.94it/s]\n",
      " 29%|██▊       | 238/834 [00:40<01:38,  6.06it/s]\n",
      " 29%|██▊       | 239/834 [00:40<01:37,  6.13it/s]\n",
      " 29%|██▉       | 240/834 [00:40<01:36,  6.13it/s]\n",
      "                                                 \n",
      "\n",
      " 29%|██▉       | 240/834 [00:40<01:36,  6.13it/s]\n",
      " 29%|██▉       | 241/834 [00:40<01:38,  6.03it/s]\n",
      " 29%|██▉       | 242/834 [00:41<01:37,  6.08it/s]\n",
      " 29%|██▉       | 243/834 [00:41<01:39,  5.92it/s]\n",
      " 29%|██▉       | 244/834 [00:41<01:41,  5.80it/s]\n",
      " 29%|██▉       | 245/834 [00:41<01:43,  5.67it/s]\n",
      " 29%|██▉       | 246/834 [00:41<01:43,  5.66it/s]\n",
      " 30%|██▉       | 247/834 [00:42<01:43,  5.67it/s]\n",
      " 30%|██▉       | 248/834 [00:42<01:42,  5.73it/s]\n",
      " 30%|██▉       | 249/834 [00:42<01:39,  5.85it/s]\n",
      " 30%|██▉       | 250/834 [00:42<01:37,  6.01it/s]\n",
      "                                                 \n",
      "\n",
      " 30%|██▉       | 250/834 [00:42<01:37,  6.01it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 37.04it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 30%|██▉       | 250/834 [00:42<01:37,  6.01it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 37.04it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 30%|███       | 251/834 [00:42<02:07,  4.56it/s]\n",
      " 30%|███       | 252/834 [00:43<01:56,  4.99it/s]\n",
      " 30%|███       | 253/834 [00:43<01:49,  5.31it/s]\n",
      " 30%|███       | 254/834 [00:43<01:44,  5.57it/s]\n",
      " 31%|███       | 255/834 [00:43<01:40,  5.76it/s]\n",
      " 31%|███       | 256/834 [00:43<01:38,  5.87it/s]\n",
      " 31%|███       | 257/834 [00:43<01:36,  5.98it/s]\n",
      " 31%|███       | 258/834 [00:43<01:34,  6.08it/s]\n",
      " 31%|███       | 259/834 [00:44<01:32,  6.19it/s]\n",
      " 31%|███       | 260/834 [00:44<01:31,  6.24it/s]\n",
      "                                                 \n",
      "\n",
      " 31%|███       | 260/834 [00:44<01:31,  6.24it/s]\n",
      " 31%|███▏      | 261/834 [00:44<01:31,  6.26it/s]\n",
      " 31%|███▏      | 262/834 [00:44<01:33,  6.12it/s]\n",
      " 32%|███▏      | 263/834 [00:44<01:35,  5.98it/s]\n",
      " 32%|███▏      | 264/834 [00:44<01:34,  6.06it/s]\n",
      " 32%|███▏      | 265/834 [00:45<01:34,  6.02it/s]\n",
      " 32%|███▏      | 266/834 [00:45<01:33,  6.10it/s]\n",
      " 32%|███▏      | 267/834 [00:45<01:32,  6.13it/s]\n",
      " 32%|███▏      | 268/834 [00:45<01:31,  6.17it/s]\n",
      " 32%|███▏      | 269/834 [00:45<01:31,  6.15it/s]\n",
      " 32%|███▏      | 270/834 [00:45<01:31,  6.20it/s]\n",
      "                                                 \n",
      "\n",
      " 32%|███▏      | 270/834 [00:45<01:31,  6.20it/s]\n",
      " 32%|███▏      | 271/834 [00:46<01:33,  6.01it/s]\n",
      " 33%|███▎      | 272/834 [00:46<01:33,  5.98it/s]\n",
      " 33%|███▎      | 273/834 [00:46<01:35,  5.87it/s]\n",
      " 33%|███▎      | 274/834 [00:46<01:33,  5.99it/s]\n",
      " 33%|███▎      | 275/834 [00:46<01:33,  6.00it/s]\n",
      " 33%|███▎      | 276/834 [00:46<01:31,  6.09it/s]\n",
      " 33%|███▎      | 277/834 [00:47<01:31,  6.07it/s]\n",
      " 33%|███▎      | 278/834 [00:47<01:30,  6.12it/s]\n",
      " 33%|███▎      | 279/834 [00:47<01:30,  6.16it/s]\n",
      " 34%|███▎      | 280/834 [00:47<01:29,  6.16it/s]\n",
      "                                                 \n",
      "\n",
      " 34%|███▎      | 280/834 [00:47<01:29,  6.16it/s]\n",
      " 34%|███▎      | 281/834 [00:47<01:32,  6.01it/s]\n",
      " 34%|███▍      | 282/834 [00:47<01:32,  6.00it/s]\n",
      " 34%|███▍      | 283/834 [00:48<01:32,  5.95it/s]\n",
      " 34%|███▍      | 284/834 [00:48<01:35,  5.75it/s]\n",
      " 34%|███▍      | 285/834 [00:48<01:35,  5.76it/s]\n",
      " 34%|███▍      | 286/834 [00:48<01:38,  5.58it/s]\n",
      " 34%|███▍      | 287/834 [00:48<01:36,  5.67it/s]\n",
      " 35%|███▍      | 288/834 [00:48<01:35,  5.71it/s]\n",
      " 35%|███▍      | 289/834 [00:49<01:34,  5.79it/s]\n",
      " 35%|███▍      | 290/834 [00:49<01:33,  5.83it/s]\n",
      "                                                 \n",
      "\n",
      " 35%|███▍      | 290/834 [00:49<01:33,  5.83it/s]\n",
      " 35%|███▍      | 291/834 [00:49<01:31,  5.91it/s]\n",
      " 35%|███▌      | 292/834 [00:49<01:35,  5.67it/s]\n",
      " 35%|███▌      | 293/834 [00:49<01:35,  5.66it/s]\n",
      " 35%|███▌      | 294/834 [00:50<01:33,  5.78it/s]\n",
      " 35%|███▌      | 295/834 [00:50<01:32,  5.86it/s]\n",
      " 35%|███▌      | 296/834 [00:50<01:32,  5.80it/s]\n",
      " 36%|███▌      | 297/834 [00:50<01:31,  5.85it/s]\n",
      " 36%|███▌      | 298/834 [00:50<01:32,  5.77it/s]\n",
      " 36%|███▌      | 299/834 [00:50<01:31,  5.84it/s]\n",
      " 36%|███▌      | 300/834 [00:51<01:31,  5.86it/s]\n",
      "                                                 \n",
      "\n",
      " 36%|███▌      | 300/834 [00:51<01:31,  5.86it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 30.51it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 36%|███▌      | 300/834 [00:51<01:31,  5.86it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 30.51it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 36%|███▌      | 301/834 [00:51<02:03,  4.31it/s]\n",
      " 36%|███▌      | 302/834 [00:51<01:53,  4.70it/s]\n",
      " 36%|███▋      | 303/834 [00:51<01:48,  4.90it/s]\n",
      " 36%|███▋      | 304/834 [00:51<01:42,  5.18it/s]\n",
      " 37%|███▋      | 305/834 [00:52<01:39,  5.31it/s]\n",
      " 37%|███▋      | 306/834 [00:52<01:39,  5.30it/s]\n",
      " 37%|███▋      | 307/834 [00:52<01:43,  5.09it/s]\n",
      " 37%|███▋      | 308/834 [00:52<01:46,  4.94it/s]\n",
      " 37%|███▋      | 309/834 [00:52<01:43,  5.07it/s]\n",
      " 37%|███▋      | 310/834 [00:53<01:39,  5.24it/s]\n",
      "                                                 \n",
      "\n",
      " 37%|███▋      | 310/834 [00:53<01:39,  5.24it/s]\n",
      " 37%|███▋      | 311/834 [00:53<01:35,  5.47it/s]\n",
      " 37%|███▋      | 312/834 [00:53<01:35,  5.48it/s]\n",
      " 38%|███▊      | 313/834 [00:53<01:32,  5.66it/s]\n",
      " 38%|███▊      | 314/834 [00:53<01:32,  5.61it/s]\n",
      " 38%|███▊      | 315/834 [00:53<01:29,  5.78it/s]\n",
      " 38%|███▊      | 316/834 [00:54<01:27,  5.90it/s]\n",
      " 38%|███▊      | 317/834 [00:54<01:25,  6.02it/s]\n",
      " 38%|███▊      | 318/834 [00:54<01:24,  6.08it/s]\n",
      " 38%|███▊      | 319/834 [00:54<01:23,  6.14it/s]\n",
      " 38%|███▊      | 320/834 [00:54<01:23,  6.15it/s]\n",
      "                                                 \n",
      "\n",
      " 38%|███▊      | 320/834 [00:54<01:23,  6.15it/s]\n",
      " 38%|███▊      | 321/834 [00:54<01:23,  6.16it/s]\n",
      " 39%|███▊      | 322/834 [00:55<01:22,  6.19it/s]\n",
      " 39%|███▊      | 323/834 [00:55<01:21,  6.23it/s]\n",
      " 39%|███▉      | 324/834 [00:55<01:22,  6.21it/s]\n",
      " 39%|███▉      | 325/834 [00:55<01:21,  6.23it/s]\n",
      " 39%|███▉      | 326/834 [00:55<01:23,  6.06it/s]\n",
      " 39%|███▉      | 327/834 [00:55<01:22,  6.14it/s]\n",
      " 39%|███▉      | 328/834 [00:56<01:22,  6.11it/s]\n",
      " 39%|███▉      | 329/834 [00:56<01:21,  6.18it/s]\n",
      " 40%|███▉      | 330/834 [00:56<01:23,  6.03it/s]\n",
      "                                                 \n",
      "\n",
      " 40%|███▉      | 330/834 [00:56<01:23,  6.03it/s]\n",
      " 40%|███▉      | 331/834 [00:56<01:23,  6.01it/s]\n",
      " 40%|███▉      | 332/834 [00:56<01:24,  5.97it/s]\n",
      " 40%|███▉      | 333/834 [00:56<01:23,  5.98it/s]\n",
      " 40%|████      | 334/834 [00:57<01:23,  6.01it/s]\n",
      " 40%|████      | 335/834 [00:57<01:24,  5.89it/s]\n",
      " 40%|████      | 336/834 [00:57<01:25,  5.80it/s]\n",
      " 40%|████      | 337/834 [00:57<01:24,  5.90it/s]\n",
      " 41%|████      | 338/834 [00:57<01:24,  5.90it/s]\n",
      " 41%|████      | 339/834 [00:57<01:24,  5.84it/s]\n",
      " 41%|████      | 340/834 [00:58<01:23,  5.90it/s]\n",
      "                                                 \n",
      "\n",
      " 41%|████      | 340/834 [00:58<01:23,  5.90it/s]\n",
      " 41%|████      | 341/834 [00:58<01:22,  5.96it/s]\n",
      " 41%|████      | 342/834 [00:58<01:22,  5.98it/s]\n",
      " 41%|████      | 343/834 [00:58<01:20,  6.08it/s]\n",
      " 41%|████      | 344/834 [00:58<01:19,  6.18it/s]\n",
      " 41%|████▏     | 345/834 [00:58<01:18,  6.20it/s]\n",
      " 41%|████▏     | 346/834 [00:59<01:19,  6.17it/s]\n",
      " 42%|████▏     | 347/834 [00:59<01:20,  6.07it/s]\n",
      " 42%|████▏     | 348/834 [00:59<01:21,  6.00it/s]\n",
      " 42%|████▏     | 349/834 [00:59<01:20,  6.06it/s]\n",
      " 42%|████▏     | 350/834 [00:59<01:19,  6.08it/s]\n",
      "                                                 \n",
      "\n",
      " 42%|████▏     | 350/834 [00:59<01:19,  6.08it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 36.53it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 42%|████▏     | 350/834 [00:59<01:19,  6.08it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 36.53it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 42%|████▏     | 351/834 [01:00<01:42,  4.73it/s]\n",
      " 42%|████▏     | 352/834 [01:00<01:34,  5.09it/s]\n",
      " 42%|████▏     | 353/834 [01:00<01:29,  5.37it/s]\n",
      " 42%|████▏     | 354/834 [01:00<01:27,  5.47it/s]\n",
      " 43%|████▎     | 355/834 [01:00<01:24,  5.67it/s]\n",
      " 43%|████▎     | 356/834 [01:00<01:21,  5.84it/s]\n",
      " 43%|████▎     | 357/834 [01:01<01:20,  5.95it/s]\n",
      " 43%|████▎     | 358/834 [01:01<01:19,  6.00it/s]\n",
      " 43%|████▎     | 359/834 [01:01<01:18,  6.05it/s]\n",
      " 43%|████▎     | 360/834 [01:01<01:17,  6.10it/s]\n",
      "                                                 \n",
      "\n",
      " 43%|████▎     | 360/834 [01:01<01:17,  6.10it/s]\n",
      " 43%|████▎     | 361/834 [01:01<01:18,  6.04it/s]\n",
      " 43%|████▎     | 362/834 [01:01<01:18,  6.01it/s]\n",
      " 44%|████▎     | 363/834 [01:02<01:18,  5.99it/s]\n",
      " 44%|████▎     | 364/834 [01:02<01:17,  6.07it/s]\n",
      " 44%|████▍     | 365/834 [01:02<01:16,  6.15it/s]\n",
      " 44%|████▍     | 366/834 [01:02<01:16,  6.10it/s]\n",
      " 44%|████▍     | 367/834 [01:02<01:16,  6.11it/s]\n",
      " 44%|████▍     | 368/834 [01:02<01:15,  6.15it/s]\n",
      " 44%|████▍     | 369/834 [01:02<01:15,  6.15it/s]\n",
      " 44%|████▍     | 370/834 [01:03<01:15,  6.18it/s]\n",
      "                                                 \n",
      "\n",
      " 44%|████▍     | 370/834 [01:03<01:15,  6.18it/s]\n",
      " 44%|████▍     | 371/834 [01:03<01:16,  6.09it/s]\n",
      " 45%|████▍     | 372/834 [01:03<01:16,  6.07it/s]\n",
      " 45%|████▍     | 373/834 [01:03<01:14,  6.17it/s]\n",
      " 45%|████▍     | 374/834 [01:03<01:15,  6.13it/s]\n",
      " 45%|████▍     | 375/834 [01:03<01:15,  6.10it/s]\n",
      " 45%|████▌     | 376/834 [01:04<01:15,  6.07it/s]\n",
      " 45%|████▌     | 377/834 [01:04<01:14,  6.10it/s]\n",
      " 45%|████▌     | 378/834 [01:04<01:14,  6.13it/s]\n",
      " 45%|████▌     | 379/834 [01:04<01:13,  6.16it/s]\n",
      " 46%|████▌     | 380/834 [01:04<01:13,  6.19it/s]\n",
      "                                                 \n",
      "\n",
      " 46%|████▌     | 380/834 [01:04<01:13,  6.19it/s]\n",
      " 46%|████▌     | 381/834 [01:04<01:15,  6.02it/s]\n",
      " 46%|████▌     | 382/834 [01:05<01:14,  6.04it/s]\n",
      " 46%|████▌     | 383/834 [01:05<01:14,  6.08it/s]\n",
      " 46%|████▌     | 384/834 [01:05<01:12,  6.18it/s]\n",
      " 46%|████▌     | 385/834 [01:05<01:13,  6.09it/s]\n",
      " 46%|████▋     | 386/834 [01:05<01:13,  6.06it/s]\n",
      " 46%|████▋     | 387/834 [01:05<01:13,  6.12it/s]\n",
      " 47%|████▋     | 388/834 [01:06<01:13,  6.06it/s]\n",
      " 47%|████▋     | 389/834 [01:06<01:14,  5.94it/s]\n",
      " 47%|████▋     | 390/834 [01:06<01:14,  5.96it/s]\n",
      "                                                 \n",
      "\n",
      " 47%|████▋     | 390/834 [01:06<01:14,  5.96it/s]\n",
      " 47%|████▋     | 391/834 [01:06<01:15,  5.88it/s]\n",
      " 47%|████▋     | 392/834 [01:06<01:13,  6.00it/s]\n",
      " 47%|████▋     | 393/834 [01:06<01:14,  5.96it/s]\n",
      " 47%|████▋     | 394/834 [01:07<01:12,  6.05it/s]\n",
      " 47%|████▋     | 395/834 [01:07<01:12,  6.09it/s]\n",
      " 47%|████▋     | 396/834 [01:07<01:11,  6.09it/s]\n",
      " 48%|████▊     | 397/834 [01:07<01:11,  6.09it/s]\n",
      " 48%|████▊     | 398/834 [01:07<01:13,  5.96it/s]\n",
      " 48%|████▊     | 399/834 [01:07<01:12,  6.02it/s]\n",
      " 48%|████▊     | 400/834 [01:08<01:14,  5.82it/s]\n",
      "                                                 \n",
      "\n",
      " 48%|████▊     | 400/834 [01:08<01:14,  5.82it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 37.38it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 48%|████▊     | 400/834 [01:08<01:14,  5.82it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 37.38it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 48%|████▊     | 401/834 [01:08<01:36,  4.50it/s]\n",
      " 48%|████▊     | 402/834 [01:08<01:30,  4.78it/s]\n",
      " 48%|████▊     | 403/834 [01:08<01:27,  4.93it/s]\n",
      " 48%|████▊     | 404/834 [01:08<01:23,  5.18it/s]\n",
      " 49%|████▊     | 405/834 [01:09<01:21,  5.28it/s]\n",
      " 49%|████▊     | 406/834 [01:09<01:19,  5.38it/s]\n",
      " 49%|████▉     | 407/834 [01:09<01:16,  5.58it/s]\n",
      " 49%|████▉     | 408/834 [01:09<01:16,  5.59it/s]\n",
      " 49%|████▉     | 409/834 [01:09<01:16,  5.57it/s]\n",
      " 49%|████▉     | 410/834 [01:10<01:14,  5.71it/s]\n",
      "                                                 \n",
      "\n",
      " 49%|████▉     | 410/834 [01:10<01:14,  5.71it/s]\n",
      " 49%|████▉     | 411/834 [01:10<01:15,  5.63it/s]\n",
      " 49%|████▉     | 412/834 [01:10<01:14,  5.67it/s]\n",
      " 50%|████▉     | 413/834 [01:10<01:13,  5.73it/s]\n",
      " 50%|████▉     | 414/834 [01:10<01:14,  5.67it/s]\n",
      " 50%|████▉     | 415/834 [01:10<01:19,  5.24it/s]\n",
      " 50%|████▉     | 416/834 [01:11<01:18,  5.30it/s]\n",
      " 50%|█████     | 417/834 [01:11<01:17,  5.37it/s]\n",
      " 50%|█████     | 418/834 [01:11<01:14,  5.59it/s]\n",
      " 50%|█████     | 419/834 [01:11<01:15,  5.49it/s]\n",
      " 50%|█████     | 420/834 [01:11<01:15,  5.51it/s]\n",
      "                                                 \n",
      "\n",
      " 50%|█████     | 420/834 [01:11<01:15,  5.51it/s]\n",
      " 50%|█████     | 421/834 [01:12<01:12,  5.69it/s]\n",
      " 51%|█████     | 422/834 [01:12<01:11,  5.75it/s]\n",
      " 51%|█████     | 423/834 [01:12<01:11,  5.78it/s]\n",
      " 51%|█████     | 424/834 [01:12<01:11,  5.73it/s]\n",
      " 51%|█████     | 425/834 [01:12<01:14,  5.50it/s]\n",
      " 51%|█████     | 426/834 [01:12<01:13,  5.55it/s]\n",
      " 51%|█████     | 427/834 [01:13<01:12,  5.61it/s]\n",
      " 51%|█████▏    | 428/834 [01:13<01:10,  5.79it/s]\n",
      " 51%|█████▏    | 429/834 [01:13<01:08,  5.88it/s]\n",
      " 52%|█████▏    | 430/834 [01:13<01:08,  5.93it/s]\n",
      "                                                 \n",
      "\n",
      " 52%|█████▏    | 430/834 [01:13<01:08,  5.93it/s]\n",
      " 52%|█████▏    | 431/834 [01:13<01:08,  5.89it/s]\n",
      " 52%|█████▏    | 432/834 [01:13<01:09,  5.82it/s]\n",
      " 52%|█████▏    | 433/834 [01:14<01:09,  5.81it/s]\n",
      " 52%|█████▏    | 434/834 [01:14<01:09,  5.79it/s]\n",
      " 52%|█████▏    | 435/834 [01:14<01:06,  5.96it/s]\n",
      " 52%|█████▏    | 436/834 [01:14<01:07,  5.89it/s]\n",
      " 52%|█████▏    | 437/834 [01:14<01:06,  5.99it/s]\n",
      " 53%|█████▎    | 438/834 [01:14<01:08,  5.81it/s]\n",
      " 53%|█████▎    | 439/834 [01:15<01:08,  5.79it/s]\n",
      " 53%|█████▎    | 440/834 [01:15<01:07,  5.82it/s]\n",
      "                                                 \n",
      "\n",
      " 53%|█████▎    | 440/834 [01:15<01:07,  5.82it/s]\n",
      " 53%|█████▎    | 441/834 [01:15<01:07,  5.78it/s]\n",
      " 53%|█████▎    | 442/834 [01:15<01:07,  5.79it/s]\n",
      " 53%|█████▎    | 443/834 [01:15<01:09,  5.66it/s]\n",
      " 53%|█████▎    | 444/834 [01:16<01:07,  5.74it/s]\n",
      " 53%|█████▎    | 445/834 [01:16<01:07,  5.75it/s]\n",
      " 53%|█████▎    | 446/834 [01:16<01:08,  5.65it/s]\n",
      " 54%|█████▎    | 447/834 [01:16<01:09,  5.54it/s]\n",
      " 54%|█████▎    | 448/834 [01:16<01:07,  5.75it/s]\n",
      " 54%|█████▍    | 449/834 [01:16<01:05,  5.89it/s]\n",
      " 54%|█████▍    | 450/834 [01:17<01:04,  5.98it/s]\n",
      "                                                 \n",
      "\n",
      " 54%|█████▍    | 450/834 [01:17<01:04,  5.98it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 37.33it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 54%|█████▍    | 450/834 [01:17<01:04,  5.98it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 37.33it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 54%|█████▍    | 451/834 [01:17<01:21,  4.73it/s]\n",
      " 54%|█████▍    | 452/834 [01:17<01:17,  4.92it/s]\n",
      " 54%|█████▍    | 453/834 [01:17<01:15,  5.03it/s]\n",
      " 54%|█████▍    | 454/834 [01:17<01:12,  5.24it/s]\n",
      " 55%|█████▍    | 455/834 [01:18<01:10,  5.40it/s]\n",
      " 55%|█████▍    | 456/834 [01:18<01:09,  5.42it/s]\n",
      " 55%|█████▍    | 457/834 [01:18<01:10,  5.32it/s]\n",
      " 55%|█████▍    | 458/834 [01:18<01:07,  5.54it/s]\n",
      " 55%|█████▌    | 459/834 [01:18<01:07,  5.56it/s]\n",
      " 55%|█████▌    | 460/834 [01:18<01:04,  5.79it/s]\n",
      "                                                 \n",
      "\n",
      " 55%|█████▌    | 460/834 [01:18<01:04,  5.79it/s]\n",
      " 55%|█████▌    | 461/834 [01:19<01:05,  5.69it/s]\n",
      " 55%|█████▌    | 462/834 [01:19<01:05,  5.67it/s]\n",
      " 56%|█████▌    | 463/834 [01:19<01:03,  5.85it/s]\n",
      " 56%|█████▌    | 464/834 [01:19<01:01,  5.98it/s]\n",
      " 56%|█████▌    | 465/834 [01:19<01:00,  6.06it/s]\n",
      " 56%|█████▌    | 466/834 [01:19<01:04,  5.69it/s]\n",
      " 56%|█████▌    | 467/834 [01:20<01:03,  5.73it/s]\n",
      " 56%|█████▌    | 468/834 [01:20<01:03,  5.72it/s]\n",
      " 56%|█████▌    | 469/834 [01:20<01:01,  5.91it/s]\n",
      " 56%|█████▋    | 470/834 [01:20<01:03,  5.75it/s]\n",
      "                                                 \n",
      "\n",
      " 56%|█████▋    | 470/834 [01:20<01:03,  5.75it/s]\n",
      " 56%|█████▋    | 471/834 [01:20<01:04,  5.62it/s]\n",
      " 57%|█████▋    | 472/834 [01:21<01:03,  5.68it/s]\n",
      " 57%|█████▋    | 473/834 [01:21<01:05,  5.54it/s]\n",
      " 57%|█████▋    | 474/834 [01:21<01:03,  5.65it/s]\n",
      " 57%|█████▋    | 475/834 [01:21<01:04,  5.58it/s]\n",
      " 57%|█████▋    | 476/834 [01:21<01:06,  5.39it/s]\n",
      " 57%|█████▋    | 477/834 [01:21<01:04,  5.53it/s]\n",
      " 57%|█████▋    | 478/834 [01:22<01:03,  5.58it/s]\n",
      " 57%|█████▋    | 479/834 [01:22<01:01,  5.79it/s]\n",
      " 58%|█████▊    | 480/834 [01:22<00:59,  5.93it/s]\n",
      "                                                 \n",
      "\n",
      " 58%|█████▊    | 480/834 [01:22<00:59,  5.93it/s]\n",
      " 58%|█████▊    | 481/834 [01:22<00:59,  5.98it/s]\n",
      " 58%|█████▊    | 482/834 [01:22<01:00,  5.83it/s]\n",
      " 58%|█████▊    | 483/834 [01:22<00:59,  5.92it/s]\n",
      " 58%|█████▊    | 484/834 [01:23<00:59,  5.86it/s]\n",
      " 58%|█████▊    | 485/834 [01:23<01:00,  5.72it/s]\n",
      " 58%|█████▊    | 486/834 [01:23<01:00,  5.80it/s]\n",
      " 58%|█████▊    | 487/834 [01:23<01:00,  5.69it/s]\n",
      " 59%|█████▊    | 488/834 [01:23<01:04,  5.38it/s]\n",
      " 59%|█████▊    | 489/834 [01:24<01:04,  5.35it/s]\n",
      " 59%|█████▉    | 490/834 [01:24<01:03,  5.43it/s]\n",
      "                                                 \n",
      "\n",
      " 59%|█████▉    | 490/834 [01:24<01:03,  5.43it/s]\n",
      " 59%|█████▉    | 491/834 [01:24<01:06,  5.12it/s]\n",
      " 59%|█████▉    | 492/834 [01:24<01:05,  5.26it/s]\n",
      " 59%|█████▉    | 493/834 [01:24<01:03,  5.35it/s]\n",
      " 59%|█████▉    | 494/834 [01:24<01:03,  5.36it/s]\n",
      " 59%|█████▉    | 495/834 [01:25<01:03,  5.36it/s]\n",
      " 59%|█████▉    | 496/834 [01:25<01:02,  5.43it/s]\n",
      " 60%|█████▉    | 497/834 [01:25<01:03,  5.31it/s]\n",
      " 60%|█████▉    | 498/834 [01:25<01:01,  5.47it/s]\n",
      " 60%|█████▉    | 499/834 [01:25<01:02,  5.33it/s]\n",
      " 60%|█████▉    | 500/834 [01:26<01:00,  5.52it/s]\n",
      "                                                 \n",
      "\n",
      " 60%|█████▉    | 500/834 [01:26<01:00,  5.52it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 29.35it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 60%|█████▉    | 500/834 [01:26<01:00,  5.52it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 29.35it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 60%|██████    | 501/834 [01:26<01:57,  2.83it/s]\n",
      " 60%|██████    | 502/834 [01:27<01:38,  3.37it/s]\n",
      " 60%|██████    | 503/834 [01:27<01:27,  3.80it/s]\n",
      " 60%|██████    | 504/834 [01:27<01:19,  4.15it/s]\n",
      " 61%|██████    | 505/834 [01:27<01:14,  4.44it/s]\n",
      " 61%|██████    | 506/834 [01:27<01:07,  4.84it/s]\n",
      " 61%|██████    | 507/834 [01:27<01:04,  5.08it/s]\n",
      " 61%|██████    | 508/834 [01:28<01:01,  5.31it/s]\n",
      " 61%|██████    | 509/834 [01:28<01:00,  5.39it/s]\n",
      " 61%|██████    | 510/834 [01:28<00:58,  5.51it/s]\n",
      "                                                 \n",
      "\n",
      " 61%|██████    | 510/834 [01:28<00:58,  5.51it/s]\n",
      " 61%|██████▏   | 511/834 [01:28<00:58,  5.50it/s]\n",
      " 61%|██████▏   | 512/834 [01:28<00:58,  5.53it/s]\n",
      " 62%|██████▏   | 513/834 [01:28<00:57,  5.60it/s]\n",
      " 62%|██████▏   | 514/834 [01:29<00:57,  5.54it/s]\n",
      " 62%|██████▏   | 515/834 [01:29<00:56,  5.62it/s]\n",
      " 62%|██████▏   | 516/834 [01:29<00:55,  5.73it/s]\n",
      " 62%|██████▏   | 517/834 [01:29<00:55,  5.74it/s]\n",
      " 62%|██████▏   | 518/834 [01:29<00:55,  5.66it/s]\n",
      " 62%|██████▏   | 519/834 [01:30<00:55,  5.64it/s]\n",
      " 62%|██████▏   | 520/834 [01:30<00:54,  5.72it/s]\n",
      "                                                 \n",
      "\n",
      " 62%|██████▏   | 520/834 [01:30<00:54,  5.72it/s]\n",
      " 62%|██████▏   | 521/834 [01:30<00:55,  5.65it/s]\n",
      " 63%|██████▎   | 522/834 [01:30<00:55,  5.66it/s]\n",
      " 63%|██████▎   | 523/834 [01:30<00:54,  5.66it/s]\n",
      " 63%|██████▎   | 524/834 [01:30<00:54,  5.65it/s]\n",
      " 63%|██████▎   | 525/834 [01:31<00:53,  5.74it/s]\n",
      " 63%|██████▎   | 526/834 [01:31<00:53,  5.78it/s]\n",
      " 63%|██████▎   | 527/834 [01:31<00:52,  5.80it/s]\n",
      " 63%|██████▎   | 528/834 [01:31<00:51,  5.92it/s]\n",
      " 63%|██████▎   | 529/834 [01:31<00:52,  5.82it/s]\n",
      " 64%|██████▎   | 530/834 [01:31<00:53,  5.66it/s]\n",
      "                                                 \n",
      "\n",
      " 64%|██████▎   | 530/834 [01:31<00:53,  5.66it/s]\n",
      " 64%|██████▎   | 531/834 [01:32<00:52,  5.73it/s]\n",
      " 64%|██████▍   | 532/834 [01:32<00:52,  5.79it/s]\n",
      " 64%|██████▍   | 533/834 [01:32<00:52,  5.76it/s]\n",
      " 64%|██████▍   | 534/834 [01:32<00:52,  5.75it/s]\n",
      " 64%|██████▍   | 535/834 [01:32<00:51,  5.79it/s]\n",
      " 64%|██████▍   | 536/834 [01:32<00:51,  5.75it/s]\n",
      " 64%|██████▍   | 537/834 [01:33<00:51,  5.75it/s]\n",
      " 65%|██████▍   | 538/834 [01:33<00:51,  5.69it/s]\n",
      " 65%|██████▍   | 539/834 [01:33<00:51,  5.70it/s]\n",
      " 65%|██████▍   | 540/834 [01:33<00:50,  5.80it/s]\n",
      "                                                 \n",
      "\n",
      " 65%|██████▍   | 540/834 [01:33<00:50,  5.80it/s]\n",
      " 65%|██████▍   | 541/834 [01:33<00:51,  5.74it/s]\n",
      " 65%|██████▍   | 542/834 [01:34<00:51,  5.66it/s]\n",
      " 65%|██████▌   | 543/834 [01:34<00:49,  5.84it/s]\n",
      " 65%|██████▌   | 544/834 [01:34<00:49,  5.84it/s]\n",
      " 65%|██████▌   | 545/834 [01:34<00:50,  5.72it/s]\n",
      " 65%|██████▌   | 546/834 [01:34<00:48,  5.88it/s]\n",
      " 66%|██████▌   | 547/834 [01:34<00:49,  5.83it/s]\n",
      " 66%|██████▌   | 548/834 [01:35<00:50,  5.71it/s]\n",
      " 66%|██████▌   | 549/834 [01:35<00:51,  5.56it/s]\n",
      " 66%|██████▌   | 550/834 [01:35<00:52,  5.40it/s]\n",
      "                                                 \n",
      "\n",
      " 66%|██████▌   | 550/834 [01:35<00:52,  5.40it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      " 75%|███████▌  | 3/4 [00:00<00:00, 26.09it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 66%|██████▌   | 550/834 [01:35<00:52,  5.40it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 26.09it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 66%|██████▌   | 551/834 [01:35<01:09,  4.05it/s]\n",
      " 66%|██████▌   | 552/834 [01:36<01:03,  4.47it/s]\n",
      " 66%|██████▋   | 553/834 [01:36<01:00,  4.64it/s]\n",
      " 66%|██████▋   | 554/834 [01:36<00:57,  4.86it/s]\n",
      " 67%|██████▋   | 555/834 [01:36<00:54,  5.14it/s]\n",
      " 67%|██████▋   | 556/834 [01:36<00:52,  5.26it/s]\n",
      " 67%|██████▋   | 557/834 [01:36<00:50,  5.49it/s]\n",
      " 67%|██████▋   | 558/834 [01:37<00:49,  5.59it/s]\n",
      " 67%|██████▋   | 559/834 [01:37<00:47,  5.75it/s]\n",
      " 67%|██████▋   | 560/834 [01:37<00:46,  5.84it/s]\n",
      "                                                 \n",
      "\n",
      " 67%|██████▋   | 560/834 [01:37<00:46,  5.84it/s]\n",
      " 67%|██████▋   | 561/834 [01:37<00:48,  5.67it/s]\n",
      " 67%|██████▋   | 562/834 [01:37<00:48,  5.63it/s]\n",
      " 68%|██████▊   | 563/834 [01:37<00:46,  5.82it/s]\n",
      " 68%|██████▊   | 564/834 [01:38<00:46,  5.86it/s]\n",
      " 68%|██████▊   | 565/834 [01:38<00:46,  5.81it/s]\n",
      " 68%|██████▊   | 566/834 [01:38<00:47,  5.68it/s]\n",
      " 68%|██████▊   | 567/834 [01:38<00:48,  5.53it/s]\n",
      " 68%|██████▊   | 568/834 [01:38<00:46,  5.70it/s]\n",
      " 68%|██████▊   | 569/834 [01:38<00:45,  5.84it/s]\n",
      " 68%|██████▊   | 570/834 [01:39<00:48,  5.45it/s]\n",
      "                                                 \n",
      "\n",
      " 68%|██████▊   | 570/834 [01:39<00:48,  5.45it/s]\n",
      " 68%|██████▊   | 571/834 [01:39<00:46,  5.60it/s]\n",
      " 69%|██████▊   | 572/834 [01:39<00:46,  5.65it/s]\n",
      " 69%|██████▊   | 573/834 [01:39<00:45,  5.77it/s]\n",
      " 69%|██████▉   | 574/834 [01:39<00:44,  5.84it/s]\n",
      " 69%|██████▉   | 575/834 [01:40<00:44,  5.87it/s]\n",
      " 69%|██████▉   | 576/834 [01:40<00:43,  5.95it/s]\n",
      " 69%|██████▉   | 577/834 [01:40<00:43,  5.86it/s]\n",
      " 69%|██████▉   | 578/834 [01:40<00:44,  5.72it/s]\n",
      " 69%|██████▉   | 579/834 [01:40<00:43,  5.89it/s]\n",
      " 70%|██████▉   | 580/834 [01:40<00:43,  5.83it/s]\n",
      "                                                 \n",
      "\n",
      " 70%|██████▉   | 580/834 [01:40<00:43,  5.83it/s]\n",
      " 70%|██████▉   | 581/834 [01:41<00:43,  5.79it/s]\n",
      " 70%|██████▉   | 582/834 [01:41<00:42,  5.94it/s]\n",
      " 70%|██████▉   | 583/834 [01:41<00:41,  5.98it/s]\n",
      " 70%|███████   | 584/834 [01:41<00:42,  5.89it/s]\n",
      " 70%|███████   | 585/834 [01:41<00:41,  6.00it/s]\n",
      " 70%|███████   | 586/834 [01:41<00:41,  5.98it/s]\n",
      " 70%|███████   | 587/834 [01:42<00:40,  6.03it/s]\n",
      " 71%|███████   | 588/834 [01:42<00:40,  6.08it/s]\n",
      " 71%|███████   | 589/834 [01:42<00:39,  6.13it/s]\n",
      " 71%|███████   | 590/834 [01:42<00:40,  6.08it/s]\n",
      "                                                 \n",
      "\n",
      " 71%|███████   | 590/834 [01:42<00:40,  6.08it/s]\n",
      " 71%|███████   | 591/834 [01:42<00:40,  6.04it/s]\n",
      " 71%|███████   | 592/834 [01:42<00:39,  6.16it/s]\n",
      " 71%|███████   | 593/834 [01:43<00:38,  6.19it/s]\n",
      " 71%|███████   | 594/834 [01:43<00:38,  6.22it/s]\n",
      " 71%|███████▏  | 595/834 [01:43<00:38,  6.26it/s]\n",
      " 71%|███████▏  | 596/834 [01:43<00:37,  6.27it/s]\n",
      " 72%|███████▏  | 597/834 [01:43<00:37,  6.29it/s]\n",
      " 72%|███████▏  | 598/834 [01:43<00:38,  6.19it/s]\n",
      " 72%|███████▏  | 599/834 [01:43<00:37,  6.23it/s]\n",
      " 72%|███████▏  | 600/834 [01:44<00:37,  6.21it/s]\n",
      "                                                 \n",
      "\n",
      " 72%|███████▏  | 600/834 [01:44<00:37,  6.21it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 37.04it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 72%|███████▏  | 600/834 [01:44<00:37,  6.21it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 37.04it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 72%|███████▏  | 601/834 [01:44<00:48,  4.76it/s]\n",
      " 72%|███████▏  | 602/834 [01:44<00:45,  5.05it/s]\n",
      " 72%|███████▏  | 603/834 [01:44<00:43,  5.35it/s]\n",
      " 72%|███████▏  | 604/834 [01:44<00:40,  5.62it/s]\n",
      " 73%|███████▎  | 605/834 [01:45<00:39,  5.80it/s]\n",
      " 73%|███████▎  | 606/834 [01:45<00:38,  5.86it/s]\n",
      " 73%|███████▎  | 607/834 [01:45<00:38,  5.97it/s]\n",
      " 73%|███████▎  | 608/834 [01:45<00:37,  6.01it/s]\n",
      " 73%|███████▎  | 609/834 [01:45<00:36,  6.10it/s]\n",
      " 73%|███████▎  | 610/834 [01:45<00:36,  6.17it/s]\n",
      "                                                 \n",
      "\n",
      " 73%|███████▎  | 610/834 [01:45<00:36,  6.17it/s]\n",
      " 73%|███████▎  | 611/834 [01:46<00:37,  5.90it/s]\n",
      " 73%|███████▎  | 612/834 [01:46<00:37,  5.94it/s]\n",
      " 74%|███████▎  | 613/834 [01:46<00:36,  6.04it/s]\n",
      " 74%|███████▎  | 614/834 [01:46<00:36,  6.01it/s]\n",
      " 74%|███████▎  | 615/834 [01:46<00:36,  6.08it/s]\n",
      " 74%|███████▍  | 616/834 [01:46<00:35,  6.10it/s]\n",
      " 74%|███████▍  | 617/834 [01:47<00:36,  5.96it/s]\n",
      " 74%|███████▍  | 618/834 [01:47<00:36,  5.98it/s]\n",
      " 74%|███████▍  | 619/834 [01:47<00:35,  6.05it/s]\n",
      " 74%|███████▍  | 620/834 [01:47<00:35,  6.06it/s]\n",
      "                                                 \n",
      "\n",
      " 74%|███████▍  | 620/834 [01:47<00:35,  6.06it/s]\n",
      " 74%|███████▍  | 621/834 [01:47<00:36,  5.83it/s]\n",
      " 75%|███████▍  | 622/834 [01:47<00:35,  5.93it/s]\n",
      " 75%|███████▍  | 623/834 [01:48<00:35,  5.93it/s]\n",
      " 75%|███████▍  | 624/834 [01:48<00:35,  5.99it/s]\n",
      " 75%|███████▍  | 625/834 [01:48<00:34,  6.05it/s]\n",
      " 75%|███████▌  | 626/834 [01:48<00:34,  6.03it/s]\n",
      " 75%|███████▌  | 627/834 [01:48<00:34,  5.93it/s]\n",
      " 75%|███████▌  | 628/834 [01:48<00:34,  6.00it/s]\n",
      " 75%|███████▌  | 629/834 [01:49<00:33,  6.06it/s]\n",
      " 76%|███████▌  | 630/834 [01:49<00:33,  6.13it/s]\n",
      "                                                 \n",
      "\n",
      " 76%|███████▌  | 630/834 [01:49<00:33,  6.13it/s]\n",
      " 76%|███████▌  | 631/834 [01:49<00:34,  5.95it/s]\n",
      " 76%|███████▌  | 632/834 [01:49<00:33,  6.05it/s]\n",
      " 76%|███████▌  | 633/834 [01:49<00:33,  5.97it/s]\n",
      " 76%|███████▌  | 634/834 [01:49<00:32,  6.08it/s]\n",
      " 76%|███████▌  | 635/834 [01:50<00:32,  6.13it/s]\n",
      " 76%|███████▋  | 636/834 [01:50<00:32,  6.18it/s]\n",
      " 76%|███████▋  | 637/834 [01:50<00:31,  6.18it/s]\n",
      " 76%|███████▋  | 638/834 [01:50<00:31,  6.22it/s]\n",
      " 77%|███████▋  | 639/834 [01:50<00:31,  6.24it/s]\n",
      " 77%|███████▋  | 640/834 [01:50<00:31,  6.23it/s]\n",
      "                                                 \n",
      "\n",
      " 77%|███████▋  | 640/834 [01:50<00:31,  6.23it/s]\n",
      " 77%|███████▋  | 641/834 [01:51<00:32,  6.01it/s]\n",
      " 77%|███████▋  | 642/834 [01:51<00:32,  5.96it/s]\n",
      " 77%|███████▋  | 643/834 [01:51<00:31,  6.04it/s]\n",
      " 77%|███████▋  | 644/834 [01:51<00:31,  6.08it/s]\n",
      " 77%|███████▋  | 645/834 [01:51<00:31,  6.08it/s]\n",
      " 77%|███████▋  | 646/834 [01:51<00:30,  6.08it/s]\n",
      " 78%|███████▊  | 647/834 [01:52<00:31,  5.99it/s]\n",
      " 78%|███████▊  | 648/834 [01:52<00:30,  6.01it/s]\n",
      " 78%|███████▊  | 649/834 [01:52<00:30,  5.99it/s]\n",
      " 78%|███████▊  | 650/834 [01:52<00:30,  6.07it/s]\n",
      "                                                 \n",
      "\n",
      " 78%|███████▊  | 650/834 [01:52<00:30,  6.07it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 37.69it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 78%|███████▊  | 650/834 [01:52<00:30,  6.07it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 37.69it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 78%|███████▊  | 651/834 [01:52<00:40,  4.57it/s]\n",
      " 78%|███████▊  | 652/834 [01:53<00:36,  4.97it/s]\n",
      " 78%|███████▊  | 653/834 [01:53<00:34,  5.24it/s]\n",
      " 78%|███████▊  | 654/834 [01:53<00:32,  5.54it/s]\n",
      " 79%|███████▊  | 655/834 [01:53<00:31,  5.74it/s]\n",
      " 79%|███████▊  | 656/834 [01:53<00:30,  5.88it/s]\n",
      " 79%|███████▉  | 657/834 [01:53<00:29,  5.97it/s]\n",
      " 79%|███████▉  | 658/834 [01:54<00:28,  6.08it/s]\n",
      " 79%|███████▉  | 659/834 [01:54<00:28,  6.15it/s]\n",
      " 79%|███████▉  | 660/834 [01:54<00:27,  6.24it/s]\n",
      "                                                 \n",
      "\n",
      " 79%|███████▉  | 660/834 [01:54<00:27,  6.24it/s]\n",
      " 79%|███████▉  | 661/834 [01:54<00:28,  6.14it/s]\n",
      " 79%|███████▉  | 662/834 [01:54<00:28,  6.02it/s]\n",
      " 79%|███████▉  | 663/834 [01:54<00:28,  6.09it/s]\n",
      " 80%|███████▉  | 664/834 [01:55<00:29,  5.82it/s]\n",
      " 80%|███████▉  | 665/834 [01:55<00:28,  5.95it/s]\n",
      " 80%|███████▉  | 666/834 [01:55<00:28,  6.00it/s]\n",
      " 80%|███████▉  | 667/834 [01:55<00:27,  6.07it/s]\n",
      " 80%|████████  | 668/834 [01:55<00:27,  6.14it/s]\n",
      " 80%|████████  | 669/834 [01:55<00:27,  6.00it/s]\n",
      " 80%|████████  | 670/834 [01:55<00:26,  6.08it/s]\n",
      "                                                 \n",
      "\n",
      " 80%|████████  | 670/834 [01:55<00:26,  6.08it/s]\n",
      " 80%|████████  | 671/834 [01:56<00:26,  6.07it/s]\n",
      " 81%|████████  | 672/834 [01:56<00:26,  6.13it/s]\n",
      " 81%|████████  | 673/834 [01:56<00:26,  6.11it/s]\n",
      " 81%|████████  | 674/834 [01:56<00:25,  6.16it/s]\n",
      " 81%|████████  | 675/834 [01:56<00:26,  6.11it/s]\n",
      " 81%|████████  | 676/834 [01:56<00:25,  6.17it/s]\n",
      " 81%|████████  | 677/834 [01:57<00:25,  6.18it/s]\n",
      " 81%|████████▏ | 678/834 [01:57<00:25,  6.19it/s]\n",
      " 81%|████████▏ | 679/834 [01:57<00:24,  6.21it/s]\n",
      " 82%|████████▏ | 680/834 [01:57<00:25,  6.00it/s]\n",
      "                                                 \n",
      "\n",
      " 82%|████████▏ | 680/834 [01:57<00:25,  6.00it/s]\n",
      " 82%|████████▏ | 681/834 [01:57<00:27,  5.65it/s]\n",
      " 82%|████████▏ | 682/834 [01:57<00:26,  5.82it/s]\n",
      " 82%|████████▏ | 683/834 [01:58<00:25,  5.85it/s]\n",
      " 82%|████████▏ | 684/834 [01:58<00:25,  5.96it/s]\n",
      " 82%|████████▏ | 685/834 [01:58<00:25,  5.92it/s]\n",
      " 82%|████████▏ | 686/834 [01:58<00:25,  5.81it/s]\n",
      " 82%|████████▏ | 687/834 [01:58<00:25,  5.76it/s]\n",
      " 82%|████████▏ | 688/834 [01:59<00:25,  5.83it/s]\n",
      " 83%|████████▎ | 689/834 [01:59<00:24,  5.97it/s]\n",
      " 83%|████████▎ | 690/834 [01:59<00:23,  6.04it/s]\n",
      "                                                 \n",
      "\n",
      " 83%|████████▎ | 690/834 [01:59<00:23,  6.04it/s]\n",
      " 83%|████████▎ | 691/834 [01:59<00:24,  5.95it/s]\n",
      " 83%|████████▎ | 692/834 [01:59<00:23,  5.98it/s]\n",
      " 83%|████████▎ | 693/834 [01:59<00:23,  6.03it/s]\n",
      " 83%|████████▎ | 694/834 [02:00<00:23,  6.02it/s]\n",
      " 83%|████████▎ | 695/834 [02:00<00:22,  6.14it/s]\n",
      " 83%|████████▎ | 696/834 [02:00<00:22,  6.16it/s]\n",
      " 84%|████████▎ | 697/834 [02:00<00:22,  6.17it/s]\n",
      " 84%|████████▎ | 698/834 [02:00<00:22,  6.04it/s]\n",
      " 84%|████████▍ | 699/834 [02:00<00:22,  5.95it/s]\n",
      " 84%|████████▍ | 700/834 [02:00<00:22,  6.00it/s]\n",
      "                                                 \n",
      "\n",
      " 84%|████████▍ | 700/834 [02:01<00:22,  6.00it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 37.04it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 84%|████████▍ | 700/834 [02:01<00:22,  6.00it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 37.04it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 84%|████████▍ | 701/834 [02:01<00:28,  4.62it/s]\n",
      " 84%|████████▍ | 702/834 [02:01<00:26,  5.00it/s]\n",
      " 84%|████████▍ | 703/834 [02:01<00:24,  5.28it/s]\n",
      " 84%|████████▍ | 704/834 [02:01<00:23,  5.47it/s]\n",
      " 85%|████████▍ | 705/834 [02:01<00:22,  5.65it/s]\n",
      " 85%|████████▍ | 706/834 [02:02<00:21,  5.82it/s]\n",
      " 85%|████████▍ | 707/834 [02:02<00:21,  5.84it/s]\n",
      " 85%|████████▍ | 708/834 [02:02<00:21,  5.94it/s]\n",
      " 85%|████████▌ | 709/834 [02:02<00:20,  5.98it/s]\n",
      " 85%|████████▌ | 710/834 [02:02<00:21,  5.88it/s]\n",
      "                                                 \n",
      "\n",
      " 85%|████████▌ | 710/834 [02:02<00:21,  5.88it/s]\n",
      " 85%|████████▌ | 711/834 [02:02<00:20,  5.97it/s]\n",
      " 85%|████████▌ | 712/834 [02:03<00:19,  6.10it/s]\n",
      " 85%|████████▌ | 713/834 [02:03<00:20,  6.01it/s]\n",
      " 86%|████████▌ | 714/834 [02:03<00:19,  6.04it/s]\n",
      " 86%|████████▌ | 715/834 [02:03<00:19,  6.14it/s]\n",
      " 86%|████████▌ | 716/834 [02:03<00:19,  6.08it/s]\n",
      " 86%|████████▌ | 717/834 [02:03<00:19,  6.06it/s]\n",
      " 86%|████████▌ | 718/834 [02:04<00:19,  5.97it/s]\n",
      " 86%|████████▌ | 719/834 [02:04<00:18,  6.09it/s]\n",
      " 86%|████████▋ | 720/834 [02:04<00:18,  6.15it/s]\n",
      "                                                 \n",
      "\n",
      " 86%|████████▋ | 720/834 [02:04<00:18,  6.15it/s]\n",
      " 86%|████████▋ | 721/834 [02:04<00:18,  6.18it/s]\n",
      " 87%|████████▋ | 722/834 [02:04<00:17,  6.25it/s]\n",
      " 87%|████████▋ | 723/834 [02:04<00:17,  6.21it/s]\n",
      " 87%|████████▋ | 724/834 [02:05<00:17,  6.25it/s]\n",
      " 87%|████████▋ | 725/834 [02:05<00:17,  6.21it/s]\n",
      " 87%|████████▋ | 726/834 [02:05<00:17,  6.24it/s]\n",
      " 87%|████████▋ | 727/834 [02:05<00:17,  6.23it/s]\n",
      " 87%|████████▋ | 728/834 [02:05<00:17,  6.24it/s]\n",
      " 87%|████████▋ | 729/834 [02:05<00:16,  6.25it/s]\n",
      " 88%|████████▊ | 730/834 [02:06<00:16,  6.24it/s]\n",
      "                                                 \n",
      "\n",
      " 88%|████████▊ | 730/834 [02:06<00:16,  6.24it/s]\n",
      " 88%|████████▊ | 731/834 [02:06<00:16,  6.13it/s]\n",
      " 88%|████████▊ | 732/834 [02:06<00:16,  6.20it/s]\n",
      " 88%|████████▊ | 733/834 [02:06<00:16,  6.17it/s]\n",
      " 88%|████████▊ | 734/834 [02:06<00:16,  6.22it/s]\n",
      " 88%|████████▊ | 735/834 [02:06<00:15,  6.26it/s]\n",
      " 88%|████████▊ | 736/834 [02:07<00:15,  6.29it/s]\n",
      " 88%|████████▊ | 737/834 [02:07<00:15,  6.27it/s]\n",
      " 88%|████████▊ | 738/834 [02:07<00:15,  6.27it/s]\n",
      " 89%|████████▊ | 739/834 [02:07<00:15,  6.25it/s]\n",
      " 89%|████████▊ | 740/834 [02:07<00:15,  6.26it/s]\n",
      "                                                 \n",
      "\n",
      " 89%|████████▊ | 740/834 [02:07<00:15,  6.26it/s]\n",
      " 89%|████████▉ | 741/834 [02:07<00:15,  5.99it/s]\n",
      " 89%|████████▉ | 742/834 [02:07<00:15,  6.03it/s]\n",
      " 89%|████████▉ | 743/834 [02:08<00:14,  6.11it/s]\n",
      " 89%|████████▉ | 744/834 [02:08<00:14,  6.16it/s]\n",
      " 89%|████████▉ | 745/834 [02:08<00:14,  6.15it/s]\n",
      " 89%|████████▉ | 746/834 [02:08<00:14,  6.03it/s]\n",
      " 90%|████████▉ | 747/834 [02:08<00:14,  5.89it/s]\n",
      " 90%|████████▉ | 748/834 [02:08<00:14,  6.04it/s]\n",
      " 90%|████████▉ | 749/834 [02:09<00:14,  6.06it/s]\n",
      " 90%|████████▉ | 750/834 [02:09<00:13,  6.11it/s]\n",
      "                                                 \n",
      "\n",
      " 90%|████████▉ | 750/834 [02:09<00:13,  6.11it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 36.00it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 90%|████████▉ | 750/834 [02:09<00:13,  6.11it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 36.00it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 90%|█████████ | 751/834 [02:09<00:18,  4.51it/s]\n",
      " 90%|█████████ | 752/834 [02:09<00:16,  4.93it/s]\n",
      " 90%|█████████ | 753/834 [02:10<00:15,  5.06it/s]\n",
      " 90%|█████████ | 754/834 [02:10<00:15,  5.32it/s]\n",
      " 91%|█████████ | 755/834 [02:10<00:14,  5.62it/s]\n",
      " 91%|█████████ | 756/834 [02:10<00:13,  5.71it/s]\n",
      " 91%|█████████ | 757/834 [02:10<00:13,  5.77it/s]\n",
      " 91%|█████████ | 758/834 [02:10<00:13,  5.80it/s]\n",
      " 91%|█████████ | 759/834 [02:11<00:12,  5.77it/s]\n",
      " 91%|█████████ | 760/834 [02:11<00:12,  5.82it/s]\n",
      "                                                 \n",
      "\n",
      " 91%|█████████ | 760/834 [02:11<00:12,  5.82it/s]\n",
      " 91%|█████████ | 761/834 [02:11<00:12,  5.77it/s]\n",
      " 91%|█████████▏| 762/834 [02:11<00:12,  5.68it/s]\n",
      " 91%|█████████▏| 763/834 [02:11<00:12,  5.68it/s]\n",
      " 92%|█████████▏| 764/834 [02:11<00:12,  5.71it/s]\n",
      " 92%|█████████▏| 765/834 [02:12<00:11,  5.83it/s]\n",
      " 92%|█████████▏| 766/834 [02:12<00:11,  5.79it/s]\n",
      " 92%|█████████▏| 767/834 [02:12<00:11,  5.67it/s]\n",
      " 92%|█████████▏| 768/834 [02:12<00:11,  5.84it/s]\n",
      " 92%|█████████▏| 769/834 [02:12<00:11,  5.66it/s]\n",
      " 92%|█████████▏| 770/834 [02:12<00:11,  5.74it/s]\n",
      "                                                 \n",
      "\n",
      " 92%|█████████▏| 770/834 [02:12<00:11,  5.74it/s]\n",
      " 92%|█████████▏| 771/834 [02:13<00:11,  5.48it/s]\n",
      " 93%|█████████▎| 772/834 [02:13<00:10,  5.68it/s]\n",
      " 93%|█████████▎| 773/834 [02:13<00:10,  5.75it/s]\n",
      " 93%|█████████▎| 774/834 [02:13<00:10,  5.71it/s]\n",
      " 93%|█████████▎| 775/834 [02:13<00:10,  5.81it/s]\n",
      " 93%|█████████▎| 776/834 [02:13<00:09,  5.92it/s]\n",
      " 93%|█████████▎| 777/834 [02:14<00:09,  5.81it/s]\n",
      " 93%|█████████▎| 778/834 [02:14<00:09,  5.79it/s]\n",
      " 93%|█████████▎| 779/834 [02:14<00:09,  5.81it/s]\n",
      " 94%|█████████▎| 780/834 [02:14<00:09,  5.85it/s]\n",
      "                                                 \n",
      "\n",
      " 94%|█████████▎| 780/834 [02:14<00:09,  5.85it/s]\n",
      " 94%|█████████▎| 781/834 [02:14<00:09,  5.84it/s]\n",
      " 94%|█████████▍| 782/834 [02:15<00:08,  5.85it/s]\n",
      " 94%|█████████▍| 783/834 [02:15<00:08,  5.86it/s]\n",
      " 94%|█████████▍| 784/834 [02:15<00:08,  5.92it/s]\n",
      " 94%|█████████▍| 785/834 [02:15<00:08,  5.86it/s]\n",
      " 94%|█████████▍| 786/834 [02:15<00:08,  5.68it/s]\n",
      " 94%|█████████▍| 787/834 [02:15<00:08,  5.65it/s]\n",
      " 94%|█████████▍| 788/834 [02:16<00:08,  5.64it/s]\n",
      " 95%|█████████▍| 789/834 [02:16<00:07,  5.77it/s]\n",
      " 95%|█████████▍| 790/834 [02:16<00:07,  5.88it/s]\n",
      "                                                 \n",
      "\n",
      " 95%|█████████▍| 790/834 [02:16<00:07,  5.88it/s]\n",
      " 95%|█████████▍| 791/834 [02:16<00:07,  5.89it/s]\n",
      " 95%|█████████▍| 792/834 [02:16<00:06,  6.03it/s]\n",
      " 95%|█████████▌| 793/834 [02:16<00:06,  5.99it/s]\n",
      " 95%|█████████▌| 794/834 [02:17<00:06,  5.90it/s]\n",
      " 95%|█████████▌| 795/834 [02:17<00:06,  5.76it/s]\n",
      " 95%|█████████▌| 796/834 [02:17<00:06,  5.78it/s]\n",
      " 96%|█████████▌| 797/834 [02:17<00:06,  5.71it/s]\n",
      " 96%|█████████▌| 798/834 [02:17<00:06,  5.55it/s]\n",
      " 96%|█████████▌| 799/834 [02:17<00:06,  5.69it/s]\n",
      " 96%|█████████▌| 800/834 [02:18<00:06,  5.63it/s]\n",
      "                                                 \n",
      "\n",
      " 96%|█████████▌| 800/834 [02:18<00:06,  5.63it/s]\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 33.03it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 96%|█████████▌| 800/834 [02:18<00:06,  5.63it/s]\n",
      "\n",
      "100%|██████████| 4/4 [00:00<00:00, 33.03it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[Ad:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "\n",
      " 96%|█████████▌| 801/834 [02:18<00:07,  4.32it/s]\n",
      " 96%|█████████▌| 802/834 [02:18<00:06,  4.71it/s]\n",
      " 96%|█████████▋| 803/834 [02:18<00:06,  4.97it/s]\n",
      " 96%|█████████▋| 804/834 [02:19<00:05,  5.23it/s]\n",
      " 97%|█████████▋| 805/834 [02:19<00:05,  5.31it/s]\n",
      " 97%|█████████▋| 806/834 [02:19<00:05,  5.48it/s]\n",
      " 97%|█████████▋| 807/834 [02:19<00:04,  5.56it/s]\n",
      " 97%|█████████▋| 808/834 [02:19<00:04,  5.55it/s]\n",
      " 97%|█████████▋| 809/834 [02:19<00:04,  5.54it/s]\n",
      " 97%|█████████▋| 810/834 [02:20<00:04,  5.45it/s]\n",
      "                                                 \n",
      "\n",
      " 97%|█████████▋| 810/834 [02:20<00:04,  5.45it/s]\n",
      " 97%|█████████▋| 811/834 [02:20<00:04,  5.58it/s]\n",
      " 97%|█████████▋| 812/834 [02:20<00:03,  5.57it/s]\n",
      " 97%|█████████▋| 813/834 [02:20<00:03,  5.40it/s]\n",
      " 98%|█████████▊| 814/834 [02:20<00:03,  5.40it/s]\n",
      " 98%|█████████▊| 815/834 [02:20<00:03,  5.54it/s]\n",
      " 98%|█████████▊| 816/834 [02:21<00:03,  5.59it/s]\n",
      " 98%|█████████▊| 817/834 [02:21<00:02,  5.73it/s]\n",
      " 98%|█████████▊| 818/834 [02:21<00:02,  5.74it/s]\n",
      " 98%|█████████▊| 819/834 [02:21<00:02,  5.81it/s]\n",
      " 98%|█████████▊| 820/834 [02:21<00:02,  5.90it/s]\n",
      "                                                 \n",
      "\n",
      " 98%|█████████▊| 820/834 [02:21<00:02,  5.90it/s]\n",
      " 98%|█████████▊| 821/834 [02:21<00:02,  6.03it/s]\n",
      " 99%|█████████▊| 822/834 [02:22<00:02,  5.94it/s]\n",
      " 99%|█████████▊| 823/834 [02:22<00:01,  5.69it/s]\n",
      " 99%|█████████▉| 824/834 [02:22<00:01,  5.75it/s]\n",
      " 99%|█████████▉| 825/834 [02:22<00:01,  5.79it/s]\n",
      " 99%|█████████▉| 826/834 [02:22<00:01,  5.79it/s]\n",
      " 99%|█████████▉| 827/834 [02:23<00:01,  5.75it/s]\n",
      " 99%|█████████▉| 828/834 [02:23<00:01,  5.80it/s]\n",
      " 99%|█████████▉| 829/834 [02:23<00:00,  5.89it/s]\n",
      "100%|█████████▉| 830/834 [02:23<00:00,  5.85it/s]\n",
      "                                                 \n",
      "\n",
      "100%|█████████▉| 830/834 [02:23<00:00,  5.85it/s]\n",
      "100%|█████████▉| 831/834 [02:23<00:00,  5.77it/s]\n",
      "100%|█████████▉| 832/834 [02:23<00:00,  5.79it/s]\n",
      "100%|█████████▉| 833/834 [02:24<00:00,  5.83it/s]\n",
      "100%|██████████| 834/834 [02:24<00:00,  5.96it/s]\n",
      "                                                 \n",
      "\n",
      "100%|██████████| 834/834 [02:24<00:00,  5.96it/s]\n",
      "100%|██████████| 834/834 [02:24<00:00,  5.78it/s]\n",
      "\u001b[32m2024-06-13 13:30:55.361\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m753\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 144.2397, 'train_samples_per_second': 17.346, 'train_steps_per_second': 5.782, 'total_flos': 599884231606272.0, 'train_loss': 3.7271630546743637, 'epoch': 1.0, 'train_samples': 2502}\u001b[0m\n",
      "\u001b[32m2024-06-13 13:30:55.361\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m754\u001b[0m - \u001b[1mSaving model checkpoint to outputs-pt-v1\u001b[0m\n",
      "\u001b[32m2024-06-13 13:30:55.648\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m762\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 35.28it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 32.69it/s]\n",
      "\u001b[32m2024-06-13 13:30:55.817\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m775\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': 3.923959732055664, 'eval_accuracy': 0.3464566929133858, 'eval_runtime': 0.1562, 'eval_samples_per_second': 64.019, 'eval_steps_per_second': 25.608, 'epoch': 1.0, 'eval_samples': 10, 'perplexity': 50.60041268325243}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !python pretraining.py \\\n",
    "#     --model_type auto \\\n",
    "#     --model_name_or_path Qwen/Qwen1.5-0.5B-Chat \\\n",
    "#     --train_file_dir data\\pretrain \\\n",
    "#     --validation_file_dir data\\pretrain \\\n",
    "#     --per_device_train_batch_size 3 \\\n",
    "#     --per_device_eval_batch_size 3 \\\n",
    "#     --do_train \\\n",
    "#     --do_eval \\\n",
    "#     --use_peft True \\\n",
    "#     --seed 42 \\\n",
    "#     --fp16 \\\n",
    "#     --max_train_samples 20000 \\\n",
    "#     --max_eval_samples 10 \\\n",
    "#     --num_train_epochs 1 \\\n",
    "#     --learning_rate 2e-4 \\\n",
    "#     --warmup_ratio 0.05 \\\n",
    "#     --weight_decay 0.01 \\\n",
    "#     --logging_strategy steps \\\n",
    "#     --logging_steps 10 \\\n",
    "#     --eval_steps 50 \\\n",
    "#     --evaluation_strategy steps \\\n",
    "#     --save_steps 500 \\\n",
    "#     --save_strategy steps \\\n",
    "#     --save_total_limit 3 \\\n",
    "#     --gradient_accumulation_steps 1 \\\n",
    "#     --preprocessing_num_workers 1 \\\n",
    "#     --block_size 128 \\\n",
    "#     --group_by_length True \\\n",
    "#     --output_dir outputs-pt-v1 \\\n",
    "#     --overwrite_output_dir \\\n",
    "#     --ddp_timeout 30000 \\\n",
    "#     --logging_first_step True \\\n",
    "#     --target_modules all \\\n",
    "#     --lora_rank 8 \\\n",
    "#     --lora_alpha 16 \\\n",
    "#     --lora_dropout 0.05 \\\n",
    "#     --torch_dtype float16 \\\n",
    "#     --device_map auto \\\n",
    "#     --report_to tensorboard \\\n",
    "#     --ddp_find_unused_parameters False \\\n",
    "#     --gradient_checkpointing True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is New Volume\n",
      " Volume Serial Number is B4E8-CC63\n",
      "\n",
      " Directory of d:\\llm\\whole_process\\MedicalGPT\n",
      "\n",
      "\n",
      " Directory of d:\\llm\\whole_process\\MedicalGPT\\outputs-pt-v1\n",
      "\n",
      "08/21/2024  07:27 PM    <DIR>          .\n",
      "06/17/2024  04:52 PM    <DIR>          ..\n",
      "08/21/2024  07:27 PM               762 adapter_config.json\n",
      "08/21/2024  07:27 PM        83,945,296 adapter_model.safetensors\n",
      "08/21/2024  07:27 PM                55 added_tokens.json\n",
      "08/21/2024  07:27 PM               486 all_results.json\n",
      "08/21/2024  07:26 PM    <DIR>          checkpoint-1000\n",
      "06/13/2024  01:29 PM    <DIR>          checkpoint-500\n",
      "08/21/2024  07:27 PM               271 eval_results.json\n",
      "06/13/2024  01:30 PM         1,823,241 merges.txt\n",
      "08/21/2024  07:27 PM             5,113 README.md\n",
      "08/21/2024  07:22 PM    <DIR>          runs\n",
      "08/21/2024  07:27 PM               443 special_tokens_map.json\n",
      "08/21/2024  07:27 PM           493,443 tokenizer.model\n",
      "08/21/2024  07:27 PM             1,715 tokenizer_config.json\n",
      "08/21/2024  07:27 PM               199 train_results.json\n",
      "08/21/2024  07:27 PM            30,745 trainer_state.json\n",
      "06/13/2024  01:30 PM         3,535,052 vocab.json\n",
      "              13 File(s)     89,836,821 bytes\n",
      "               5 Dir(s)  446,001,139,712 bytes free\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File Not Found\n"
     ]
    }
   ],
   "source": [
    "%ls -lh outputs-pt-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型训练结果：\n",
    "- 使用lora训练模型，则保存的lora权重是`adapter_model.bin`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
    "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model_type='auto', base_model='Qwen1.5-1.8B-Chat', tokenizer_path=None, lora_model='outputs-pt-v1', resize_emb=False, output_dir='merged-pt/', hf_hub_model_id='', hf_hub_token=None)\n",
      "Base model: Qwen1.5-1.8B-Chat\n",
      "LoRA model: outputs-pt-v1\n",
      "Loading LoRA for causal language model\n",
      "Merging with merge_and_unload...\n",
      "Saving to Hugging Face format...\n",
      "Done! model saved to merged-pt/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "!python merge_peft_adapter.py --model_type auto \\\n",
    "    --base_model Qwen1.5-1.8B-Chat --lora_model outputs-pt-v1 --output_dir merged-pt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid switch - \"\".\n"
     ]
    }
   ],
   "source": [
    "%ls -lh merged-pt/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%cat` not found.\n"
     ]
    }
   ],
   "source": [
    "%cat merged-pt/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage1 增量预训练完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T13:56:17.081153Z",
     "start_time": "2023-06-15T13:56:17.032821Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Stage 2: Supervised FineTuning\n",
    "\n",
    "第二阶段：SFT(Supervised Fine-tuning)有监督微调，构造指令微调数据集，在预训练模型基础上做指令精调，以对齐指令意图，并注入领域知识\n",
    "\n",
    "| Stage 2: Supervised Fine-tuning | [supervised_finetuning.py](https://github.com/shibing624/MedicalGPT/blob/main/supervised_finetuning.py) | [run_sft.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_sft.sh)  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 说明：\n",
    "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
    "\n",
    "1. 生成模型：使用的是Bloom的`bigscience/bloomz-560m` 或者 Stage1得到的预训练模型\n",
    "2. 数据集：SFT阶段使用的是使用的是Belle的1千条抽样数据，位于`data/finetune`文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stage2 咱们开始吧\n",
    "\n",
    "训练步骤如下：\n",
    "\n",
    "1. 确认训练集\n",
    "2. 执行训练脚本\n",
    "\n",
    "训练脚本的执行逻辑如下：\n",
    "1. 导入依赖包\n",
    "2. 设置参数\n",
    "3. 定义各函数并加载训练集\n",
    "4. 加载模型和tokenizer\n",
    "5. 开始训练并评估\n",
    "6. 查看训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is New Volume\n",
      " Volume Serial Number is B4E8-CC63\n",
      "\n",
      " Directory of d:\\llm\\whole_process\\MedicalGPT\\merged-pt\n",
      "\n",
      "08/21/2024  08:34 PM    <DIR>          .\n",
      "08/21/2024  08:24 PM    <DIR>          ..\n",
      "08/21/2024  08:34 PM                85 added_tokens.json\n",
      "08/21/2024  08:34 PM               729 config.json\n",
      "08/21/2024  08:34 PM               217 generation_config.json\n",
      "08/21/2024  08:34 PM         1,671,853 merges.txt\n",
      "08/21/2024  08:34 PM     3,673,690,400 model.safetensors\n",
      "08/21/2024  07:29 PM            24,248 model.safetensors.index.json\n",
      "08/21/2024  08:34 PM               387 special_tokens_map.json\n",
      "08/21/2024  08:34 PM         7,028,015 tokenizer.json\n",
      "08/21/2024  07:29 PM           493,443 tokenizer.model\n",
      "08/21/2024  08:34 PM             1,342 tokenizer_config.json\n",
      "08/21/2024  08:34 PM         2,776,833 vocab.json\n",
      "              11 File(s)  3,685,687,552 bytes\n",
      "               2 Dir(s)  436,107,190,272 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls merged-pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T13:58:38.966506Z",
     "start_time": "2023-06-15T13:58:38.778132Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is New Volume\n",
      " Volume Serial Number is B4E8-CC63\n",
      "\n",
      " Directory of d:\\llm\\whole_process\\MedicalGPT\\data\\finetune\n",
      "\n",
      "06/12/2024  07:20 PM    <DIR>          .\n",
      "06/12/2024  07:20 PM    <DIR>          ..\n",
      "06/12/2024  07:20 PM           766,815 medical_sft_1K_format.jsonl\n",
      "06/12/2024  07:20 PM         4,082,858 sharegpt_zh_1K_format.jsonl\n",
      "               2 File(s)      4,849,673 bytes\n",
      "               2 Dir(s)  436,107,190,272 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls data\\finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7,495,680 || all params: 1,844,324,352 || trainable%: 0.4064187512284173\n",
      "{'loss': 2.763, 'grad_norm': 0.9128183722496033, 'learning_rate': 1.5384615384615387e-06, 'epoch': 0.0}\n",
      "{'loss': 2.3173, 'grad_norm': 2.5105676651000977, 'learning_rate': 1.5384615384615387e-05, 'epoch': 0.04}\n",
      "{'loss': 2.4568, 'grad_norm': 1.6755942106246948, 'learning_rate': 1.9409282700421944e-05, 'epoch': 0.08}\n",
      "{'loss': 2.6113, 'grad_norm': 1.8691452741622925, 'learning_rate': 1.856540084388186e-05, 'epoch': 0.12}\n",
      "{'loss': 2.1971, 'grad_norm': 1.1665077209472656, 'learning_rate': 1.7721518987341772e-05, 'epoch': 0.16}\n",
      "{'loss': 2.2834, 'grad_norm': 1.2559572458267212, 'learning_rate': 1.687763713080169e-05, 'epoch': 0.2}\n",
      "{'eval_loss': 2.512449264526367, 'eval_runtime': 0.1816, 'eval_samples_per_second': 55.076, 'eval_steps_per_second': 16.523, 'epoch': 0.2}\n",
      "{'loss': 2.158, 'grad_norm': 1.4853975772857666, 'learning_rate': 1.6033755274261603e-05, 'epoch': 0.24}\n",
      "{'loss': 2.2105, 'grad_norm': 0.7574702501296997, 'learning_rate': 1.5189873417721521e-05, 'epoch': 0.28}\n",
      "{'loss': 2.2097, 'grad_norm': 0.8658046722412109, 'learning_rate': 1.4345991561181437e-05, 'epoch': 0.32}\n",
      "{'loss': 2.0165, 'grad_norm': 1.544792652130127, 'learning_rate': 1.350210970464135e-05, 'epoch': 0.36}\n",
      "{'loss': 1.978, 'grad_norm': 1.026879072189331, 'learning_rate': 1.2658227848101268e-05, 'epoch': 0.4}\n",
      "{'eval_loss': 2.3738949298858643, 'eval_runtime': 0.1987, 'eval_samples_per_second': 50.325, 'eval_steps_per_second': 15.098, 'epoch': 0.4}\n",
      "{'loss': 2.1963, 'grad_norm': 1.424285650253296, 'learning_rate': 1.1814345991561182e-05, 'epoch': 0.44}\n",
      "{'loss': 2.3303, 'grad_norm': 0.9307119846343994, 'learning_rate': 1.0970464135021096e-05, 'epoch': 0.48}\n",
      "{'loss': 2.2133, 'grad_norm': 1.9390019178390503, 'learning_rate': 1.0126582278481014e-05, 'epoch': 0.52}\n",
      "{'loss': 2.2654, 'grad_norm': 1.1597095727920532, 'learning_rate': 9.28270042194093e-06, 'epoch': 0.56}\n",
      "{'loss': 2.2521, 'grad_norm': 0.780483603477478, 'learning_rate': 8.438818565400846e-06, 'epoch': 0.6}\n",
      "{'eval_loss': 2.3271689414978027, 'eval_runtime': 0.1865, 'eval_samples_per_second': 53.63, 'eval_steps_per_second': 16.089, 'epoch': 0.6}\n",
      "{'loss': 2.2426, 'grad_norm': 1.2566627264022827, 'learning_rate': 7.5949367088607605e-06, 'epoch': 0.64}\n",
      "{'loss': 2.1211, 'grad_norm': 1.0674129724502563, 'learning_rate': 6.751054852320675e-06, 'epoch': 0.68}\n",
      "{'loss': 1.9399, 'grad_norm': 1.302935004234314, 'learning_rate': 5.907172995780591e-06, 'epoch': 0.72}\n",
      "{'loss': 2.1514, 'grad_norm': 1.5078836679458618, 'learning_rate': 5.063291139240507e-06, 'epoch': 0.76}\n",
      "{'loss': 1.865, 'grad_norm': 1.2921652793884277, 'learning_rate': 4.219409282700423e-06, 'epoch': 0.8}\n",
      "{'eval_loss': 2.307551860809326, 'eval_runtime': 0.1788, 'eval_samples_per_second': 55.936, 'eval_steps_per_second': 16.781, 'epoch': 0.8}\n",
      "{'loss': 2.1216, 'grad_norm': 1.1732054948806763, 'learning_rate': 3.3755274261603377e-06, 'epoch': 0.84}\n",
      "{'loss': 2.1967, 'grad_norm': 1.2202152013778687, 'learning_rate': 2.5316455696202535e-06, 'epoch': 0.88}\n",
      "{'loss': 2.049, 'grad_norm': 1.4315383434295654, 'learning_rate': 1.6877637130801689e-06, 'epoch': 0.92}\n",
      "{'loss': 2.0339, 'grad_norm': 2.247882127761841, 'learning_rate': 8.438818565400844e-07, 'epoch': 0.96}\n",
      "{'loss': 2.2251, 'grad_norm': 1.4679749011993408, 'learning_rate': 0.0, 'epoch': 1.0}\n",
      "{'eval_loss': 2.3060154914855957, 'eval_runtime': 0.1754, 'eval_samples_per_second': 57.027, 'eval_steps_per_second': 17.108, 'epoch': 1.0}\n",
      "{'train_runtime': 64.9026, 'train_samples_per_second': 15.377, 'train_steps_per_second': 3.852, 'train_loss': 2.1874768581390382, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =        1.0\n",
      "  train_loss               =     2.1875\n",
      "  train_runtime            = 0:01:04.90\n",
      "  train_samples            =       1000\n",
      "  train_samples_per_second =     15.377\n",
      "  train_steps_per_second   =      3.852\n",
      "***** eval metrics *****\n",
      "  epoch                   =        1.0\n",
      "  eval_loss               =     2.3059\n",
      "  eval_runtime            = 0:00:00.19\n",
      "  eval_samples            =         10\n",
      "  eval_samples_per_second =     50.852\n",
      "  eval_steps_per_second   =     15.256\n",
      "  perplexity              =    10.0333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 20:34:29.765689: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-21 20:34:30.164756: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[32m2024-08-21 20:34:30.817\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__post_init__\u001b[0m:\u001b[36m221\u001b[0m - \u001b[33m\u001b[1mYou may set max_train_samples = -1 to run all samples in production.\u001b[0m\n",
      "\u001b[32m2024-08-21 20:34:30.818\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m500\u001b[0m - \u001b[1mModel args: ModelArguments(model_type='auto', model_name_or_path='merged-pt', load_in_8bit=False, load_in_4bit=False, tokenizer_name_or_path=None, cache_dir=None, model_revision='main', hf_hub_token=None, use_fast_tokenizer=False, torch_dtype='float16', device_map='auto', trust_remote_code=True, rope_scaling=None, flash_attn=False, shift_attn=False, neft_alpha=0)\u001b[0m\n",
      "\u001b[32m2024-08-21 20:34:30.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m501\u001b[0m - \u001b[1mData args: DataArguments(dataset_name=None, dataset_config_name=None, train_file_dir='./data/finetune', validation_file_dir='./data/finetune', max_train_samples=1000, max_eval_samples=10, ignore_pad_token_for_loss=True, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=1)\u001b[0m\n",
      "\u001b[32m2024-08-21 20:34:30.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m502\u001b[0m - \u001b[1mTraining args: Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=False,\n",
      "ddp_timeout=30000,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=50,\n",
      "evaluation_strategy=steps,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=2e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=outputs-sft-v1\\runs\\Aug21_20-34-30_dpg,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=10,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=outputs-sft-v1,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=outputs-sft-v1,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=3,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.05,\n",
      ")\u001b[0m\n",
      "\u001b[32m2024-08-21 20:34:30.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m503\u001b[0m - \u001b[1mScript args: ScriptArguments(use_peft=True, train_on_inputs=False, target_modules='all', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, modules_to_save=None, peft_path=None, qlora=False, model_max_length=512, template_name='vicuna')\u001b[0m\n",
      "\u001b[32m2024-08-21 20:34:30.819\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m504\u001b[0m - \u001b[1mProcess rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: True\u001b[0m\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m2024-08-21 20:34:30.960\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m531\u001b[0m - \u001b[1mAdd bos_token: <|im_end|>, bos_token_id: 151645\u001b[0m\n",
      "\u001b[32m2024-08-21 20:34:30.960\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m538\u001b[0m - \u001b[34m\u001b[1mTokenizer: Qwen2Tokenizer(name_or_path='merged-pt', vocab_size=151643, model_max_length=32768, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|im_end|>', 'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\u001b[0m\n",
      "\u001b[32m2024-08-21 20:34:30.960\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m566\u001b[0m - \u001b[1mtrain files: ['./data/finetune\\\\medical_sft_1K_format.jsonl', './data/finetune\\\\sharegpt_zh_1K_format.jsonl']\u001b[0m\n",
      "\u001b[32m2024-08-21 20:34:30.961\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m571\u001b[0m - \u001b[1meval files: ['./data/finetune\\\\medical_sft_1K_format.jsonl', './data/finetune\\\\sharegpt_zh_1K_format.jsonl']\u001b[0m\n",
      "\u001b[32m2024-08-21 20:34:31.292\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m587\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['conversations'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['conversations'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\u001b[0m\n",
      "\u001b[32m2024-08-21 20:34:31.293\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m678\u001b[0m - \u001b[34m\u001b[1mExample train_dataset[0]: {'conversations': [{'from': 'human', 'value': '做广告者。'}, {'from': 'gpt', 'value': '当然。作为广告客户，我如何帮助您？您广告活动的目标是什么？您的目标受众是什么？请提供更多信息，以便我能够为您提供更有针对性的支持。'}, {'from': 'human', 'value': '请为我找到FOMO的替代概念，以便我将其用作广告。\\n例如，我为某个受众利用FOMO来创造类似“我能排除某人吗？”或“我能变得有阶级意识吗？”等概念。\\n您需要为我找到FOMO的其他替代概念，这将对我创建广告内容很有帮助。'}, {'from': 'gpt', 'value': '以下是一些你可以用于广告活动的替代概念：\\n1. 紧迫性：在优惠活动周围创造一种紧迫感，让人们感到必须立刻行动，以免错失机会。\\n2. 独家性：让人们感到他们正在购买独特的东西，而这些东西不是每个人都可以得到的。\\n3. 稀缺性：在促销活动周围创造一种稀缺感，让人们感到可用的产品数量有限。\\n4. 新鲜感：强调产品或服务是新的和创新的，以及如何为人们的生活带来积极变化。\\n5. 独特好处：突出产品或服务的独特好处，以及如何解决你目标受众的特定问题。\\n这些只是一些你可以用于广告活动的替代概念的例子。重要的是你要了解你目标受众并能够创建一个与他们共鸣的信息。'}, {'from': 'human', 'value': 'Benissimo，列出其他10个概念。'}, {'from': 'gpt', 'value': '以下是另外10个可供广告活动使用的替代概念：\\n1. 便利性：突出产品或服务可以使人们的生活更简单、更方便。\\n2. 质量：强调产品或服务的质量，并说明它如何与竞争对手相比有所区别。\\n3. 流行度：让购买者感受到他们正在购买受欢迎的物品，有很多其他人已经选择了这个产品。\\n4. 节约：强调产品或服务可以帮助节省金钱或时间。\\n5. 个性化：强调产品或服务可以根据人们的具体需求进行个性化定制。\\n6. 满意保证：强调您的满意保证政策，并说明这可以给购买者带来安心，如果不满意可以退货。\\n7. 支持：强调您在销售前后提供的支持。\\n8. 透明度：强调您的透明度，在报价方面让人们感觉他们确切知道自己正在购买什么。\\n9. 趋势：突显产品或服务与当前趋势保持一致，并说明它如何成为人们生活中的时尚附加品。\\n10. 价值：强调产品或服务的价值，证明购买者可以花费相应的价格获得很多物品。\\n这些都是另外10个替代概念，可供广告活动使用。希望这些可以为你提供新的灵感，让你更好地创建下一个广告内容。'}]}\u001b[0m\n",
      "\n",
      "Running tokenizer on train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
      "Running tokenizer on train dataset: 100%|██████████| 1000/1000 [00:07<00:00, 136.82 examples/s]\n",
      "Running tokenizer on train dataset: 100%|██████████| 1000/1000 [00:07<00:00, 136.47 examples/s]\n",
      "\n",
      "Filter:   0%|          | 0/998 [00:00<?, ? examples/s]\n",
      "Filter: 100%|██████████| 998/998 [00:00<00:00, 6476.37 examples/s]\n",
      "Filter: 100%|██████████| 998/998 [00:00<00:00, 5866.65 examples/s]\n",
      "\u001b[32m2024-08-21 20:34:40.246\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m689\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 998\u001b[0m\n",
      "\u001b[32m2024-08-21 20:34:40.246\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m690\u001b[0m - \u001b[34m\u001b[1mTokenized training example:\u001b[0m\n",
      "\u001b[32m2024-08-21 20:34:40.248\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m691\u001b[0m - \u001b[34m\u001b[1mDecode input_ids[0]:\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: 狗摩教学法和词汇教学法在语言学习方面有何不同？ ASSISTANT:Dogme（突破教学法）和词汇教学法是教授外语的两种不同方法论。\n",
      "Dogme教学法由教师主导、以学生为中心，着眼于应对语言和学习者的需求和兴趣，强调学生自发性语言。它的特点是不依赖于课本，聚焦于会话活动，突出学习者自发产生的语言。 Dogme的目的是利用学生的个人经验和动机创造有意义、有吸引力的学习体验。\n",
      "另一方面，词汇教学法是一种教学方法，注重通过词汇积累来获取外语。词汇教学法强调在目标语言中积累大量的词汇是有效沟通的关键。它认为语法规则和结构自然而然地来自于语境中对单词的接触。词汇教学法基于这样一个理念：意思是语言的核心，因此词汇应在有意义的“词汇短语”中教授。\n",
      "总的来说，Dogme教学法强调学习者自发性语言和个人经验，而词汇教学法则重视词汇积累作为学习语言的关键。<|im_end|>\u001b[0m\n",
      "\u001b[32m2024-08-21 20:34:40.249\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m694\u001b[0m - \u001b[34m\u001b[1mDecode labels[0]:\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>Dogme（突破教学法）和词汇教学法是教授外语的两种不同方法论。\n",
      "Dogme教学法由教师主导、以学生为中心，着眼于应对语言和学习者的需求和兴趣，强调学生自发性语言。它的特点是不依赖于课本，聚焦于会话活动，突出学习者自发产生的语言。 Dogme的目的是利用学生的个人经验和动机创造有意义、有吸引力的学习体验。\n",
      "另一方面，词汇教学法是一种教学方法，注重通过词汇积累来获取外语。词汇教学法强调在目标语言中积累大量的词汇是有效沟通的关键。它认为语法规则和结构自然而然地来自于语境中对单词的接触。词汇教学法基于这样一个理念：意思是语言的核心，因此词汇应在有意义的“词汇短语”中教授。\n",
      "总的来说，Dogme教学法强调学习者自发性语言和个人经验，而词汇教学法则重视词汇积累作为学习语言的关键。<|im_end|>\u001b[0m\n",
      "\u001b[32m2024-08-21 20:34:40.249\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m708\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
      "\u001b[32m2024-08-21 20:34:40.250\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m712\u001b[0m - \u001b[34m\u001b[1mExample eval_dataset[0]: {'conversations': [{'from': 'human', 'value': '治疗阳痿吃什么药呢？，性生活一直很正常的，但是这段时间感觉性欲变低了，有时勃起都感觉很困难，试过许多的方法都没效果，听朋友说我这种情况可能会是早泄，想知道治疗早泄的药物？'}, {'from': 'gpt', 'value': '男子早泄、早泄病症的再次发生，多由恣情纵欲，或青年误犯性交，至命门火衰，精气虚寒；或思量忧郁，伤损心脾；或因恐惧伤肾，也有因湿热下注，宗筋弛而痿的。但主要是肾阳虚衰而痿。肾阳为那身阳气之根本，有温煦形体，蒸化水液，增进围产生长发育等功能。肾阳虚衰则温煦失责，气化无权。因而再次发生畏寒肢冷，性机能减退。故见男子早泄不举或不坚，且伴发头晕目眩。'}]}\u001b[0m\n",
      "\n",
      "Running tokenizer on validation dataset:   0%|          | 0/10 [00:00<?, ? examples/s]\n",
      "Running tokenizer on validation dataset: 100%|██████████| 10/10 [00:00<00:00, 186.48 examples/s]\n",
      "\n",
      "Filter:   0%|          | 0/10 [00:00<?, ? examples/s]\n",
      "Filter: 100%|██████████| 10/10 [00:00<00:00, 542.57 examples/s]\n",
      "\u001b[32m2024-08-21 20:34:41.888\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m722\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 10\u001b[0m\n",
      "\u001b[32m2024-08-21 20:34:41.888\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m723\u001b[0m - \u001b[34m\u001b[1mTokenized eval example:\u001b[0m\n",
      "\u001b[32m2024-08-21 20:34:41.890\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m724\u001b[0m - \u001b[34m\u001b[1mA chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: 治疗阳痿吃什么药呢？，性生活一直很正常的，但是这段时间感觉性欲变低了，有时勃起都感觉很困难，试过许多的方法都没效果，听朋友说我这种情况可能会是早泄，想知道治疗早泄的药物？ ASSISTANT:男子早泄、早泄病症的再次发生，多由恣情纵欲，或青年误犯性交，至命门火衰，精气虚寒；或思量忧郁，伤损心脾；或因恐惧伤肾，也有因湿热下注，宗筋弛而痿的。但主要是肾阳虚衰而痿。肾阳为那身阳气之根本，有温煦形体，蒸化水液，增进围产生长发育等功能。肾阳虚衰则温煦失责，气化无权。因而再次发生畏寒肢冷，性机能减退。故见男子早泄不举或不坚，且伴发头晕目眩。<|im_end|>\u001b[0m\n",
      "\u001b[32m2024-08-21 20:34:43.618\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m857\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
      "\u001b[32m2024-08-21 20:34:43.618\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m872\u001b[0m - \u001b[1mInit new peft model\u001b[0m\n",
      "\u001b[32m2024-08-21 20:34:43.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m881\u001b[0m - \u001b[1mPeft target_modules: ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj']\u001b[0m\n",
      "\u001b[32m2024-08-21 20:34:43.619\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m882\u001b[0m - \u001b[1mPeft lora_rank: 8\u001b[0m\n",
      "\u001b[32m2024-08-21 20:34:44.041\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m904\u001b[0m - \u001b[1mGradient checkpointing enabled.\u001b[0m\n",
      "\u001b[32m2024-08-21 20:34:44.084\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m932\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
      "\u001b[32m2024-08-21 20:34:44.093\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m935\u001b[0m - \u001b[34m\u001b[1mTrain dataloader example: {'input_ids': tensor([[    32,   6236,   1948,  ..., 101083,   1773, 151645],\n",
      "        [    32,   6236,   1948,  ..., 151643, 151643, 151643],\n",
      "        [    32,   6236,   1948,  ..., 151643, 151643, 151643],\n",
      "        [    32,   6236,   1948,  ..., 151643, 151643, 151643]],\n",
      "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0'), 'labels': tensor([[  -100,   -100,   -100,  ..., 101083,   1773, 151645],\n",
      "        [  -100,   -100,   -100,  ...,   -100,   -100,   -100],\n",
      "        [  -100,   -100,   -100,  ...,   -100,   -100,   -100],\n",
      "        [  -100,   -100,   -100,  ...,   -100,   -100,   -100]],\n",
      "       device='cuda:0')}\u001b[0m\n",
      "\u001b[32m2024-08-21 20:34:44.126\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m936\u001b[0m - \u001b[34m\u001b[1minput_ids:\n",
      "[tensor([    32,   6236,   1948,    264,  22208,   1196,    323,    458,  20443,\n",
      "         11229,  17847,     13,    576,  17847,   6696,  10950,     11,  11682,\n",
      "            11,    323,  47787,  11253,    311,    279,   1196,    594,   4755,\n",
      "          3918,     82,     29,   6448,     25,    220, 109662,  32108, 115623,\n",
      "         99494,  54542,  35560,   3846,   2821,     25,  67338, 104316,  99556,\n",
      "         73670, 107522,   3837,  32108, 115623, 106334,  99652, 104404,   3837,\n",
      "        115790,   9370, 109662,  57191,  99877,  99250,  99252, 100517, 101304,\n",
      "         68536,  31838, 106806, 116545, 100682,   1773,     22,     15,      4,\n",
      "         20412,  73145, 100669,  99495,  67071,  32108, 115623,  33071,  63314,\n",
      "         77959, 100517, 107503, 101304,   1773,  32108, 115623,  91572,  36993,\n",
      "        115563, 115623, 100058,   3837,  20412, 111936,  99877, 110819, 101925,\n",
      "        108165, 100113, 112705,  57191, 101474, 101264,   9370, 100235,  33108,\n",
      "         52853,   3837, 110819, 101925, 102150,   5373,  99877,  18493, 107368,\n",
      "         33108,  15946,  33071, 101425, 102150, 104739,   9370, 102835, 115278,\n",
      "        108735, 104149,  16872,  99726, 100058,  32108, 100250,  99561,   3837,\n",
      "        102189, 102890,   9370, 109181, 102914,  20221,   3837, 101894,  99894,\n",
      "         99246, 100827, 100815, 106140,  57191,  99789, 110408, 100113, 112705,\n",
      "         99762, 109181,   1773, 109198, 106983, 104693, 104423, 109662, 101304,\n",
      "        100136, 100347,  32108, 115623,   3837, 103962,  71817,  87752,  99195,\n",
      "        104481,  54542,   5122,     16,     13, 113492, 115340, 100636, 113630,\n",
      "         25710, 100058,   3837, 100398,  58362, 100345, 103998,   9370, 105262,\n",
      "         36407, 103930,  24968,     17,     13, 106304,  47815,  11622, 105202,\n",
      "        100067, 102353,  90395,  37029,  99420, 112758, 100517,  99471, 103143,\n",
      "         99271,  42140, 101285,  49567,  24968,     18,     13,  29524,  17714,\n",
      "         99575,  99533, 104057, 100205,  33071, 118039,   9370, 101304,   3837,\n",
      "         58362, 103998,  27091,  99781,  57191,  71817,  99389, 106307, 101071,\n",
      "        105262,   1773, 104043,   3837, 109861, 106304, 113788,  44636,  26232,\n",
      "         99425,  99225,  57191, 120414, 119615, 105608,  71817, 106304,  32320,\n",
      "        100439,   1773,  18493,  97639, 106533, 104103,  99879, 109662,  32108,\n",
      "        115623,   3837, 103962,  60533, 100158, 112224,   5122,     16,   5373,\n",
      "        102215,  70589, 113195,  99917, 107503, 101304,   3837, 103962, 100724,\n",
      "        100667, 104886, 115623, 100058, 104395, 103118,  52853,   3837, 100270,\n",
      "         13343, 106750, 107826, 117273,   3837,  23031, 100724, 100418,   9370,\n",
      "        117273,   1773,     17,   5373, 106304,  57191, 105418,  99892,  32320,\n",
      "        100439, 104459,  99420, 101304,   1773,     18,   5373, 105709, 100904,\n",
      "         43268, 107503,  59956,  62945, 104175, 100673,   9370, 101304,   3837,\n",
      "        103962,  99633,  32876, 101953,  39426,  31843,   3837,  44063, 100904,\n",
      "         43268, 106692,   1773,     19,   5373, 100418, 117273,  33447, 103962,\n",
      "         11622, 102891,  51827,  67279, 100506, 101358,   1773,     20,   5373,\n",
      "        103962,  60533, 101046, 102100,   3837, 100939,  39762, 101925, 100085,\n",
      "         99450, 118970, 102153,  81217, 107680,   1773,  99841,  42140,  99450,\n",
      "        100646, 104838, 104618,   5373, 104451,   3837,  41299,  99285, 105349,\n",
      "          5373,  99285, 115545, 101083,  29524,   5122, 116158,   5373, 116690,\n",
      "          5373, 117539,  99800,   5373,  99955, 106219,   5373,  55135,  99278,\n",
      "          5373, 115641,   5373, 100655,  99894,   5373, 102757,  99894,   5373,\n",
      "        118408,   5373,  99705,  99955,  21515,  49567,   1773,  99841,  42140,\n",
      "         99450,  99251,  99955,  21515, 104204, 105069,   1773,  99841, 106423,\n",
      "        104155,  99318,   3837, 102087, 101239,  99318,   1773, 116105, 107895,\n",
      "          5373,  21287, 103306,  49567, 104313,  33071, 102153,  57191, 118970,\n",
      "        101083,   1773, 151645], device='cuda:0'), tensor([    32,   6236,   1948,    264,  22208,   1196,    323,    458,  20443,\n",
      "         11229,  17847,     13,    576,  17847,   6696,  10950,     11,  11682,\n",
      "            11,    323,  47787,  11253,    311,    279,   1196,    594,   4755,\n",
      "          3918,     82,     29,   6448,     25,  46750,    241,  99593, 100179,\n",
      "        106386, 100179, 100771, 109155,      0, 110679, 103924,  12069,   3837,\n",
      "         21894, 101050,     17,     16,  92015,   4891,    236,    119,   7948,\n",
      "         62112,  75437, 105732,   8908,    227,    108,  32948, 103036,  99616,\n",
      "         12803,  99225,  33108,    302,  71268,  54021, 110013, 114815,     11,\n",
      "         77288,     16,   7948, 104593,  99486,  70927, 106303,  86546,  34187,\n",
      "         91777,  99616,    198,     15,     20, 104300,     16,     16,   9754,\n",
      "            17,     20,   8903,  49434,    239,  26939, 101949, 100634,  99616,\n",
      "         69526, 115065,     11, 103998, 108967, 104560, 102714, 101232, 100439,\n",
      "            11,  75437,  14777, 100179, 106386, 109984,     11, 101959,  80158,\n",
      "        106677,     11, 105024,  20412,  49828,  32463, 100180,     10,     17,\n",
      "             4,  59532,  42140,  99603,  62112,     11,  75437,  17992,  33447,\n",
      "        107821,     11, 101959,  80158, 108352,     11,  20412, 104217, 104961,\n",
      "        100634, 106376,  70927, 117087,     11, 103961,  99801, 104810,  34187,\n",
      "            11, 104994, 102997,     11, 103922, 104171,  28291,  56652,     11,\n",
      "         20221,  99476, 100868,     11, 103922,  35946,  99927,  20412, 102023,\n",
      "         26939, 100634, 116480,  32948,     11, 100995,  34187,     17, 102874,\n",
      "         99801,  52801, 112384,     11, 103998, 101075,  36587, 104198, 102997,\n",
      "         35560,   3846,   2821,     25, 110030, 104256,     13, 108386, 108145,\n",
      "        104256,      0,     17,     16, 103285,  53393, 100366,     16,     17,\n",
      "            15,    313,     17,     15,     15,     13,  99222, 104264,  34187,\n",
      "            13, 100004,  56568, 100148, 100397,  27733,     13, 101043, 118856,\n",
      "        104393,     13, 101948,     13, 100284,  97611, 104595, 100034,  56568,\n",
      "        105348,  26939, 111738,  99565, 100664,  99789,  99457,  99451, 106256,\n",
      "         99796,  32757,   9370,  20929, 100664,  99789,  79766,  99931, 106256,\n",
      "         99796,  32757,   9370,     18,  17177, 100653,  39762,  99405,     13,\n",
      "         99360, 106141,  67338,  92032,  99364, 102945,  85336,     13, 100626,\n",
      "         35946, 106434,  46944,  99835,  23384,     13,  99486,  11622,  46944,\n",
      "         99679,  99888, 105397, 100464,  79766,  33108,     17,  77540, 109546,\n",
      "        109290, 106674,     11,  18493,  99938,  33447,     18,  92536,  13343,\n",
      "        100399,  16872,     13, 103285,  80158, 100194,  34187,     13,  80443,\n",
      "        111018,     13,  56568,  60726,  11622,  43288,  99835,  23384,     13,\n",
      "         99405,  99796,     13,  29437,     18,  35727,  87256,  99405,  99471,\n",
      "         99360,     13,    220, 151645, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643], device='cuda:0'), tensor([    32,   6236,   1948,    264,  22208,   1196,    323,    458,  20443,\n",
      "         11229,  17847,     13,    576,  17847,   6696,  10950,     11,  11682,\n",
      "            11,    323,  47787,  11253,    311,    279,   1196,    594,   4755,\n",
      "          3918,     82,     29,   6448,     25,  68739,    247, 104191, 101599,\n",
      "          2073, 100382, 101937, 102369,  33108,  75117,   9370, 100385,  97907,\n",
      "        100376,  99556, 101039, 100157,   1773,  35560,   3846,   2821,     25,\n",
      "        102335,  28311,  18493, 104721, 100177,   3837, 100382, 101937, 105940,\n",
      "        101053, 100064, 111654, 103923,  99886,  75768,   1773, 105267,  99602,\n",
      "          9370, 105499, 105178, 102421,  33071,   3837, 106297, 100064, 104495,\n",
      "         67338, 100382, 116563, 101937,   9370, 105877,  33108, 103941,  81167,\n",
      "          1773, 103968,   3837, 103925, 100382, 101937, 104257, 100694, 106404,\n",
      "          3837,  77288, 104017, 107474, 105178,  30440,  75117,  33071, 106750,\n",
      "        101118, 100646, 100376,  86119,   1773,  21894,  99556, 101039, 106166,\n",
      "         99556, 100382, 101937,   9370, 100369,  99936, 100385,   8545, 101894,\n",
      "         33108,  75117,   8545, 101034, 102074,  41146, 106806, 100376,  86119,\n",
      "          1773,  99556,  44063, 101042, 110971, 100376, 102724,  32664, 100382,\n",
      "        101937, 105606,  90395, 102086,  41146,  18493, 104397,  82587,  99719,\n",
      "        102220, 104036, 104481, 114522,   1773,  21894,  99556, 105375, 102086,\n",
      "         99602,  18493, 101902, 100382, 101937,   9370, 101894,  33108,  75117,\n",
      "         99522, 104149,  90395,  60610, 105537, 106815,  99361,   9370, 100376,\n",
      "         99564,   1773, 103941,   3837,  21894,  99556,  44063, 106166, 101080,\n",
      "        101046, 100382, 101937, 100376, 102724,   9370, 101898,   3837,  23031,\n",
      "        103944,  18493, 106560,  82587,  99489,  15946, 104017, 107474, 105178,\n",
      "         30440,  75117,  33071,   1773, 151645, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
      "        151643, 151643, 151643], device='cuda:0')], \n",
      "labels:\n",
      "[tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,  67338, 104316,  99556,\n",
      "         73670, 107522,   3837,  32108, 115623, 106334,  99652, 104404,   3837,\n",
      "        115790,   9370, 109662,  57191,  99877,  99250,  99252, 100517, 101304,\n",
      "         68536,  31838, 106806, 116545, 100682,   1773,     22,     15,      4,\n",
      "         20412,  73145, 100669,  99495,  67071,  32108, 115623,  33071,  63314,\n",
      "         77959, 100517, 107503, 101304,   1773,  32108, 115623,  91572,  36993,\n",
      "        115563, 115623, 100058,   3837,  20412, 111936,  99877, 110819, 101925,\n",
      "        108165, 100113, 112705,  57191, 101474, 101264,   9370, 100235,  33108,\n",
      "         52853,   3837, 110819, 101925, 102150,   5373,  99877,  18493, 107368,\n",
      "         33108,  15946,  33071, 101425, 102150, 104739,   9370, 102835, 115278,\n",
      "        108735, 104149,  16872,  99726, 100058,  32108, 100250,  99561,   3837,\n",
      "        102189, 102890,   9370, 109181, 102914,  20221,   3837, 101894,  99894,\n",
      "         99246, 100827, 100815, 106140,  57191,  99789, 110408, 100113, 112705,\n",
      "         99762, 109181,   1773, 109198, 106983, 104693, 104423, 109662, 101304,\n",
      "        100136, 100347,  32108, 115623,   3837, 103962,  71817,  87752,  99195,\n",
      "        104481,  54542,   5122,     16,     13, 113492, 115340, 100636, 113630,\n",
      "         25710, 100058,   3837, 100398,  58362, 100345, 103998,   9370, 105262,\n",
      "         36407, 103930,  24968,     17,     13, 106304,  47815,  11622, 105202,\n",
      "        100067, 102353,  90395,  37029,  99420, 112758, 100517,  99471, 103143,\n",
      "         99271,  42140, 101285,  49567,  24968,     18,     13,  29524,  17714,\n",
      "         99575,  99533, 104057, 100205,  33071, 118039,   9370, 101304,   3837,\n",
      "         58362, 103998,  27091,  99781,  57191,  71817,  99389, 106307, 101071,\n",
      "        105262,   1773, 104043,   3837, 109861, 106304, 113788,  44636,  26232,\n",
      "         99425,  99225,  57191, 120414, 119615, 105608,  71817, 106304,  32320,\n",
      "        100439,   1773,  18493,  97639, 106533, 104103,  99879, 109662,  32108,\n",
      "        115623,   3837, 103962,  60533, 100158, 112224,   5122,     16,   5373,\n",
      "        102215,  70589, 113195,  99917, 107503, 101304,   3837, 103962, 100724,\n",
      "        100667, 104886, 115623, 100058, 104395, 103118,  52853,   3837, 100270,\n",
      "         13343, 106750, 107826, 117273,   3837,  23031, 100724, 100418,   9370,\n",
      "        117273,   1773,     17,   5373, 106304,  57191, 105418,  99892,  32320,\n",
      "        100439, 104459,  99420, 101304,   1773,     18,   5373, 105709, 100904,\n",
      "         43268, 107503,  59956,  62945, 104175, 100673,   9370, 101304,   3837,\n",
      "        103962,  99633,  32876, 101953,  39426,  31843,   3837,  44063, 100904,\n",
      "         43268, 106692,   1773,     19,   5373, 100418, 117273,  33447, 103962,\n",
      "         11622, 102891,  51827,  67279, 100506, 101358,   1773,     20,   5373,\n",
      "        103962,  60533, 101046, 102100,   3837, 100939,  39762, 101925, 100085,\n",
      "         99450, 118970, 102153,  81217, 107680,   1773,  99841,  42140,  99450,\n",
      "        100646, 104838, 104618,   5373, 104451,   3837,  41299,  99285, 105349,\n",
      "          5373,  99285, 115545, 101083,  29524,   5122, 116158,   5373, 116690,\n",
      "          5373, 117539,  99800,   5373,  99955, 106219,   5373,  55135,  99278,\n",
      "          5373, 115641,   5373, 100655,  99894,   5373, 102757,  99894,   5373,\n",
      "        118408,   5373,  99705,  99955,  21515,  49567,   1773,  99841,  42140,\n",
      "         99450,  99251,  99955,  21515, 104204, 105069,   1773,  99841, 106423,\n",
      "        104155,  99318,   3837, 102087, 101239,  99318,   1773, 116105, 107895,\n",
      "          5373,  21287, 103306,  49567, 104313,  33071, 102153,  57191, 118970,\n",
      "        101083,   1773, 151645], device='cuda:0'), tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100, 110030, 104256,     13, 108386, 108145,\n",
      "        104256,      0,     17,     16, 103285,  53393, 100366,     16,     17,\n",
      "            15,    313,     17,     15,     15,     13,  99222, 104264,  34187,\n",
      "            13, 100004,  56568, 100148, 100397,  27733,     13, 101043, 118856,\n",
      "        104393,     13, 101948,     13, 100284,  97611, 104595, 100034,  56568,\n",
      "        105348,  26939, 111738,  99565, 100664,  99789,  99457,  99451, 106256,\n",
      "         99796,  32757,   9370,  20929, 100664,  99789,  79766,  99931, 106256,\n",
      "         99796,  32757,   9370,     18,  17177, 100653,  39762,  99405,     13,\n",
      "         99360, 106141,  67338,  92032,  99364, 102945,  85336,     13, 100626,\n",
      "         35946, 106434,  46944,  99835,  23384,     13,  99486,  11622,  46944,\n",
      "         99679,  99888, 105397, 100464,  79766,  33108,     17,  77540, 109546,\n",
      "        109290, 106674,     11,  18493,  99938,  33447,     18,  92536,  13343,\n",
      "        100399,  16872,     13, 103285,  80158, 100194,  34187,     13,  80443,\n",
      "        111018,     13,  56568,  60726,  11622,  43288,  99835,  23384,     13,\n",
      "         99405,  99796,     13,  29437,     18,  35727,  87256,  99405,  99471,\n",
      "         99360,     13,    220, 151645,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100], device='cuda:0'), tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "        102335,  28311,  18493, 104721, 100177,   3837, 100382, 101937, 105940,\n",
      "        101053, 100064, 111654, 103923,  99886,  75768,   1773, 105267,  99602,\n",
      "          9370, 105499, 105178, 102421,  33071,   3837, 106297, 100064, 104495,\n",
      "         67338, 100382, 116563, 101937,   9370, 105877,  33108, 103941,  81167,\n",
      "          1773, 103968,   3837, 103925, 100382, 101937, 104257, 100694, 106404,\n",
      "          3837,  77288, 104017, 107474, 105178,  30440,  75117,  33071, 106750,\n",
      "        101118, 100646, 100376,  86119,   1773,  21894,  99556, 101039, 106166,\n",
      "         99556, 100382, 101937,   9370, 100369,  99936, 100385,   8545, 101894,\n",
      "         33108,  75117,   8545, 101034, 102074,  41146, 106806, 100376,  86119,\n",
      "          1773,  99556,  44063, 101042, 110971, 100376, 102724,  32664, 100382,\n",
      "        101937, 105606,  90395, 102086,  41146,  18493, 104397,  82587,  99719,\n",
      "        102220, 104036, 104481, 114522,   1773,  21894,  99556, 105375, 102086,\n",
      "         99602,  18493, 101902, 100382, 101937,   9370, 101894,  33108,  75117,\n",
      "         99522, 104149,  90395,  60610, 105537, 106815,  99361,   9370, 100376,\n",
      "         99564,   1773, 103941,   3837,  21894,  99556,  44063, 106166, 101080,\n",
      "        101046, 100382, 101937, 100376, 102724,   9370, 101898,   3837,  23031,\n",
      "        103944,  18493, 106560,  82587,  99489,  15946, 104017, 107474, 105178,\n",
      "         30440,  75117,  33071,   1773, 151645,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100], device='cuda:0')]\u001b[0m\n",
      "\u001b[32m2024-08-21 20:34:44.127\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m937\u001b[0m - \u001b[34m\u001b[1mDecode input_ids[0]:\n",
      "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.</s>USER: 伤口化脓怎么处理 ASSISTANT:通过医学研究可以得出，化脓这种情况它是因为，破损的伤口或组织被病菌感染而所发生的病理变化。70%是直接受到了由化脓性链球菌引起的感染。化脓同时会伴有脓液，是机体组织炎症过程中形成的浓稠或稀薄的混和物，炎症过程中细胞、组织在细菌和中性粒细胞释放的蛋白溶解酶的作用下发生液化坏死，加上血管的液体渗出，形成肉眼呈灰黄色或黄白色的浓稠状液体。如果我们身边的病人出现了伤口感染且出现化脓，一定要进行以下几方面的处理：1.口服抗生素甚至静脉输液，具体需根据医生的诊断来决定；2.局部外用消毒剂清洁，并使用抗阳性菌药膏百多邦等；3.如为革兰氏阴性杆菌的感染，需医生面诊或进行血常规检查诊断。此外，还可局部照射高能红光或氦氖激光进行局部消炎。在我们平常的生活发现伤口化脓，一定要注意一下几点：1、无论是以上哪种原因引起的感染，一定要做到及时清理脓液或是分泌物，必要时还需要放置引流，以做到充分的引流。2、局部或全身应用消炎药物抗感染。3、如果是缝线引起的排异反应导致的感染，一定要探查刀口内，将缝线拆除。4、充分引流后一定要用纱布包扎固定。5、一定要注意加强营养，愈合过程中禁食辛辣食物及喝酒。宜多食各种新鲜水果、蔬菜，进低脂肪、低胆固醇食品如：香菇、木耳、芹菜、豆芽、海带、藕、鱼肉、兔肉、鸡肉、鲜豆类等。宜多食干豆类及其制品。宜选用植物油，不用动物油。少吃辣椒、生蒜等刺激性食物或辛辣食品。<|im_end|>\u001b[0m\n",
      "\u001b[32m2024-08-21 20:34:44.163\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m940\u001b[0m - \u001b[34m\u001b[1mDecode labels[0]:\n",
      "<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>通过医学研究可以得出，化脓这种情况它是因为，破损的伤口或组织被病菌感染而所发生的病理变化。70%是直接受到了由化脓性链球菌引起的感染。化脓同时会伴有脓液，是机体组织炎症过程中形成的浓稠或稀薄的混和物，炎症过程中细胞、组织在细菌和中性粒细胞释放的蛋白溶解酶的作用下发生液化坏死，加上血管的液体渗出，形成肉眼呈灰黄色或黄白色的浓稠状液体。如果我们身边的病人出现了伤口感染且出现化脓，一定要进行以下几方面的处理：1.口服抗生素甚至静脉输液，具体需根据医生的诊断来决定；2.局部外用消毒剂清洁，并使用抗阳性菌药膏百多邦等；3.如为革兰氏阴性杆菌的感染，需医生面诊或进行血常规检查诊断。此外，还可局部照射高能红光或氦氖激光进行局部消炎。在我们平常的生活发现伤口化脓，一定要注意一下几点：1、无论是以上哪种原因引起的感染，一定要做到及时清理脓液或是分泌物，必要时还需要放置引流，以做到充分的引流。2、局部或全身应用消炎药物抗感染。3、如果是缝线引起的排异反应导致的感染，一定要探查刀口内，将缝线拆除。4、充分引流后一定要用纱布包扎固定。5、一定要注意加强营养，愈合过程中禁食辛辣食物及喝酒。宜多食各种新鲜水果、蔬菜，进低脂肪、低胆固醇食品如：香菇、木耳、芹菜、豆芽、海带、藕、鱼肉、兔肉、鸡肉、鲜豆类等。宜多食干豆类及其制品。宜选用植物油，不用动物油。少吃辣椒、生蒜等刺激性食物或辛辣食品。<|im_end|>\u001b[0m\n",
      "\n",
      "  0%|          | 0/250 [00:00<?, ?it/s]d:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\py39\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:698: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "\n",
      "  0%|          | 1/250 [00:00<02:11,  1.89it/s]\n",
      "                                               \n",
      "\n",
      "  0%|          | 1/250 [00:00<02:11,  1.89it/s]\n",
      "  1%|          | 2/250 [00:00<01:41,  2.43it/s]\n",
      "  1%|          | 3/250 [00:01<01:25,  2.90it/s]\n",
      "  2%|▏         | 4/250 [00:01<01:20,  3.07it/s]\n",
      "  2%|▏         | 5/250 [00:01<01:14,  3.27it/s]\n",
      "  2%|▏         | 6/250 [00:02<01:16,  3.20it/s]\n",
      "  3%|▎         | 7/250 [00:02<01:11,  3.40it/s]\n",
      "  3%|▎         | 8/250 [00:02<01:09,  3.49it/s]\n",
      "  4%|▎         | 9/250 [00:02<01:03,  3.79it/s]\n",
      "  4%|▍         | 10/250 [00:02<00:56,  4.24it/s]\n",
      "                                                \n",
      "\n",
      "  4%|▍         | 10/250 [00:02<00:56,  4.24it/s]\n",
      "  4%|▍         | 11/250 [00:03<01:00,  3.95it/s]\n",
      "  5%|▍         | 12/250 [00:03<01:02,  3.83it/s]\n",
      "  5%|▌         | 13/250 [00:03<01:00,  3.95it/s]\n",
      "  6%|▌         | 14/250 [00:03<00:55,  4.23it/s]\n",
      "  6%|▌         | 15/250 [00:04<00:57,  4.07it/s]\n",
      "  6%|▋         | 16/250 [00:04<00:59,  3.92it/s]\n",
      "  7%|▋         | 17/250 [00:04<00:54,  4.25it/s]\n",
      "  7%|▋         | 18/250 [00:04<00:57,  4.01it/s]\n",
      "  8%|▊         | 19/250 [00:05<00:57,  4.02it/s]\n",
      "  8%|▊         | 20/250 [00:05<00:52,  4.35it/s]\n",
      "                                                \n",
      "\n",
      "  8%|▊         | 20/250 [00:05<00:52,  4.35it/s]\n",
      "  8%|▊         | 21/250 [00:05<00:53,  4.31it/s]\n",
      "  9%|▉         | 22/250 [00:05<00:50,  4.54it/s]\n",
      "  9%|▉         | 23/250 [00:06<00:53,  4.23it/s]\n",
      " 10%|▉         | 24/250 [00:06<00:53,  4.25it/s]\n",
      " 10%|█         | 25/250 [00:06<00:55,  4.03it/s]\n",
      " 10%|█         | 26/250 [00:06<00:57,  3.88it/s]\n",
      " 11%|█         | 27/250 [00:07<00:58,  3.83it/s]\n",
      " 11%|█         | 28/250 [00:07<00:54,  4.07it/s]\n",
      " 12%|█▏        | 29/250 [00:07<00:56,  3.90it/s]\n",
      " 12%|█▏        | 30/250 [00:07<00:58,  3.79it/s]\n",
      "                                                \n",
      "\n",
      " 12%|█▏        | 30/250 [00:07<00:58,  3.79it/s]\n",
      " 12%|█▏        | 31/250 [00:08<00:57,  3.82it/s]\n",
      " 13%|█▎        | 32/250 [00:08<00:56,  3.86it/s]\n",
      " 13%|█▎        | 33/250 [00:08<00:55,  3.92it/s]\n",
      " 14%|█▎        | 34/250 [00:08<00:56,  3.80it/s]\n",
      " 14%|█▍        | 35/250 [00:09<00:57,  3.72it/s]\n",
      " 14%|█▍        | 36/250 [00:09<00:56,  3.80it/s]\n",
      " 15%|█▍        | 37/250 [00:09<00:54,  3.94it/s]\n",
      " 15%|█▌        | 38/250 [00:10<00:55,  3.82it/s]\n",
      " 16%|█▌        | 39/250 [00:10<00:52,  4.00it/s]\n",
      " 16%|█▌        | 40/250 [00:10<00:49,  4.21it/s]\n",
      "                                                \n",
      "\n",
      " 16%|█▌        | 40/250 [00:10<00:49,  4.21it/s]\n",
      " 16%|█▋        | 41/250 [00:10<00:53,  3.91it/s]\n",
      " 17%|█▋        | 42/250 [00:11<00:54,  3.81it/s]\n",
      " 17%|█▋        | 43/250 [00:11<00:54,  3.80it/s]\n",
      " 18%|█▊        | 44/250 [00:11<00:54,  3.77it/s]\n",
      " 18%|█▊        | 45/250 [00:11<00:48,  4.19it/s]\n",
      " 18%|█▊        | 46/250 [00:11<00:51,  3.99it/s]\n",
      " 19%|█▉        | 47/250 [00:12<00:52,  3.85it/s]\n",
      " 19%|█▉        | 48/250 [00:12<00:51,  3.89it/s]\n",
      " 20%|█▉        | 49/250 [00:12<00:51,  3.87it/s]\n",
      " 20%|██        | 50/250 [00:12<00:47,  4.21it/s]\n",
      "                                                \n",
      "\n",
      " 20%|██        | 50/250 [00:12<00:47,  4.21it/s]\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 22.98it/s]\u001b[A\n",
      "                                                \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 20%|██        | 50/250 [00:13<00:47,  4.21it/s]\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 22.98it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[A\n",
      " 20%|██        | 51/250 [00:13<01:01,  3.24it/s]\n",
      " 21%|██        | 52/250 [00:13<00:59,  3.34it/s]\n",
      " 21%|██        | 53/250 [00:14<00:57,  3.42it/s]\n",
      " 22%|██▏       | 54/250 [00:14<00:51,  3.79it/s]\n",
      " 22%|██▏       | 55/250 [00:14<00:50,  3.86it/s]\n",
      " 22%|██▏       | 56/250 [00:14<00:49,  3.96it/s]\n",
      " 23%|██▎       | 57/250 [00:14<00:50,  3.82it/s]\n",
      " 23%|██▎       | 58/250 [00:15<00:51,  3.74it/s]\n",
      " 24%|██▎       | 59/250 [00:15<00:51,  3.68it/s]\n",
      " 24%|██▍       | 60/250 [00:15<00:52,  3.63it/s]\n",
      "                                                \n",
      "\n",
      " 24%|██▍       | 60/250 [00:15<00:52,  3.63it/s]\n",
      " 24%|██▍       | 61/250 [00:16<00:52,  3.63it/s]\n",
      " 25%|██▍       | 62/250 [00:16<00:47,  3.96it/s]\n",
      " 25%|██▌       | 63/250 [00:16<00:43,  4.30it/s]\n",
      " 26%|██▌       | 64/250 [00:16<00:43,  4.28it/s]\n",
      " 26%|██▌       | 65/250 [00:17<00:45,  4.02it/s]\n",
      " 26%|██▋       | 66/250 [00:17<00:47,  3.87it/s]\n",
      " 27%|██▋       | 67/250 [00:17<00:47,  3.82it/s]\n",
      " 27%|██▋       | 68/250 [00:17<00:46,  3.95it/s]\n",
      " 28%|██▊       | 69/250 [00:18<00:47,  3.82it/s]\n",
      " 28%|██▊       | 70/250 [00:18<00:48,  3.74it/s]\n",
      "                                                \n",
      "\n",
      " 28%|██▊       | 70/250 [00:18<00:48,  3.74it/s]\n",
      " 28%|██▊       | 71/250 [00:18<00:49,  3.60it/s]\n",
      " 29%|██▉       | 72/250 [00:18<00:49,  3.58it/s]\n",
      " 29%|██▉       | 73/250 [00:19<00:43,  4.04it/s]\n",
      " 30%|██▉       | 74/250 [00:19<00:41,  4.27it/s]\n",
      " 30%|███       | 75/250 [00:19<00:43,  4.02it/s]\n",
      " 30%|███       | 76/250 [00:19<00:44,  3.89it/s]\n",
      " 31%|███       | 77/250 [00:20<00:45,  3.79it/s]\n",
      " 31%|███       | 78/250 [00:20<00:45,  3.76it/s]\n",
      " 32%|███▏      | 79/250 [00:20<00:46,  3.69it/s]\n",
      " 32%|███▏      | 80/250 [00:20<00:46,  3.65it/s]\n",
      "                                                \n",
      "\n",
      " 32%|███▏      | 80/250 [00:20<00:46,  3.65it/s]\n",
      " 32%|███▏      | 81/250 [00:21<00:46,  3.60it/s]\n",
      " 33%|███▎      | 82/250 [00:21<00:45,  3.68it/s]\n",
      " 33%|███▎      | 83/250 [00:21<00:46,  3.61it/s]\n",
      " 34%|███▎      | 84/250 [00:22<00:45,  3.67it/s]\n",
      " 34%|███▍      | 85/250 [00:22<00:45,  3.63it/s]\n",
      " 34%|███▍      | 86/250 [00:22<00:43,  3.80it/s]\n",
      " 35%|███▍      | 87/250 [00:22<00:43,  3.79it/s]\n",
      " 35%|███▌      | 88/250 [00:23<00:43,  3.72it/s]\n",
      " 36%|███▌      | 89/250 [00:23<00:43,  3.67it/s]\n",
      " 36%|███▌      | 90/250 [00:23<00:44,  3.63it/s]\n",
      "                                                \n",
      "\n",
      " 36%|███▌      | 90/250 [00:23<00:44,  3.63it/s]\n",
      " 36%|███▋      | 91/250 [00:23<00:42,  3.70it/s]\n",
      " 37%|███▋      | 92/250 [00:24<00:42,  3.71it/s]\n",
      " 37%|███▋      | 93/250 [00:24<00:39,  3.97it/s]\n",
      " 38%|███▊      | 94/250 [00:24<00:40,  3.84it/s]\n",
      " 38%|███▊      | 95/250 [00:25<00:41,  3.75it/s]\n",
      " 38%|███▊      | 96/250 [00:25<00:40,  3.80it/s]\n",
      " 39%|███▉      | 97/250 [00:25<00:41,  3.73it/s]\n",
      " 39%|███▉      | 98/250 [00:25<00:38,  3.96it/s]\n",
      " 40%|███▉      | 99/250 [00:26<00:39,  3.83it/s]\n",
      " 40%|████      | 100/250 [00:26<00:38,  3.85it/s]\n",
      "                                                 \n",
      "\n",
      " 40%|████      | 100/250 [00:26<00:38,  3.85it/s]\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 21.46it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 40%|████      | 100/250 [00:26<00:38,  3.85it/s]\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 21.46it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[A\n",
      " 40%|████      | 101/250 [00:26<00:48,  3.04it/s]\n",
      " 41%|████      | 102/250 [00:27<00:46,  3.18it/s]\n",
      " 41%|████      | 103/250 [00:27<00:40,  3.59it/s]\n",
      " 42%|████▏     | 104/250 [00:27<00:38,  3.84it/s]\n",
      " 42%|████▏     | 105/250 [00:27<00:38,  3.77it/s]\n",
      " 42%|████▏     | 106/250 [00:28<00:38,  3.70it/s]\n",
      " 43%|████▎     | 107/250 [00:28<00:35,  4.05it/s]\n",
      " 43%|████▎     | 108/250 [00:28<00:33,  4.20it/s]\n",
      " 44%|████▎     | 109/250 [00:28<00:35,  3.98it/s]\n",
      " 44%|████▍     | 110/250 [00:29<00:36,  3.84it/s]\n",
      "                                                 \n",
      "\n",
      " 44%|████▍     | 110/250 [00:29<00:36,  3.84it/s]\n",
      " 44%|████▍     | 111/250 [00:29<00:37,  3.72it/s]\n",
      " 45%|████▍     | 112/250 [00:29<00:37,  3.67it/s]\n",
      " 45%|████▌     | 113/250 [00:29<00:33,  4.04it/s]\n",
      " 46%|████▌     | 114/250 [00:30<00:35,  3.83it/s]\n",
      " 46%|████▌     | 115/250 [00:30<00:35,  3.77it/s]\n",
      " 46%|████▋     | 116/250 [00:30<00:36,  3.67it/s]\n",
      " 47%|████▋     | 117/250 [00:30<00:36,  3.66it/s]\n",
      " 47%|████▋     | 118/250 [00:31<00:36,  3.60it/s]\n",
      " 48%|████▊     | 119/250 [00:31<00:36,  3.62it/s]\n",
      " 48%|████▊     | 120/250 [00:31<00:33,  3.86it/s]\n",
      "                                                 \n",
      "\n",
      " 48%|████▊     | 120/250 [00:31<00:33,  3.86it/s]\n",
      " 48%|████▊     | 121/250 [00:31<00:31,  4.11it/s]\n",
      " 49%|████▉     | 122/250 [00:32<00:32,  3.90it/s]\n",
      " 49%|████▉     | 123/250 [00:32<00:33,  3.76it/s]\n",
      " 50%|████▉     | 124/250 [00:32<00:34,  3.68it/s]\n",
      " 50%|█████     | 125/250 [00:33<00:34,  3.61it/s]\n",
      " 50%|█████     | 126/250 [00:33<00:32,  3.87it/s]\n",
      " 51%|█████     | 127/250 [00:33<00:31,  3.89it/s]\n",
      " 51%|█████     | 128/250 [00:33<00:29,  4.07it/s]\n",
      " 52%|█████▏    | 129/250 [00:33<00:29,  4.12it/s]\n",
      " 52%|█████▏    | 130/250 [00:34<00:30,  3.94it/s]\n",
      "                                                 \n",
      "\n",
      " 52%|█████▏    | 130/250 [00:34<00:30,  3.94it/s]\n",
      " 52%|█████▏    | 131/250 [00:34<00:26,  4.41it/s]\n",
      " 53%|█████▎    | 132/250 [00:34<00:28,  4.21it/s]\n",
      " 53%|█████▎    | 133/250 [00:34<00:29,  3.95it/s]\n",
      " 54%|█████▎    | 134/250 [00:35<00:29,  3.94it/s]\n",
      " 54%|█████▍    | 135/250 [00:35<00:29,  3.88it/s]\n",
      " 54%|█████▍    | 136/250 [00:35<00:26,  4.23it/s]\n",
      " 55%|█████▍    | 137/250 [00:35<00:25,  4.51it/s]\n",
      " 55%|█████▌    | 138/250 [00:36<00:26,  4.17it/s]\n",
      " 56%|█████▌    | 139/250 [00:36<00:26,  4.20it/s]\n",
      " 56%|█████▌    | 140/250 [00:36<00:25,  4.30it/s]\n",
      "                                                 \n",
      "\n",
      " 56%|█████▌    | 140/250 [00:36<00:25,  4.30it/s]\n",
      " 56%|█████▋    | 141/250 [00:36<00:26,  4.13it/s]\n",
      " 57%|█████▋    | 142/250 [00:37<00:26,  4.02it/s]\n",
      " 57%|█████▋    | 143/250 [00:37<00:24,  4.32it/s]\n",
      " 58%|█████▊    | 144/250 [00:37<00:25,  4.21it/s]\n",
      " 58%|█████▊    | 145/250 [00:37<00:26,  3.98it/s]\n",
      " 58%|█████▊    | 146/250 [00:38<00:26,  3.93it/s]\n",
      " 59%|█████▉    | 147/250 [00:38<00:27,  3.78it/s]\n",
      " 59%|█████▉    | 148/250 [00:38<00:27,  3.69it/s]\n",
      " 60%|█████▉    | 149/250 [00:38<00:26,  3.77it/s]\n",
      " 60%|██████    | 150/250 [00:39<00:26,  3.75it/s]\n",
      "                                                 \n",
      "\n",
      " 60%|██████    | 150/250 [00:39<00:26,  3.75it/s]\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 22.61it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 60%|██████    | 150/250 [00:39<00:26,  3.75it/s]\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 22.61it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[A\n",
      " 60%|██████    | 151/250 [00:39<00:32,  3.04it/s]\n",
      " 61%|██████    | 152/250 [00:39<00:31,  3.13it/s]\n",
      " 61%|██████    | 153/250 [00:40<00:29,  3.25it/s]\n",
      " 62%|██████▏   | 154/250 [00:40<00:28,  3.35it/s]\n",
      " 62%|██████▏   | 155/250 [00:40<00:27,  3.41it/s]\n",
      " 62%|██████▏   | 156/250 [00:41<00:25,  3.75it/s]\n",
      " 63%|██████▎   | 157/250 [00:41<00:25,  3.69it/s]\n",
      " 63%|██████▎   | 158/250 [00:41<00:25,  3.65it/s]\n",
      " 64%|██████▎   | 159/250 [00:41<00:25,  3.62it/s]\n",
      " 64%|██████▍   | 160/250 [00:42<00:24,  3.60it/s]\n",
      "                                                 \n",
      "\n",
      " 64%|██████▍   | 160/250 [00:42<00:24,  3.60it/s]\n",
      " 64%|██████▍   | 161/250 [00:42<00:24,  3.58it/s]\n",
      " 65%|██████▍   | 162/250 [00:42<00:23,  3.69it/s]\n",
      " 65%|██████▌   | 163/250 [00:42<00:23,  3.72it/s]\n",
      " 66%|██████▌   | 164/250 [00:43<00:23,  3.70it/s]\n",
      " 66%|██████▌   | 165/250 [00:43<00:20,  4.16it/s]\n",
      " 66%|██████▋   | 166/250 [00:43<00:20,  4.05it/s]\n",
      " 67%|██████▋   | 167/250 [00:43<00:21,  3.89it/s]\n",
      " 67%|██████▋   | 168/250 [00:44<00:21,  3.81it/s]\n",
      " 68%|██████▊   | 169/250 [00:44<00:20,  3.88it/s]\n",
      " 68%|██████▊   | 170/250 [00:44<00:19,  4.14it/s]\n",
      "                                                 \n",
      "\n",
      " 68%|██████▊   | 170/250 [00:44<00:19,  4.14it/s]\n",
      " 68%|██████▊   | 171/250 [00:44<00:20,  3.94it/s]\n",
      " 69%|██████▉   | 172/250 [00:45<00:20,  3.82it/s]\n",
      " 69%|██████▉   | 173/250 [00:45<00:20,  3.74it/s]\n",
      " 70%|██████▉   | 174/250 [00:45<00:20,  3.67it/s]\n",
      " 70%|███████   | 175/250 [00:45<00:18,  4.09it/s]\n",
      " 70%|███████   | 176/250 [00:46<00:18,  3.91it/s]\n",
      " 71%|███████   | 177/250 [00:46<00:19,  3.79it/s]\n",
      " 71%|███████   | 178/250 [00:46<00:18,  3.92it/s]\n",
      " 72%|███████▏  | 179/250 [00:46<00:16,  4.19it/s]\n",
      " 72%|███████▏  | 180/250 [00:47<00:15,  4.47it/s]\n",
      "                                                 \n",
      "\n",
      " 72%|███████▏  | 180/250 [00:47<00:15,  4.47it/s]\n",
      " 72%|███████▏  | 181/250 [00:47<00:15,  4.40it/s]\n",
      " 73%|███████▎  | 182/250 [00:47<00:15,  4.37it/s]\n",
      " 73%|███████▎  | 183/250 [00:47<00:16,  4.09it/s]\n",
      " 74%|███████▎  | 184/250 [00:48<00:15,  4.31it/s]\n",
      " 74%|███████▍  | 185/250 [00:48<00:15,  4.20it/s]\n",
      " 74%|███████▍  | 186/250 [00:48<00:15,  4.22it/s]\n",
      " 75%|███████▍  | 187/250 [00:48<00:14,  4.24it/s]\n",
      " 75%|███████▌  | 188/250 [00:49<00:15,  4.01it/s]\n",
      " 76%|███████▌  | 189/250 [00:49<00:15,  3.86it/s]\n",
      " 76%|███████▌  | 190/250 [00:49<00:15,  3.99it/s]\n",
      "                                                 \n",
      "\n",
      " 76%|███████▌  | 190/250 [00:49<00:15,  3.99it/s]\n",
      " 76%|███████▋  | 191/250 [00:49<00:15,  3.89it/s]\n",
      " 77%|███████▋  | 192/250 [00:50<00:14,  4.00it/s]\n",
      " 77%|███████▋  | 193/250 [00:50<00:14,  3.90it/s]\n",
      " 78%|███████▊  | 194/250 [00:50<00:14,  3.91it/s]\n",
      " 78%|███████▊  | 195/250 [00:50<00:14,  3.81it/s]\n",
      " 78%|███████▊  | 196/250 [00:51<00:14,  3.73it/s]\n",
      " 79%|███████▉  | 197/250 [00:51<00:14,  3.68it/s]\n",
      " 79%|███████▉  | 198/250 [00:51<00:14,  3.64it/s]\n",
      " 80%|███████▉  | 199/250 [00:52<00:13,  3.74it/s]\n",
      " 80%|████████  | 200/250 [00:52<00:12,  4.04it/s]\n",
      "                                                 \n",
      "\n",
      " 80%|████████  | 200/250 [00:52<00:12,  4.04it/s]\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 23.88it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      " 80%|████████  | 200/250 [00:52<00:12,  4.04it/s]\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 23.88it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[A\n",
      " 80%|████████  | 201/250 [00:52<00:15,  3.24it/s]\n",
      " 81%|████████  | 202/250 [00:52<00:14,  3.34it/s]\n",
      " 81%|████████  | 203/250 [00:53<00:13,  3.41it/s]\n",
      " 82%|████████▏ | 204/250 [00:53<00:13,  3.49it/s]\n",
      " 82%|████████▏ | 205/250 [00:53<00:12,  3.56it/s]\n",
      " 82%|████████▏ | 206/250 [00:53<00:11,  3.85it/s]\n",
      " 83%|████████▎ | 207/250 [00:54<00:11,  3.88it/s]\n",
      " 83%|████████▎ | 208/250 [00:54<00:09,  4.28it/s]\n",
      " 84%|████████▎ | 209/250 [00:54<00:09,  4.14it/s]\n",
      " 84%|████████▍ | 210/250 [00:54<00:09,  4.02it/s]\n",
      "                                                 \n",
      "\n",
      " 84%|████████▍ | 210/250 [00:54<00:09,  4.02it/s]\n",
      " 84%|████████▍ | 211/250 [00:55<00:10,  3.80it/s]\n",
      " 85%|████████▍ | 212/250 [00:55<00:10,  3.72it/s]\n",
      " 85%|████████▌ | 213/250 [00:55<00:09,  3.72it/s]\n",
      " 86%|████████▌ | 214/250 [00:55<00:08,  4.18it/s]\n",
      " 86%|████████▌ | 215/250 [00:56<00:08,  4.06it/s]\n",
      " 86%|████████▋ | 216/250 [00:56<00:08,  3.98it/s]\n",
      " 87%|████████▋ | 217/250 [00:56<00:07,  4.24it/s]\n",
      " 87%|████████▋ | 218/250 [00:56<00:07,  4.02it/s]\n",
      " 88%|████████▊ | 219/250 [00:57<00:08,  3.85it/s]\n",
      " 88%|████████▊ | 220/250 [00:57<00:07,  4.05it/s]\n",
      "                                                 \n",
      "\n",
      " 88%|████████▊ | 220/250 [00:57<00:07,  4.05it/s]\n",
      " 88%|████████▊ | 221/250 [00:57<00:07,  3.97it/s]\n",
      " 89%|████████▉ | 222/250 [00:57<00:06,  4.31it/s]\n",
      " 89%|████████▉ | 223/250 [00:58<00:06,  4.10it/s]\n",
      " 90%|████████▉ | 224/250 [00:58<00:06,  3.93it/s]\n",
      " 90%|█████████ | 225/250 [00:58<00:05,  4.20it/s]\n",
      " 90%|█████████ | 226/250 [00:58<00:05,  4.33it/s]\n",
      " 91%|█████████ | 227/250 [00:59<00:05,  4.07it/s]\n",
      " 91%|█████████ | 228/250 [00:59<00:05,  3.98it/s]\n",
      " 92%|█████████▏| 229/250 [00:59<00:05,  4.05it/s]\n",
      " 92%|█████████▏| 230/250 [00:59<00:04,  4.06it/s]\n",
      "                                                 \n",
      "\n",
      " 92%|█████████▏| 230/250 [00:59<00:04,  4.06it/s]\n",
      " 92%|█████████▏| 231/250 [01:00<00:04,  3.94it/s]\n",
      " 93%|█████████▎| 232/250 [01:00<00:04,  3.89it/s]\n",
      " 93%|█████████▎| 233/250 [01:00<00:04,  4.13it/s]\n",
      " 94%|█████████▎| 234/250 [01:00<00:03,  4.08it/s]\n",
      " 94%|█████████▍| 235/250 [01:01<00:03,  4.48it/s]\n",
      " 94%|█████████▍| 236/250 [01:01<00:03,  4.23it/s]\n",
      " 95%|█████████▍| 237/250 [01:01<00:03,  4.01it/s]\n",
      " 95%|█████████▌| 238/250 [01:01<00:02,  4.40it/s]\n",
      " 96%|█████████▌| 239/250 [01:02<00:02,  4.15it/s]\n",
      " 96%|█████████▌| 240/250 [01:02<00:02,  4.21it/s]\n",
      "                                                 \n",
      "\n",
      " 96%|█████████▌| 240/250 [01:02<00:02,  4.21it/s]\n",
      " 96%|█████████▋| 241/250 [01:02<00:02,  4.31it/s]\n",
      " 97%|█████████▋| 242/250 [01:02<00:01,  4.29it/s]\n",
      " 97%|█████████▋| 243/250 [01:02<00:01,  4.21it/s]\n",
      " 98%|█████████▊| 244/250 [01:03<00:01,  4.05it/s]\n",
      " 98%|█████████▊| 245/250 [01:03<00:01,  4.14it/s]\n",
      " 98%|█████████▊| 246/250 [01:03<00:01,  3.94it/s]\n",
      " 99%|█████████▉| 247/250 [01:04<00:00,  3.81it/s]\n",
      " 99%|█████████▉| 248/250 [01:04<00:00,  3.73it/s]\n",
      "100%|█████████▉| 249/250 [01:04<00:00,  4.01it/s]\n",
      "100%|██████████| 250/250 [01:04<00:00,  4.45it/s]\n",
      "                                                 \n",
      "\n",
      "100%|██████████| 250/250 [01:04<00:00,  4.45it/s]\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 24.41it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                             \n",
      "\u001b[A\n",
      "100%|██████████| 250/250 [01:04<00:00,  4.45it/s]\n",
      "\n",
      "100%|██████████| 3/3 [00:00<00:00, 24.41it/s]\u001b[A\n",
      "\n",
      "                                             \u001b[A\n",
      "                                                 \n",
      "\n",
      "100%|██████████| 250/250 [01:04<00:00,  4.45it/s]\n",
      "100%|██████████| 250/250 [01:04<00:00,  3.85it/s]\n",
      "\u001b[32m2024-08-21 20:35:49.191\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m957\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 64.9026, 'train_samples_per_second': 15.377, 'train_steps_per_second': 3.852, 'train_loss': 2.1874768581390382, 'epoch': 1.0, 'train_samples': 1000}\u001b[0m\n",
      "\u001b[32m2024-08-21 20:35:49.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m958\u001b[0m - \u001b[1mSaving model checkpoint to outputs-sft-v1\u001b[0m\n",
      "\u001b[32m2024-08-21 20:35:49.744\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m966\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n",
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 22.33it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 18.70it/s]\n",
      "\u001b[32m2024-08-21 20:35:49.964\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m979\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': 2.3059122562408447, 'eval_runtime': 0.1966, 'eval_samples_per_second': 50.852, 'eval_steps_per_second': 15.256, 'epoch': 1.0, 'eval_samples': 10, 'perplexity': 10.033327043981686}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python supervised_finetuning.py \\\n",
    "    --model_type auto \\\n",
    "    --model_name_or_path merged-pt \\\n",
    "    --train_file_dir ./data/finetune \\\n",
    "    --validation_file_dir ./data/finetune \\\n",
    "    --per_device_train_batch_size 4 \\\n",
    "    --per_device_eval_batch_size 4 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --use_peft True \\\n",
    "    --fp16 \\\n",
    "    --max_train_samples 1000 \\\n",
    "    --max_eval_samples 10 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --warmup_ratio 0.05 \\\n",
    "    --weight_decay 0.05 \\\n",
    "    --logging_strategy steps \\\n",
    "    --logging_steps 10 \\\n",
    "    --eval_steps 50 \\\n",
    "    --evaluation_strategy steps \\\n",
    "    --save_steps 500 \\\n",
    "    --save_strategy steps \\\n",
    "    --save_total_limit 3 \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --preprocessing_num_workers 1 \\\n",
    "    --output_dir outputs-sft-v1 \\\n",
    "    --overwrite_output_dir \\\n",
    "    --ddp_timeout 30000 \\\n",
    "    --logging_first_step True \\\n",
    "    --target_modules all \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --torch_dtype float16 \\\n",
    "    --device_map auto \\\n",
    "    --report_to tensorboard \\\n",
    "    --ddp_find_unused_parameters False \\\n",
    "    --gradient_checkpointing True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is New Volume\n",
      " Volume Serial Number is B4E8-CC63\n",
      "\n",
      " Directory of d:\\llm\\whole_process\\MedicalGPT\n",
      "\n",
      "\n",
      " Directory of d:\\llm\\whole_process\\MedicalGPT\\outputs-sft-v1\n",
      "\n",
      "06/13/2024  01:37 PM    <DIR>          .\n",
      "08/21/2024  08:24 PM    <DIR>          ..\n",
      "08/21/2024  08:35 PM               746 adapter_config.json\n",
      "08/21/2024  08:35 PM        30,026,872 adapter_model.safetensors\n",
      "08/21/2024  08:35 PM                85 added_tokens.json\n",
      "08/21/2024  08:35 PM               446 all_results.json\n",
      "08/21/2024  08:35 PM               231 eval_results.json\n",
      "08/21/2024  08:35 PM         1,821,636 merges.txt\n",
      "08/21/2024  08:35 PM             5,097 README.md\n",
      "08/21/2024  08:34 PM    <DIR>          runs\n",
      "08/21/2024  08:35 PM               417 special_tokens_map.json\n",
      "08/21/2024  08:35 PM             1,377 tokenizer_config.json\n",
      "08/21/2024  08:35 PM               199 train_results.json\n",
      "08/21/2024  08:35 PM             6,060 trainer_state.json\n",
      "08/21/2024  08:35 PM         3,535,052 vocab.json\n",
      "              12 File(s)     35,398,218 bytes\n",
      "               3 Dir(s)  436,092,338,176 bytes free\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File Not Found\n"
     ]
    }
   ],
   "source": [
    "%ls -lh outputs-sft-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "模型训练结果：\n",
    "- 使用lora训练模型，则保存的lora权重是`adapter_model.bin`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
    "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model_type='auto', base_model='merged-pt', tokenizer_path=None, lora_model='outputs-sft-v1', resize_emb=False, output_dir='./merged-sft', hf_hub_model_id='', hf_hub_token=None)\n",
      "Base model: merged-pt\n",
      "LoRA model: outputs-sft-v1\n",
      "Loading LoRA for causal language model\n",
      "Merging with merge_and_unload...\n",
      "Saving to Hugging Face format...\n",
      "Done! model saved to ./merged-sft\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "!python merge_peft_adapter.py --model_type auto \\\n",
    "    --base_model merged-pt --lora_model outputs-sft-v1 --output_dir ./merged-sft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Invalid switch - \"\".\n"
     ]
    }
   ],
   "source": [
    "%ls -lh merged-sft/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%cat` not found.\n"
     ]
    }
   ],
   "source": [
    "%cat merged-sft/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Stage2 SFT训练完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-15T14:07:40.752635Z",
     "start_time": "2023-06-15T14:07:40.731186Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Stage 3: DPO(Direct Preference Optimization)\n",
    "\n",
    "第三阶段：DPO(Direct Preference Optimization)直接偏好优化，DPO通过直接优化语言模型来实现对其行为的精确控制，而无需使用复杂的强化学习，也可以有效学习到人类偏好，DPO相较于RLHF更容易实现且易于训练，效果更好\n",
    "\n",
    "| Stage 3: Direct Preference Optimization        |  [dpo_training.py](https://github.com/shibing624/MedicalGPT/blob/main/dpo_training.py) | [run_dpo.sh](https://github.com/shibing624/MedicalGPT/blob/main/run_dpo.sh)    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 说明：\n",
    "以下 notebook/colab 代码为了快速验证训练代码可用，我们使用了小size的生成模型和小样本数据集，实际使用时，需要使用更大的模型和数据集，以获得更好的效果。\n",
    "\n",
    "1. 生成模型：使用的是Bloom的`bigscience/bloomz-560m` 或者 Stage2得到的SFT模型\n",
    "2. 数据集：DPO阶段使用的是医疗reward数据，抽样了500条，位于`data/reward`文件夹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Stage3 咱们开始吧\n",
    "\n",
    "训练步骤如下：\n",
    "\n",
    "1. 确认训练集\n",
    "2. 执行训练脚本\n",
    "\n",
    "训练脚本的执行逻辑如下：\n",
    "1. 导入依赖包\n",
    "2. 设置参数\n",
    "3. 定义各函数并加载训练集\n",
    "4. 加载模型和tokenizer\n",
    "5. 开始训练并评估\n",
    "6. 查看训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is New Volume\n",
      " Volume Serial Number is B4E8-CC63\n",
      "\n",
      " Directory of d:\\llm\\whole_process\\MedicalGPT\\Qwen1.5-1.8B-Chat\n",
      "\n",
      "08/21/2024  08:24 PM    <DIR>          .\n",
      "08/21/2024  08:24 PM    <DIR>          ..\n",
      "04/30/2024  03:49 PM             1,554 .gitattributes\n",
      "08/21/2024  08:24 PM    <DIR>          __pycache__\n",
      "08/21/2024  08:24 PM    <DIR>          chatgpt\n",
      "04/30/2024  03:49 PM               689 config.json\n",
      "04/30/2024  03:49 PM                55 configuration.json\n",
      "04/30/2024  03:49 PM               219 generation_config.json\n",
      "04/30/2024  03:49 PM             7,335 LICENSE\n",
      "04/30/2024  07:58 PM             5,856 main.py\n",
      "04/30/2024  03:49 PM         1,823,226 merges.txt\n",
      "04/30/2024  07:06 PM               960 mess_test.py\n",
      "04/30/2024  07:38 PM               822 message_test.py\n",
      "04/30/2024  03:52 PM     3,673,690,696 model.safetensors\n",
      "04/26/2024  08:22 PM             3,196 openai_api_request.py\n",
      "04/30/2024  03:49 PM             4,344 README.md\n",
      "08/21/2024  08:24 PM    <DIR>          test\n",
      "08/21/2024  08:24 PM    <DIR>          test2\n",
      "08/21/2024  08:24 PM    <DIR>          test3\n",
      "08/21/2024  08:24 PM    <DIR>          test4\n",
      "08/21/2024  08:24 PM    <DIR>          test5\n",
      "04/30/2024  03:49 PM         7,331,125 tokenizer.json\n",
      "04/30/2024  03:49 PM             1,327 tokenizer_config.json\n",
      "04/30/2024  03:49 PM         2,776,833 vocab.json\n",
      "04/30/2024  04:03 PM             6,850 web_demo_gradio.py\n",
      "04/30/2024  04:23 PM             3,065 web_demo_streamlit.py\n",
      "04/30/2024  10:20 PM                 0 web_mod.py\n",
      "              18 File(s)  3,685,658,152 bytes\n",
      "               9 Dir(s)  433,346,654,208 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls Qwen1.5-1.8B-Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'system': '', 'history': [], 'question': \"You will be given a definition of a task first, then some input of the task.\\nThis task is about using the specified sentence and converting the sentence to Resource Description Framework (RDF) triplets of the form (subject, predicate object). The RDF triplets generated must be such that the triplets accurately capture the structure and semantics of the input sentence. The input is a sentence and the output is a list of triplets of the form [subject, predicate, object] that capture the relationships present in the sentence. When a sentence has more than 1 RDF triplet possible, the output must contain all of them.\\n\\nAFC Ajax (amateurs)'s ground is Sportpark De Toekomst where Ajax Youth Academy also play.\\nOutput:\", 'response_chosen': '[\\n  [\"AFC Ajax (amateurs)\", \"has ground\", \"Sportpark De Toekomst\"],\\n  [\"Ajax Youth Academy\", \"plays at\", \"Sportpark De Toekomst\"]\\n]', 'response_rejected': \" Sure, I'd be happy to help! Here are the RDF triplets for the input sentence:\\n\\n[AFC Ajax (amateurs), hasGround, Sportpark De Toekomst]\\n[Ajax Youth Academy, playsAt, Sportpark De Toekomst]\\n\\nExplanation:\\n\\n* AFC Ajax (amateurs) is the subject of the first triplet, and hasGround is the predicate that describes the relationship between AFC Ajax (amateurs) and Sportpark De Toekomst.\\n* Ajax Youth Academy is the subject of the second triplet, and playsAt is the predicate that describes the relationship between Ajax Youth Academy and Sportpark De Toekomst.\\n\\nNote that there may be other possible RDF triplets that could be derived from the input sentence, but the above triplets capture the main relationships present in the sentence.\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Define the path to your JSON file\n",
    "json_file_path = r'data\\reward\\orca_rlhf_mod.jsonl'\n",
    "\n",
    "# Open the JSON file and load its content\n",
    "with open(json_file_path, 'r') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "# Now, `data` is a Python dictionary (or list, depending on the JSON structure)\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the path to the input and output JSONL files\n",
    "input_jsonl_file_path = r'D:\\llm\\whole_process\\zuoye\\orca_rlhf.jsonl'\n",
    "output_jsonl_file_path = r'data\\reward\\orca_rlhf_mod.jsonl'\n",
    "\n",
    "def modify_keys_and_add_history(json_obj):\n",
    "    # Ensure the JSON object contains the required keys\n",
    "    if 'system' in json_obj and 'question' in json_obj:\n",
    "        # Create a new ordered dictionary to maintain the order of keys\n",
    "        modified_obj = {}\n",
    "        for key, value in json_obj.items():\n",
    "            if key == 'chosen':\n",
    "                modified_obj['response_chosen'] = value\n",
    "            elif key == 'rejected':\n",
    "                modified_obj['response_rejected'] = value\n",
    "            elif key == 'system':\n",
    "                modified_obj[key] = value\n",
    "                modified_obj['history'] = []\n",
    "            else:\n",
    "                modified_obj[key] = value\n",
    "        return modified_obj\n",
    "    return json_obj\n",
    "\n",
    "\n",
    "# Read, modify, and write the JSONL file\n",
    "with open(input_jsonl_file_path, 'r') as infile, open(output_jsonl_file_path, 'w') as outfile:\n",
    "    for line in infile:\n",
    "        json_obj = json.loads(line)\n",
    "        modified_obj = modify_keys_and_add_history(json_obj)\n",
    "        outfile.write(json.dumps(modified_obj) + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoModelForCausalLMWithValueHead(\n",
      "  (pretrained_model): Qwen2ForCausalLM(\n",
      "    (model): Qwen2Model(\n",
      "      (embed_tokens): Embedding(151936, 2048)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x Qwen2DecoderLayer(\n",
      "          (self_attn): Qwen2SdpaAttention(\n",
      "            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (k_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (v_proj): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (rotary_emb): Qwen2RotaryEmbedding()\n",
      "          )\n",
      "          (mlp): Qwen2MLP(\n",
      "            (gate_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "            (up_proj): Linear(in_features=2048, out_features=5504, bias=False)\n",
      "            (down_proj): Linear(in_features=5504, out_features=2048, bias=False)\n",
      "            (act_fn): SiLU()\n",
      "          )\n",
      "          (input_layernorm): Qwen2RMSNorm()\n",
      "          (post_attention_layernorm): Qwen2RMSNorm()\n",
      "        )\n",
      "      )\n",
      "      (norm): Qwen2RMSNorm()\n",
      "    )\n",
      "    (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
      "  )\n",
      "  (v_head): ValueHead(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "    (summary): Linear(in_features=2048, out_features=1, bias=True)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from trl import AutoModelForCausalLMWithValueHead\n",
    "\n",
    "model_name_or_path = './Qwen1.5-1.8B-Chat'  # Path to your model\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(model_name_or_path)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is New Volume\n",
      " Volume Serial Number is B4E8-CC63\n",
      "\n",
      " Directory of d:\\llm\\whole_process\\MedicalGPT\\data\\reward\n",
      "\n",
      "06/15/2024  10:42 AM    <DIR>          .\n",
      "06/12/2024  07:20 PM    <DIR>          ..\n",
      "06/15/2024  10:43 AM        36,747,287 orca_rlhf_mod.jsonl\n",
      "               1 File(s)     36,747,287 bytes\n",
      "               2 Dir(s)  433,346,654,208 bytes free\n"
     ]
    }
   ],
   "source": [
    "%ls data\\reward\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Clear CUDA memory cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1572864 || all params: 1838401536 || trainable%: 0.08555606428736143\n",
      "{'loss': 0.6931, 'grad_norm': 2.6533761024475098, 'learning_rate': 5e-06, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -139.6123504638672, 'logps/chosen': -40.92887878417969, 'logits/rejected': -2.5189437866210938, 'logits/chosen': -1.860050916671753, 'epoch': 0.04}\n",
      "{'loss': 0.6931, 'grad_norm': 2.7892494201660156, 'learning_rate': 1e-05, 'rewards/chosen': 0.0, 'rewards/rejected': 0.0, 'rewards/accuracies': 0.0, 'rewards/margins': 0.0, 'logps/rejected': -177.3958740234375, 'logps/chosen': -83.04783630371094, 'logits/rejected': -1.9823558330535889, 'logits/chosen': -1.9620705842971802, 'epoch': 0.08}\n",
      "{'loss': 0.6945, 'grad_norm': 2.251851797103882, 'learning_rate': 1.5e-05, 'rewards/chosen': -0.00027399061946198344, 'rewards/rejected': 0.002441120333969593, 'rewards/accuracies': 0.25, 'rewards/margins': -0.002715111244469881, 'logps/rejected': -96.19709777832031, 'logps/chosen': -74.74574279785156, 'logits/rejected': -2.414177417755127, 'logits/chosen': -2.446559190750122, 'epoch': 0.12}\n",
      "{'loss': 0.6935, 'grad_norm': 1.7867050170898438, 'learning_rate': 2e-05, 'rewards/chosen': 0.0008289813995361328, 'rewards/rejected': 0.0016311168437823653, 'rewards/accuracies': 0.5, 'rewards/margins': -0.0008021355024538934, 'logps/rejected': -64.59234619140625, 'logps/chosen': -45.47614669799805, 'logits/rejected': -2.6664721965789795, 'logits/chosen': -2.5478644371032715, 'epoch': 0.16}\n",
      "{'loss': 0.6907, 'grad_norm': 1.9712355136871338, 'learning_rate': 2.5e-05, 'rewards/chosen': 0.002386474749073386, 'rewards/rejected': -0.002612972166389227, 'rewards/accuracies': 0.75, 'rewards/margins': 0.0049994466826319695, 'logps/rejected': -80.53486633300781, 'logps/chosen': -49.99530029296875, 'logits/rejected': -2.6305055618286133, 'logits/chosen': -2.3599698543548584, 'epoch': 0.2}\n",
      "{'loss': 0.6942, 'grad_norm': 1.8290557861328125, 'learning_rate': 3e-05, 'rewards/chosen': -0.0005060195690020919, 'rewards/rejected': 0.0016127587296068668, 'rewards/accuracies': 0.0, 'rewards/margins': -0.002118778182193637, 'logps/rejected': -64.73138427734375, 'logps/chosen': -65.90971374511719, 'logits/rejected': -2.6075243949890137, 'logits/chosen': -2.4097936153411865, 'epoch': 0.24}\n",
      "{'loss': 0.6856, 'grad_norm': 3.6420788764953613, 'learning_rate': 3.5000000000000004e-05, 'rewards/chosen': -7.123954128473997e-05, 'rewards/rejected': -0.015263557434082031, 'rewards/accuracies': 0.75, 'rewards/margins': 0.015192318707704544, 'logps/rejected': -173.64126586914062, 'logps/chosen': -68.64642333984375, 'logits/rejected': -2.6359364986419678, 'logits/chosen': -2.1128785610198975, 'epoch': 0.29}\n",
      "{'loss': 0.6883, 'grad_norm': 2.2855782508850098, 'learning_rate': 4e-05, 'rewards/chosen': 0.0014597892295569181, 'rewards/rejected': -0.008288097567856312, 'rewards/accuracies': 1.0, 'rewards/margins': 0.009747887030243874, 'logps/rejected': -74.59375762939453, 'logps/chosen': -60.67374801635742, 'logits/rejected': -2.7919921875, 'logits/chosen': -2.5450587272644043, 'epoch': 0.33}\n",
      "{'loss': 0.6868, 'grad_norm': 2.767033338546753, 'learning_rate': 4.4999999999999996e-05, 'rewards/chosen': 0.00031416420824825764, 'rewards/rejected': -0.012491226196289062, 'rewards/accuracies': 1.0, 'rewards/margins': 0.012805390171706676, 'logps/rejected': -140.61715698242188, 'logps/chosen': -83.103759765625, 'logits/rejected': -2.65360689163208, 'logits/chosen': -2.055295944213867, 'epoch': 0.37}\n",
      "{'loss': 0.6831, 'grad_norm': 2.293832778930664, 'learning_rate': 5e-05, 'rewards/chosen': 0.005823207087814808, 'rewards/rejected': -0.014484405517578125, 'rewards/accuracies': 1.0, 'rewards/margins': 0.02030761167407036, 'logps/rejected': -133.3685302734375, 'logps/chosen': -98.81468200683594, 'logits/rejected': -2.599489450454712, 'logits/chosen': -2.327723741531372, 'epoch': 0.41}\n",
      "{'loss': 0.6725, 'grad_norm': 2.220648765563965, 'learning_rate': 5.5e-05, 'rewards/chosen': 0.006932521238923073, 'rewards/rejected': -0.03487758710980415, 'rewards/accuracies': 1.0, 'rewards/margins': 0.041810110211372375, 'logps/rejected': -115.2291030883789, 'logps/chosen': -66.57440185546875, 'logits/rejected': -2.5113558769226074, 'logits/chosen': -2.3830630779266357, 'epoch': 0.45}\n",
      "{'loss': 0.6624, 'grad_norm': 2.7580320835113525, 'learning_rate': 6e-05, 'rewards/chosen': 0.009650994092226028, 'rewards/rejected': -0.05316796153783798, 'rewards/accuracies': 1.0, 'rewards/margins': 0.06281895935535431, 'logps/rejected': -128.20230102539062, 'logps/chosen': -88.71305084228516, 'logits/rejected': -2.429699659347534, 'logits/chosen': -2.5237090587615967, 'epoch': 0.49}\n",
      "{'loss': 0.6241, 'grad_norm': 3.8377933502197266, 'learning_rate': 6.500000000000001e-05, 'rewards/chosen': 0.014048958197236061, 'rewards/rejected': -0.12928353250026703, 'rewards/accuracies': 1.0, 'rewards/margins': 0.14333248138427734, 'logps/rejected': -226.49502563476562, 'logps/chosen': -96.01658630371094, 'logits/rejected': -2.331101655960083, 'logits/chosen': -2.2371346950531006, 'epoch': 0.53}\n",
      "{'loss': 0.5935, 'grad_norm': 3.481494426727295, 'learning_rate': 7.000000000000001e-05, 'rewards/chosen': 0.027362346649169922, 'rewards/rejected': -0.1855979859828949, 'rewards/accuracies': 1.0, 'rewards/margins': 0.2129603624343872, 'logps/rejected': -165.64378356933594, 'logps/chosen': -109.0013656616211, 'logits/rejected': -2.506222724914551, 'logits/chosen': -2.366088390350342, 'epoch': 0.57}\n",
      "{'loss': 0.6524, 'grad_norm': 2.101217746734619, 'learning_rate': 7.5e-05, 'rewards/chosen': 0.015779493376612663, 'rewards/rejected': -0.06796436011791229, 'rewards/accuracies': 1.0, 'rewards/margins': 0.0837438553571701, 'logps/rejected': -80.68675231933594, 'logps/chosen': -63.467987060546875, 'logits/rejected': -2.660517692565918, 'logits/chosen': -2.3730971813201904, 'epoch': 0.61}\n",
      "{'loss': 0.6135, 'grad_norm': 2.69488263130188, 'learning_rate': 8e-05, 'rewards/chosen': 0.02099236287176609, 'rewards/rejected': -0.14853791892528534, 'rewards/accuracies': 1.0, 'rewards/margins': 0.16953028738498688, 'logps/rejected': -111.39215850830078, 'logps/chosen': -87.40739440917969, 'logits/rejected': -2.5828208923339844, 'logits/chosen': -2.422348976135254, 'epoch': 0.65}\n",
      "{'loss': 0.5883, 'grad_norm': 2.799720525741577, 'learning_rate': 8.5e-05, 'rewards/chosen': 0.004825758747756481, 'rewards/rejected': -0.2235834151506424, 'rewards/accuracies': 1.0, 'rewards/margins': 0.22840918600559235, 'logps/rejected': -154.9222412109375, 'logps/chosen': -68.91036987304688, 'logits/rejected': -2.598980665206909, 'logits/chosen': -2.395517349243164, 'epoch': 0.69}\n",
      "{'loss': 0.5416, 'grad_norm': 2.6630942821502686, 'learning_rate': 8.999999999999999e-05, 'rewards/chosen': 0.056995779275894165, 'rewards/rejected': -0.2967521548271179, 'rewards/accuracies': 1.0, 'rewards/margins': 0.3537479639053345, 'logps/rejected': -152.69882202148438, 'logps/chosen': -63.985450744628906, 'logits/rejected': -2.311917304992676, 'logits/chosen': -2.1701488494873047, 'epoch': 0.73}\n",
      "{'loss': 0.5325, 'grad_norm': 2.820277690887451, 'learning_rate': 9.5e-05, 'rewards/chosen': 0.07852153480052948, 'rewards/rejected': -0.28086453676223755, 'rewards/accuracies': 1.0, 'rewards/margins': 0.3593860864639282, 'logps/rejected': -183.69277954101562, 'logps/chosen': -68.84095764160156, 'logits/rejected': -2.457395553588867, 'logits/chosen': -1.9237382411956787, 'epoch': 0.78}\n",
      "{'loss': 0.452, 'grad_norm': 2.625664234161377, 'learning_rate': 0.0001, 'rewards/chosen': 0.1140870600938797, 'rewards/rejected': -0.4802674651145935, 'rewards/accuracies': 1.0, 'rewards/margins': 0.594354510307312, 'logps/rejected': -176.11831665039062, 'logps/chosen': -63.22773361206055, 'logits/rejected': -2.6890482902526855, 'logits/chosen': -2.168381690979004, 'epoch': 0.82}\n",
      "{'eval_loss': 0.6150219440460205, 'eval_runtime': 0.0577, 'eval_samples_per_second': 17.324, 'eval_steps_per_second': 17.324, 'eval_rewards/chosen': 0.03340435028076172, 'eval_rewards/rejected': -0.1294708251953125, 'eval_rewards/accuracies': 1.0, 'eval_rewards/margins': 0.16287517547607422, 'eval_logps/rejected': -37.086246490478516, 'eval_logps/chosen': -13.530881881713867, 'eval_logits/rejected': -2.6693241596221924, 'eval_logits/chosen': -2.274200916290283, 'epoch': 0.82}\n",
      "{'loss': 0.5348, 'grad_norm': 2.8494486808776855, 'learning_rate': 0.000105, 'rewards/chosen': 0.08140318095684052, 'rewards/rejected': -0.3048463761806488, 'rewards/accuracies': 0.75, 'rewards/margins': 0.3862495422363281, 'logps/rejected': -137.9550018310547, 'logps/chosen': -119.10314178466797, 'logits/rejected': -2.165329933166504, 'logits/chosen': -1.7602674961090088, 'epoch': 0.86}\n",
      "{'loss': 0.5382, 'grad_norm': 2.5759458541870117, 'learning_rate': 0.00011, 'rewards/chosen': 0.058968186378479004, 'rewards/rejected': -0.28124579787254333, 'rewards/accuracies': 1.0, 'rewards/margins': 0.34021395444869995, 'logps/rejected': -94.22265625, 'logps/chosen': -58.65491485595703, 'logits/rejected': -2.1695210933685303, 'logits/chosen': -1.8522133827209473, 'epoch': 0.9}\n",
      "{'loss': 0.3414, 'grad_norm': 2.409597396850586, 'learning_rate': 0.000115, 'rewards/chosen': 0.1467880755662918, 'rewards/rejected': -0.9412903785705566, 'rewards/accuracies': 1.0, 'rewards/margins': 1.0880783796310425, 'logps/rejected': -199.87257385253906, 'logps/chosen': -35.372291564941406, 'logits/rejected': -2.644505023956299, 'logits/chosen': -1.9396438598632812, 'epoch': 0.94}\n",
      "{'loss': 0.6595, 'grad_norm': nan, 'learning_rate': 0.000115, 'rewards/chosen': -0.10967700928449631, 'rewards/rejected': -0.18360042572021484, 'rewards/accuracies': 0.75, 'rewards/margins': 0.07392343133687973, 'logps/rejected': -68.69580078125, 'logps/chosen': -33.56473922729492, 'logits/rejected': -2.5279700756073, 'logits/chosen': -2.168577194213867, 'epoch': 0.98}\n",
      "{'loss': 0.4887, 'grad_norm': 2.6415021419525146, 'learning_rate': 0.00012, 'rewards/chosen': 0.17549529671669006, 'rewards/rejected': -0.3564320504665375, 'rewards/accuracies': 1.0, 'rewards/margins': 0.5319273471832275, 'logps/rejected': -125.65724182128906, 'logps/chosen': -118.82440185546875, 'logits/rejected': -1.8621103763580322, 'logits/chosen': -1.9141355752944946, 'epoch': 1.02}\n",
      "{'loss': 0.2912, 'grad_norm': 1.6439094543457031, 'learning_rate': 0.000125, 'rewards/chosen': 0.2519218921661377, 'rewards/rejected': -1.095865249633789, 'rewards/accuracies': 1.0, 'rewards/margins': 1.3477871417999268, 'logps/rejected': -158.8124542236328, 'logps/chosen': -67.89947509765625, 'logits/rejected': -2.5717334747314453, 'logits/chosen': -2.1655149459838867, 'epoch': 1.06}\n",
      "{'loss': 0.1516, 'grad_norm': 1.7653788328170776, 'learning_rate': 0.00013000000000000002, 'rewards/chosen': 0.1757824867963791, 'rewards/rejected': -1.7243478298187256, 'rewards/accuracies': 1.0, 'rewards/margins': 1.900130271911621, 'logps/rejected': -185.72433471679688, 'logps/chosen': -71.4977035522461, 'logits/rejected': -2.336392879486084, 'logits/chosen': -2.240290880203247, 'epoch': 1.1}\n",
      "{'loss': 0.345, 'grad_norm': 1.912458062171936, 'learning_rate': 0.000135, 'rewards/chosen': 0.27823498845100403, 'rewards/rejected': -0.6825671195983887, 'rewards/accuracies': 1.0, 'rewards/margins': 0.9608021974563599, 'logps/rejected': -122.52029418945312, 'logps/chosen': -38.99402618408203, 'logits/rejected': -2.3540704250335693, 'logits/chosen': -2.1297473907470703, 'epoch': 1.14}\n",
      "{'loss': 0.1663, 'grad_norm': 1.3259910345077515, 'learning_rate': 0.00014000000000000001, 'rewards/chosen': 0.4651467204093933, 'rewards/rejected': -1.7925465106964111, 'rewards/accuracies': 1.0, 'rewards/margins': 2.257693290710449, 'logps/rejected': -140.7316131591797, 'logps/chosen': -49.54093551635742, 'logits/rejected': -2.5297069549560547, 'logits/chosen': -2.1217637062072754, 'epoch': 1.18}\n",
      "{'loss': 0.4452, 'grad_norm': nan, 'learning_rate': 0.00014000000000000001, 'rewards/chosen': -0.15267300605773926, 'rewards/rejected': -1.4762024879455566, 'rewards/accuracies': 0.75, 'rewards/margins': 1.3235294818878174, 'logps/rejected': -113.55754089355469, 'logps/chosen': -54.136688232421875, 'logits/rejected': -2.444270133972168, 'logits/chosen': -2.193682909011841, 'epoch': 1.22}\n",
      "{'loss': 0.2511, 'grad_norm': 1.754855751991272, 'learning_rate': 0.000145, 'rewards/chosen': 0.19858866930007935, 'rewards/rejected': -1.551737666130066, 'rewards/accuracies': 1.0, 'rewards/margins': 1.7503262758255005, 'logps/rejected': -163.62217712402344, 'logps/chosen': -78.8060302734375, 'logits/rejected': -2.552403450012207, 'logits/chosen': -2.035069465637207, 'epoch': 1.27}\n",
      "{'loss': 0.2493, 'grad_norm': 1.3186421394348145, 'learning_rate': 0.00015, 'rewards/chosen': 0.19010767340660095, 'rewards/rejected': -3.0332818031311035, 'rewards/accuracies': 1.0, 'rewards/margins': 3.2233896255493164, 'logps/rejected': -171.42869567871094, 'logps/chosen': -62.592430114746094, 'logits/rejected': -2.55397891998291, 'logits/chosen': -2.461766242980957, 'epoch': 1.31}\n",
      "{'loss': 0.0747, 'grad_norm': 0.9477246999740601, 'learning_rate': 0.000155, 'rewards/chosen': 0.048834800720214844, 'rewards/rejected': -3.1592025756835938, 'rewards/accuracies': 1.0, 'rewards/margins': 3.2080373764038086, 'logps/rejected': -198.9907684326172, 'logps/chosen': -113.85016632080078, 'logits/rejected': -2.3831892013549805, 'logits/chosen': -2.3104002475738525, 'epoch': 1.35}\n",
      "{'loss': 0.2144, 'grad_norm': 1.542109727859497, 'learning_rate': 0.00016, 'rewards/chosen': 0.2334306538105011, 'rewards/rejected': -2.2865407466888428, 'rewards/accuracies': 1.0, 'rewards/margins': 2.5199713706970215, 'logps/rejected': -151.62332153320312, 'logps/chosen': -52.38156509399414, 'logits/rejected': -2.223414421081543, 'logits/chosen': -1.5433650016784668, 'epoch': 1.39}\n",
      "{'loss': 0.2341, 'grad_norm': 1.7015739679336548, 'learning_rate': 0.000165, 'rewards/chosen': 0.13844594359397888, 'rewards/rejected': -1.429690957069397, 'rewards/accuracies': 1.0, 'rewards/margins': 1.5681368112564087, 'logps/rejected': -111.9674072265625, 'logps/chosen': -38.19160842895508, 'logits/rejected': -2.4865784645080566, 'logits/chosen': -2.0635437965393066, 'epoch': 1.43}\n",
      "{'loss': 0.2635, 'grad_norm': 1.4215115308761597, 'learning_rate': 0.00017, 'rewards/chosen': 0.10268905013799667, 'rewards/rejected': -2.5762526988983154, 'rewards/accuracies': 1.0, 'rewards/margins': 2.678941488265991, 'logps/rejected': -117.55648803710938, 'logps/chosen': -52.84212875366211, 'logits/rejected': -2.466892957687378, 'logits/chosen': -2.06095290184021, 'epoch': 1.47}\n",
      "{'loss': 0.1247, 'grad_norm': 1.156715989112854, 'learning_rate': 0.000175, 'rewards/chosen': 0.2796994149684906, 'rewards/rejected': -3.4539008140563965, 'rewards/accuracies': 1.0, 'rewards/margins': 3.733600616455078, 'logps/rejected': -161.9485321044922, 'logps/chosen': -41.802330017089844, 'logits/rejected': -2.2669436931610107, 'logits/chosen': -1.892129898071289, 'epoch': 1.51}\n",
      "{'loss': 0.1353, 'grad_norm': 1.2098493576049805, 'learning_rate': 0.00017999999999999998, 'rewards/chosen': 0.156703382730484, 'rewards/rejected': -2.215924024581909, 'rewards/accuracies': 1.0, 'rewards/margins': 2.3726274967193604, 'logps/rejected': -132.9895782470703, 'logps/chosen': -87.40440368652344, 'logits/rejected': -2.4661178588867188, 'logits/chosen': -2.261110544204712, 'epoch': 1.55}\n",
      "{'loss': 0.006, 'grad_norm': 0.14482861757278442, 'learning_rate': 0.000185, 'rewards/chosen': 0.45525801181793213, 'rewards/rejected': -5.3841552734375, 'rewards/accuracies': 1.0, 'rewards/margins': 5.839413166046143, 'logps/rejected': -235.12545776367188, 'logps/chosen': -55.056758880615234, 'logits/rejected': -2.424351215362549, 'logits/chosen': -1.710449457168579, 'epoch': 1.59}\n",
      "{'loss': 0.0544, 'grad_norm': 1.0219428539276123, 'learning_rate': 0.00019, 'rewards/chosen': -0.33747991919517517, 'rewards/rejected': -7.42466402053833, 'rewards/accuracies': 1.0, 'rewards/margins': 7.087184429168701, 'logps/rejected': -236.40487670898438, 'logps/chosen': -93.67021179199219, 'logits/rejected': -2.3855643272399902, 'logits/chosen': -2.161487340927124, 'epoch': 1.63}\n",
      "{'eval_loss': 0.059167325496673584, 'eval_runtime': 0.0638, 'eval_samples_per_second': 15.664, 'eval_steps_per_second': 15.664, 'eval_rewards/chosen': 0.5863364338874817, 'eval_rewards/rejected': -2.2113196849823, 'eval_rewards/accuracies': 1.0, 'eval_rewards/margins': 2.7976560592651367, 'eval_logps/rejected': -57.90473556518555, 'eval_logps/chosen': -8.001561164855957, 'eval_logits/rejected': -2.4748246669769287, 'eval_logits/chosen': -2.018378973007202, 'epoch': 1.63}\n",
      "{'loss': 0.0743, 'grad_norm': 0.9105450510978699, 'learning_rate': 0.00019500000000000002, 'rewards/chosen': 0.5203937292098999, 'rewards/rejected': -2.916799783706665, 'rewards/accuracies': 1.0, 'rewards/margins': 3.4371936321258545, 'logps/rejected': -104.90677642822266, 'logps/chosen': -26.399017333984375, 'logits/rejected': -2.2414355278015137, 'logits/chosen': -1.8168309926986694, 'epoch': 1.67}\n",
      "{'loss': 0.0584, 'grad_norm': 0.6538698077201843, 'learning_rate': 0.0002, 'rewards/chosen': 0.6134827733039856, 'rewards/rejected': -3.681403875350952, 'rewards/accuracies': 1.0, 'rewards/margins': 4.294886589050293, 'logps/rejected': -146.68734741210938, 'logps/chosen': -34.52654266357422, 'logits/rejected': -2.262294292449951, 'logits/chosen': -2.051802396774292, 'epoch': 1.71}\n",
      "{'loss': 0.0927, 'grad_norm': 1.159350872039795, 'learning_rate': 0.000205, 'rewards/chosen': 0.028017766773700714, 'rewards/rejected': -4.458138465881348, 'rewards/accuracies': 1.0, 'rewards/margins': 4.486156463623047, 'logps/rejected': -157.79495239257812, 'logps/chosen': -105.58950805664062, 'logits/rejected': -2.0079689025878906, 'logits/chosen': -1.8876361846923828, 'epoch': 1.76}\n",
      "{'loss': 0.0518, 'grad_norm': 0.6314859986305237, 'learning_rate': 0.00021, 'rewards/chosen': 0.2947385311126709, 'rewards/rejected': -5.370021820068359, 'rewards/accuracies': 1.0, 'rewards/margins': 5.664759635925293, 'logps/rejected': -193.17813110351562, 'logps/chosen': -81.1954574584961, 'logits/rejected': -2.277172088623047, 'logits/chosen': -1.8948701620101929, 'epoch': 1.8}\n",
      "{'loss': 0.0015, 'grad_norm': 0.04660985991358757, 'learning_rate': 0.000215, 'rewards/chosen': -0.12023153901100159, 'rewards/rejected': -9.609930038452148, 'rewards/accuracies': 1.0, 'rewards/margins': 9.48969841003418, 'logps/rejected': -274.955810546875, 'logps/chosen': -93.524658203125, 'logits/rejected': -2.14076828956604, 'logits/chosen': -1.9811547994613647, 'epoch': 1.84}\n",
      "{'loss': 0.0976, 'grad_norm': 1.30251944065094, 'learning_rate': 0.00022, 'rewards/chosen': 0.1579846292734146, 'rewards/rejected': -5.44072961807251, 'rewards/accuracies': 1.0, 'rewards/margins': 5.598714828491211, 'logps/rejected': -171.2467803955078, 'logps/chosen': -53.406978607177734, 'logits/rejected': -2.373741388320923, 'logits/chosen': -1.876283884048462, 'epoch': 1.88}\n",
      "{'loss': 0.0231, 'grad_norm': 0.5924153923988342, 'learning_rate': 0.00022500000000000002, 'rewards/chosen': -0.11716614663600922, 'rewards/rejected': -10.379233360290527, 'rewards/accuracies': 1.0, 'rewards/margins': 10.262067794799805, 'logps/rejected': -259.9256286621094, 'logps/chosen': -104.78213500976562, 'logits/rejected': -2.290248155593872, 'logits/chosen': -2.126033306121826, 'epoch': 1.92}\n",
      "{'loss': 0.0478, 'grad_norm': 0.7303395867347717, 'learning_rate': 0.00023, 'rewards/chosen': -0.009548276662826538, 'rewards/rejected': -4.2280168533325195, 'rewards/accuracies': 1.0, 'rewards/margins': 4.21846866607666, 'logps/rejected': -105.60352325439453, 'logps/chosen': -71.94483947753906, 'logits/rejected': -2.178323745727539, 'logits/chosen': -2.139167070388794, 'epoch': 1.96}\n",
      "{'loss': 0.0582, 'grad_norm': 0.6193222403526306, 'learning_rate': 0.000235, 'rewards/chosen': 0.83241206407547, 'rewards/rejected': -9.751709938049316, 'rewards/accuracies': 1.0, 'rewards/margins': 10.584121704101562, 'logps/rejected': -247.07846069335938, 'logps/chosen': -65.16926574707031, 'logits/rejected': -1.9943366050720215, 'logits/chosen': -1.4614261388778687, 'epoch': 2.0}\n",
      "{'loss': 0.0542, 'grad_norm': 0.5715683102607727, 'learning_rate': 0.00024, 'rewards/chosen': 0.2446342557668686, 'rewards/rejected': -5.289243698120117, 'rewards/accuracies': 1.0, 'rewards/margins': 5.533877849578857, 'logps/rejected': -119.58151245117188, 'logps/chosen': -95.02523040771484, 'logits/rejected': -1.832923173904419, 'logits/chosen': -1.8788667917251587, 'epoch': 2.04}\n",
      "{'loss': 0.0001, 'grad_norm': 0.005578548181802034, 'learning_rate': 0.000245, 'rewards/chosen': -0.9230411648750305, 'rewards/rejected': -12.689336776733398, 'rewards/accuracies': 1.0, 'rewards/margins': 11.76629638671875, 'logps/rejected': -279.91131591796875, 'logps/chosen': -133.6237335205078, 'logits/rejected': -1.8420733213424683, 'logits/chosen': -1.552488923072815, 'epoch': 2.08}\n",
      "{'loss': 0.0148, 'grad_norm': 0.21188785135746002, 'learning_rate': 0.00025, 'rewards/chosen': 0.4197511076927185, 'rewards/rejected': -14.742295265197754, 'rewards/accuracies': 1.0, 'rewards/margins': 15.162046432495117, 'logps/rejected': -301.9954528808594, 'logps/chosen': -49.3067512512207, 'logits/rejected': -2.181433916091919, 'logits/chosen': -1.5246021747589111, 'epoch': 2.12}\n",
      "{'loss': 0.0008, 'grad_norm': 0.022411612793803215, 'learning_rate': 0.000255, 'rewards/chosen': -1.5091805458068848, 'rewards/rejected': -16.188982009887695, 'rewards/accuracies': 1.0, 'rewards/margins': 14.679800033569336, 'logps/rejected': -336.1528625488281, 'logps/chosen': -118.30078125, 'logits/rejected': -1.9389839172363281, 'logits/chosen': -1.8378912210464478, 'epoch': 2.16}\n",
      "{'loss': 0.0002, 'grad_norm': 0.01030399464070797, 'learning_rate': 0.00026000000000000003, 'rewards/chosen': -0.2908196747303009, 'rewards/rejected': -14.185060501098633, 'rewards/accuracies': 1.0, 'rewards/margins': 13.894241333007812, 'logps/rejected': -288.5732116699219, 'logps/chosen': -76.40377807617188, 'logits/rejected': -2.0105245113372803, 'logits/chosen': -1.8424732685089111, 'epoch': 2.2}\n",
      "{'loss': 0.0005, 'grad_norm': 0.013969479128718376, 'learning_rate': 0.00026500000000000004, 'rewards/chosen': -0.361262708902359, 'rewards/rejected': -10.811346054077148, 'rewards/accuracies': 1.0, 'rewards/margins': 10.450082778930664, 'logps/rejected': -220.3206329345703, 'logps/chosen': -81.74073028564453, 'logits/rejected': -2.050959825515747, 'logits/chosen': -1.8211381435394287, 'epoch': 2.24}\n",
      "{'loss': 0.16, 'grad_norm': 3.0300381183624268, 'learning_rate': 0.00027, 'rewards/chosen': -0.6441971063613892, 'rewards/rejected': -8.940755844116211, 'rewards/accuracies': 1.0, 'rewards/margins': 8.296558380126953, 'logps/rejected': -177.03060913085938, 'logps/chosen': -93.95562744140625, 'logits/rejected': -1.8412545919418335, 'logits/chosen': -1.4710798263549805, 'epoch': 2.29}\n",
      "{'loss': 0.0007, 'grad_norm': 0.01911858655512333, 'learning_rate': 0.000275, 'rewards/chosen': -0.11879020929336548, 'rewards/rejected': -12.70512580871582, 'rewards/accuracies': 1.0, 'rewards/margins': 12.586335182189941, 'logps/rejected': -287.0143737792969, 'logps/chosen': -76.07672119140625, 'logits/rejected': -1.6346395015716553, 'logits/chosen': -1.2517472505569458, 'epoch': 2.33}\n",
      "{'loss': 0.0004, 'grad_norm': 0.009470637887716293, 'learning_rate': 0.00028000000000000003, 'rewards/chosen': 0.6661921739578247, 'rewards/rejected': -11.92507553100586, 'rewards/accuracies': 1.0, 'rewards/margins': 12.591268539428711, 'logps/rejected': -242.5028839111328, 'logps/chosen': -51.98975372314453, 'logits/rejected': -2.0776500701904297, 'logits/chosen': -1.471874475479126, 'epoch': 2.37}\n",
      "{'loss': 0.0011, 'grad_norm': 0.040419623255729675, 'learning_rate': 0.000285, 'rewards/chosen': -0.3234032690525055, 'rewards/rejected': -10.625783920288086, 'rewards/accuracies': 1.0, 'rewards/margins': 10.302379608154297, 'logps/rejected': -243.9813995361328, 'logps/chosen': -95.54408264160156, 'logits/rejected': -1.9922306537628174, 'logits/chosen': -1.73134183883667, 'epoch': 2.41}\n",
      "{'loss': 0.0003, 'grad_norm': 0.0181443989276886, 'learning_rate': 0.00029, 'rewards/chosen': -0.8710544109344482, 'rewards/rejected': -13.663053512573242, 'rewards/accuracies': 1.0, 'rewards/margins': 12.791999816894531, 'logps/rejected': -301.0035705566406, 'logps/chosen': -75.93173217773438, 'logits/rejected': -1.5337893962860107, 'logits/chosen': -1.2504551410675049, 'epoch': 2.45}\n",
      "{'eval_loss': 0.00011466311116237193, 'eval_runtime': 0.0578, 'eval_samples_per_second': 17.312, 'eval_steps_per_second': 17.312, 'eval_rewards/chosen': 0.7175735831260681, 'eval_rewards/rejected': -8.35588264465332, 'eval_rewards/accuracies': 1.0, 'eval_rewards/margins': 9.073455810546875, 'eval_logps/rejected': -119.35035705566406, 'eval_logps/chosen': -6.689189910888672, 'eval_logits/rejected': -2.080418586730957, 'eval_logits/chosen': -1.5572291612625122, 'epoch': 2.45}\n",
      "{'loss': 0.0002, 'grad_norm': 0.007433739956468344, 'learning_rate': 0.000295, 'rewards/chosen': -1.381912350654602, 'rewards/rejected': -16.300289154052734, 'rewards/accuracies': 1.0, 'rewards/margins': 14.918376922607422, 'logps/rejected': -293.68939208984375, 'logps/chosen': -82.7559814453125, 'logits/rejected': -1.998956561088562, 'logits/chosen': -1.5460364818572998, 'epoch': 2.49}\n",
      "{'loss': 0.0104, 'grad_norm': 0.5720478296279907, 'learning_rate': 0.0003, 'rewards/chosen': 0.19228476285934448, 'rewards/rejected': -5.578102111816406, 'rewards/accuracies': 1.0, 'rewards/margins': 5.770386695861816, 'logps/rejected': -119.15948486328125, 'logps/chosen': -46.198402404785156, 'logits/rejected': -1.842430830001831, 'logits/chosen': -1.5092841386795044, 'epoch': 2.53}\n",
      "{'loss': 0.0049, 'grad_norm': 0.10129647701978683, 'learning_rate': 0.000305, 'rewards/chosen': 0.7363174557685852, 'rewards/rejected': -9.941573143005371, 'rewards/accuracies': 1.0, 'rewards/margins': 10.677889823913574, 'logps/rejected': -209.0299072265625, 'logps/chosen': -40.0620002746582, 'logits/rejected': -1.6597241163253784, 'logits/chosen': -1.3757622241973877, 'epoch': 2.57}\n",
      "{'loss': 0.0008, 'grad_norm': 0.021417677402496338, 'learning_rate': 0.00031, 'rewards/chosen': -2.489004611968994, 'rewards/rejected': -23.715595245361328, 'rewards/accuracies': 1.0, 'rewards/margins': 21.226593017578125, 'logps/rejected': -396.1246032714844, 'logps/chosen': -104.65486145019531, 'logits/rejected': -1.6770930290222168, 'logits/chosen': -1.4671111106872559, 'epoch': 2.61}\n",
      "{'loss': 0.0671, 'grad_norm': 2.2390329837799072, 'learning_rate': 0.000315, 'rewards/chosen': -0.9151992201805115, 'rewards/rejected': -14.799896240234375, 'rewards/accuracies': 1.0, 'rewards/margins': 13.884696960449219, 'logps/rejected': -281.5467529296875, 'logps/chosen': -82.20960998535156, 'logits/rejected': -1.6859469413757324, 'logits/chosen': -1.3864874839782715, 'epoch': 2.65}\n",
      "{'loss': 0.0008, 'grad_norm': 0.029340693727135658, 'learning_rate': 0.00032, 'rewards/chosen': 0.5112138390541077, 'rewards/rejected': -11.660442352294922, 'rewards/accuracies': 1.0, 'rewards/margins': 12.171655654907227, 'logps/rejected': -215.7581329345703, 'logps/chosen': -42.60182571411133, 'logits/rejected': -1.8275840282440186, 'logits/chosen': -1.361832618713379, 'epoch': 2.69}\n",
      "{'loss': 0.0005, 'grad_norm': 0.014079254120588303, 'learning_rate': 0.00032500000000000004, 'rewards/chosen': -0.17676806449890137, 'rewards/rejected': -14.621349334716797, 'rewards/accuracies': 1.0, 'rewards/margins': 14.444580078125, 'logps/rejected': -300.52459716796875, 'logps/chosen': -82.84599304199219, 'logits/rejected': -1.8410401344299316, 'logits/chosen': -1.5642467737197876, 'epoch': 2.73}\n",
      "{'loss': 0.0003, 'grad_norm': 0.008524476550519466, 'learning_rate': 0.00033, 'rewards/chosen': -1.0388343334197998, 'rewards/rejected': -22.97974395751953, 'rewards/accuracies': 1.0, 'rewards/margins': 21.94091033935547, 'logps/rejected': -415.7778015136719, 'logps/chosen': -79.59425354003906, 'logits/rejected': -1.918565034866333, 'logits/chosen': -1.2516027688980103, 'epoch': 2.78}\n",
      "{'loss': 0.0001, 'grad_norm': 0.003747151466086507, 'learning_rate': 0.000335, 'rewards/chosen': 0.8407633304595947, 'rewards/rejected': -13.98399829864502, 'rewards/accuracies': 1.0, 'rewards/margins': 14.824761390686035, 'logps/rejected': -260.0303955078125, 'logps/chosen': -46.204505920410156, 'logits/rejected': -2.0597643852233887, 'logits/chosen': -1.2338306903839111, 'epoch': 2.82}\n",
      "{'loss': 0.0001, 'grad_norm': 0.004216546658426523, 'learning_rate': 0.00034, 'rewards/chosen': -0.24651747941970825, 'rewards/rejected': -13.013314247131348, 'rewards/accuracies': 1.0, 'rewards/margins': 12.766796112060547, 'logps/rejected': -243.97946166992188, 'logps/chosen': -81.50726318359375, 'logits/rejected': -1.9351937770843506, 'logits/chosen': -1.7421393394470215, 'epoch': 2.86}\n",
      "{'loss': 0.0001, 'grad_norm': 0.002309159142896533, 'learning_rate': 0.000345, 'rewards/chosen': 0.3642197549343109, 'rewards/rejected': -11.631160736083984, 'rewards/accuracies': 1.0, 'rewards/margins': 11.995380401611328, 'logps/rejected': -211.90708923339844, 'logps/chosen': -46.746559143066406, 'logits/rejected': -1.9879920482635498, 'logits/chosen': -1.282468318939209, 'epoch': 2.9}\n",
      "{'loss': 0.0005, 'grad_norm': 0.018283674493432045, 'learning_rate': 0.00035, 'rewards/chosen': 0.22454480826854706, 'rewards/rejected': -11.968927383422852, 'rewards/accuracies': 1.0, 'rewards/margins': 12.193471908569336, 'logps/rejected': -264.5810546875, 'logps/chosen': -47.74428939819336, 'logits/rejected': -1.7010509967803955, 'logits/chosen': -1.0654327869415283, 'epoch': 2.94}\n",
      "{'loss': 0.0001, 'grad_norm': 0.00282063870690763, 'learning_rate': 0.000355, 'rewards/chosen': 0.4950469732284546, 'rewards/rejected': -18.904396057128906, 'rewards/accuracies': 1.0, 'rewards/margins': 19.399442672729492, 'logps/rejected': -344.68450927734375, 'logps/chosen': -23.873119354248047, 'logits/rejected': -1.9890985488891602, 'logits/chosen': -1.261185646057129, 'epoch': 2.98}\n",
      "{'loss': 0.004, 'grad_norm': 0.10079441219568253, 'learning_rate': 0.00035999999999999997, 'rewards/chosen': -0.510877251625061, 'rewards/rejected': -14.357948303222656, 'rewards/accuracies': 1.0, 'rewards/margins': 13.847070693969727, 'logps/rejected': -254.33851623535156, 'logps/chosen': -90.75080871582031, 'logits/rejected': -1.7105947732925415, 'logits/chosen': -1.2686972618103027, 'epoch': 3.02}\n",
      "{'loss': 0.0007, 'grad_norm': 0.0154630858451128, 'learning_rate': 0.000365, 'rewards/chosen': -1.8068907260894775, 'rewards/rejected': -10.024937629699707, 'rewards/accuracies': 1.0, 'rewards/margins': 8.218047142028809, 'logps/rejected': -210.2765655517578, 'logps/chosen': -78.86231994628906, 'logits/rejected': -1.9114385843276978, 'logits/chosen': -1.6241751909255981, 'epoch': 3.06}\n",
      "{'loss': 0.0008, 'grad_norm': 0.02679143287241459, 'learning_rate': 0.00037, 'rewards/chosen': -0.08455261588096619, 'rewards/rejected': -12.340885162353516, 'rewards/accuracies': 1.0, 'rewards/margins': 12.256332397460938, 'logps/rejected': -219.62319946289062, 'logps/chosen': -49.30609893798828, 'logits/rejected': -1.6284652948379517, 'logits/chosen': -0.9855273365974426, 'epoch': 3.1}\n",
      "{'loss': 0.0, 'grad_norm': 3.265540726715699e-05, 'learning_rate': 0.000375, 'rewards/chosen': -1.4373136758804321, 'rewards/rejected': -20.02682113647461, 'rewards/accuracies': 1.0, 'rewards/margins': 18.589508056640625, 'logps/rejected': -399.0063171386719, 'logps/chosen': -124.10104370117188, 'logits/rejected': -1.8616865873336792, 'logits/chosen': -1.6032828092575073, 'epoch': 3.14}\n",
      "{'loss': 0.0001, 'grad_norm': 0.003969978541135788, 'learning_rate': 0.00038, 'rewards/chosen': 0.07855424284934998, 'rewards/rejected': -12.341854095458984, 'rewards/accuracies': 1.0, 'rewards/margins': 12.420408248901367, 'logps/rejected': -216.14132690429688, 'logps/chosen': -63.58735656738281, 'logits/rejected': -1.8076963424682617, 'logits/chosen': -1.3204450607299805, 'epoch': 3.18}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0032941303215920925, 'learning_rate': 0.00038500000000000003, 'rewards/chosen': 1.2128846645355225, 'rewards/rejected': -11.185372352600098, 'rewards/accuracies': 1.0, 'rewards/margins': 12.398258209228516, 'logps/rejected': -203.8529052734375, 'logps/chosen': -31.57832908630371, 'logits/rejected': -2.0517702102661133, 'logits/chosen': -1.4699788093566895, 'epoch': 3.22}\n",
      "{'loss': 0.0003, 'grad_norm': 0.009211604483425617, 'learning_rate': 0.00039000000000000005, 'rewards/chosen': -1.0132114887237549, 'rewards/rejected': -19.101728439331055, 'rewards/accuracies': 1.0, 'rewards/margins': 18.088516235351562, 'logps/rejected': -327.6563720703125, 'logps/chosen': -75.0986557006836, 'logits/rejected': -1.7601983547210693, 'logits/chosen': -1.2498878240585327, 'epoch': 3.27}\n",
      "{'eval_loss': 5.668514404533198e-06, 'eval_runtime': 0.0567, 'eval_samples_per_second': 17.626, 'eval_steps_per_second': 17.626, 'eval_rewards/chosen': 0.675722599029541, 'eval_rewards/rejected': -11.404858589172363, 'eval_rewards/accuracies': 1.0, 'eval_rewards/margins': 12.080581665039062, 'eval_logps/rejected': -149.84011840820312, 'eval_logps/chosen': -7.107699394226074, 'eval_logits/rejected': -2.0469508171081543, 'eval_logits/chosen': -1.5586581230163574, 'epoch': 3.27}\n",
      "{'loss': 0.0006, 'grad_norm': 0.014735225588083267, 'learning_rate': 0.000395, 'rewards/chosen': 0.9332859516143799, 'rewards/rejected': -9.10043716430664, 'rewards/accuracies': 1.0, 'rewards/margins': 10.033722877502441, 'logps/rejected': -157.2213592529297, 'logps/chosen': -51.832122802734375, 'logits/rejected': -1.9464088678359985, 'logits/chosen': -1.7965242862701416, 'epoch': 3.31}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0022481379564851522, 'learning_rate': 0.0004, 'rewards/chosen': -1.1097058057785034, 'rewards/rejected': -17.62552833557129, 'rewards/accuracies': 1.0, 'rewards/margins': 16.515823364257812, 'logps/rejected': -305.60589599609375, 'logps/chosen': -78.04161834716797, 'logits/rejected': -1.7065646648406982, 'logits/chosen': -1.4122499227523804, 'epoch': 3.35}\n",
      "{'loss': 0.0, 'grad_norm': 0.0006017583655193448, 'learning_rate': 0.00040500000000000003, 'rewards/chosen': -1.1429290771484375, 'rewards/rejected': -15.260204315185547, 'rewards/accuracies': 1.0, 'rewards/margins': 14.11727523803711, 'logps/rejected': -283.1822204589844, 'logps/chosen': -100.59783172607422, 'logits/rejected': -1.4653770923614502, 'logits/chosen': -1.6355856657028198, 'epoch': 3.39}\n",
      "{'loss': 0.002, 'grad_norm': 0.07302191108465195, 'learning_rate': 0.00041, 'rewards/chosen': -1.5476568937301636, 'rewards/rejected': -13.821754455566406, 'rewards/accuracies': 1.0, 'rewards/margins': 12.274097442626953, 'logps/rejected': -275.89569091796875, 'logps/chosen': -71.57839965820312, 'logits/rejected': -1.8524972200393677, 'logits/chosen': -1.3396496772766113, 'epoch': 3.43}\n",
      "{'loss': 0.0, 'grad_norm': 0.00020823236263822764, 'learning_rate': 0.000415, 'rewards/chosen': -0.17142479121685028, 'rewards/rejected': -19.286422729492188, 'rewards/accuracies': 1.0, 'rewards/margins': 19.11499786376953, 'logps/rejected': -361.76715087890625, 'logps/chosen': -81.63777160644531, 'logits/rejected': -1.7151827812194824, 'logits/chosen': -1.2669951915740967, 'epoch': 3.47}\n",
      "{'loss': 0.0, 'grad_norm': 4.3708908492590126e-07, 'learning_rate': 0.00042, 'rewards/chosen': -2.33029842376709, 'rewards/rejected': -31.52948760986328, 'rewards/accuracies': 1.0, 'rewards/margins': 29.199188232421875, 'logps/rejected': -543.19580078125, 'logps/chosen': -128.2257537841797, 'logits/rejected': -1.8639640808105469, 'logits/chosen': -1.5086071491241455, 'epoch': 3.51}\n",
      "{'loss': 0.0011, 'grad_norm': 0.03391806408762932, 'learning_rate': 0.000425, 'rewards/chosen': 0.10505254566669464, 'rewards/rejected': -20.739351272583008, 'rewards/accuracies': 1.0, 'rewards/margins': 20.844406127929688, 'logps/rejected': -347.2615661621094, 'logps/chosen': -37.32024383544922, 'logits/rejected': -1.7911795377731323, 'logits/chosen': -1.0518898963928223, 'epoch': 3.55}\n",
      "{'loss': 0.0102, 'grad_norm': 0.3471135199069977, 'learning_rate': 0.00043, 'rewards/chosen': -0.13321971893310547, 'rewards/rejected': -12.598306655883789, 'rewards/accuracies': 1.0, 'rewards/margins': 12.465087890625, 'logps/rejected': -253.6400146484375, 'logps/chosen': -132.98330688476562, 'logits/rejected': -1.9183518886566162, 'logits/chosen': -1.8366565704345703, 'epoch': 3.59}\n",
      "{'loss': 0.0007, 'grad_norm': 0.023137396201491356, 'learning_rate': 0.000435, 'rewards/chosen': 0.519905686378479, 'rewards/rejected': -9.24520206451416, 'rewards/accuracies': 1.0, 'rewards/margins': 9.765108108520508, 'logps/rejected': -169.40914916992188, 'logps/chosen': -40.84653854370117, 'logits/rejected': -1.8082886934280396, 'logits/chosen': -1.3687942028045654, 'epoch': 3.63}\n",
      "{'loss': 0.0001, 'grad_norm': 0.0018049394711852074, 'learning_rate': 0.00044, 'rewards/chosen': -1.4887803792953491, 'rewards/rejected': -25.553281784057617, 'rewards/accuracies': 1.0, 'rewards/margins': 24.06450080871582, 'logps/rejected': -456.71197509765625, 'logps/chosen': -82.89038848876953, 'logits/rejected': -1.733279824256897, 'logits/chosen': -1.1610947847366333, 'epoch': 3.67}\n",
      "{'loss': 0.0025, 'grad_norm': 0.05811260640621185, 'learning_rate': 0.00044500000000000003, 'rewards/chosen': -1.4106523990631104, 'rewards/rejected': -13.982779502868652, 'rewards/accuracies': 1.0, 'rewards/margins': 12.572126388549805, 'logps/rejected': -269.1950988769531, 'logps/chosen': -103.57567596435547, 'logits/rejected': -1.9071800708770752, 'logits/chosen': -1.537562608718872, 'epoch': 3.71}\n",
      "{'loss': 0.0, 'grad_norm': 0.0009057183633558452, 'learning_rate': 0.00045000000000000004, 'rewards/chosen': 0.23170480132102966, 'rewards/rejected': -17.148895263671875, 'rewards/accuracies': 1.0, 'rewards/margins': 17.38060188293457, 'logps/rejected': -296.667724609375, 'logps/chosen': -46.32843780517578, 'logits/rejected': -1.8153585195541382, 'logits/chosen': -1.3219510316848755, 'epoch': 3.76}\n",
      "{'loss': 0.0011, 'grad_norm': 0.021481391042470932, 'learning_rate': 0.000455, 'rewards/chosen': 0.739471435546875, 'rewards/rejected': -9.569759368896484, 'rewards/accuracies': 1.0, 'rewards/margins': 10.309229850769043, 'logps/rejected': -173.63536071777344, 'logps/chosen': -56.91704559326172, 'logits/rejected': -1.7103594541549683, 'logits/chosen': -1.517418622970581, 'epoch': 3.8}\n",
      "{'loss': 0.0012, 'grad_norm': 0.04796776548027992, 'learning_rate': 0.00046, 'rewards/chosen': -0.9347207546234131, 'rewards/rejected': -13.80357837677002, 'rewards/accuracies': 1.0, 'rewards/margins': 12.868858337402344, 'logps/rejected': -267.12884521484375, 'logps/chosen': -104.94788360595703, 'logits/rejected': -1.8306045532226562, 'logits/chosen': -1.7025485038757324, 'epoch': 3.84}\n",
      "{'loss': 0.0, 'grad_norm': 0.00010020712215919048, 'learning_rate': 0.000465, 'rewards/chosen': -0.15113943815231323, 'rewards/rejected': -16.860748291015625, 'rewards/accuracies': 1.0, 'rewards/margins': 16.709609985351562, 'logps/rejected': -266.9242858886719, 'logps/chosen': -46.819522857666016, 'logits/rejected': -1.9278961420059204, 'logits/chosen': -1.7164843082427979, 'epoch': 3.88}\n",
      "{'loss': 0.0002, 'grad_norm': 0.005081143695861101, 'learning_rate': 0.00047, 'rewards/chosen': -0.38763922452926636, 'rewards/rejected': -16.529281616210938, 'rewards/accuracies': 1.0, 'rewards/margins': 16.141643524169922, 'logps/rejected': -304.82281494140625, 'logps/chosen': -72.98814392089844, 'logits/rejected': -1.7197613716125488, 'logits/chosen': -1.4064114093780518, 'epoch': 3.92}\n",
      "{'loss': 0.0, 'grad_norm': 1.1615660696406849e-05, 'learning_rate': 0.000475, 'rewards/chosen': -2.179586887359619, 'rewards/rejected': -24.413440704345703, 'rewards/accuracies': 1.0, 'rewards/margins': 22.233856201171875, 'logps/rejected': -421.5474548339844, 'logps/chosen': -118.04263305664062, 'logits/rejected': -1.5138132572174072, 'logits/chosen': -1.3051077127456665, 'epoch': 3.96}\n",
      "{'loss': 0.0012, 'grad_norm': 0.024667419493198395, 'learning_rate': 0.00048, 'rewards/chosen': 0.02672630548477173, 'rewards/rejected': -9.864419937133789, 'rewards/accuracies': 1.0, 'rewards/margins': 9.891146659851074, 'logps/rejected': -171.96778869628906, 'logps/chosen': -44.305747985839844, 'logits/rejected': -1.4190701246261597, 'logits/chosen': -0.9738848805427551, 'epoch': 4.0}\n",
      "{'loss': 0.0006, 'grad_norm': 0.018520314246416092, 'learning_rate': 0.00048499999999999997, 'rewards/chosen': -0.5740712881088257, 'rewards/rejected': -18.499237060546875, 'rewards/accuracies': 1.0, 'rewards/margins': 17.925167083740234, 'logps/rejected': -306.10125732421875, 'logps/chosen': -71.61772918701172, 'logits/rejected': -1.7278110980987549, 'logits/chosen': -1.5137567520141602, 'epoch': 4.04}\n",
      "{'loss': 0.0, 'grad_norm': 1.1561952305783052e-05, 'learning_rate': 0.00049, 'rewards/chosen': 0.02422936260700226, 'rewards/rejected': -26.187477111816406, 'rewards/accuracies': 1.0, 'rewards/margins': 26.211708068847656, 'logps/rejected': -451.50506591796875, 'logps/chosen': -65.75938415527344, 'logits/rejected': -1.6287683248519897, 'logits/chosen': -0.9272162318229675, 'epoch': 4.08}\n",
      "{'eval_loss': 2.5671938601590227e-06, 'eval_runtime': 0.1298, 'eval_samples_per_second': 7.703, 'eval_steps_per_second': 7.703, 'eval_rewards/chosen': 0.6950359344482422, 'eval_rewards/rejected': -12.17765998840332, 'eval_rewards/accuracies': 1.0, 'eval_rewards/margins': 12.872695922851562, 'eval_logps/rejected': -157.56813049316406, 'eval_logps/chosen': -6.9145660400390625, 'eval_logits/rejected': -1.8979873657226562, 'eval_logits/chosen': -1.4076347351074219, 'epoch': 4.08}\n",
      "{'train_runtime': 69.6288, 'train_samples_per_second': 5.745, 'train_steps_per_second': 1.436, 'train_loss': 0.19257629114462763, 'epoch': 4.08}\n",
      "***** train metrics *****\n",
      "  epoch                    =       4.08\n",
      "  train_loss               =     0.1926\n",
      "  train_runtime            = 0:01:09.62\n",
      "  train_samples            =       1000\n",
      "  train_samples_per_second =      5.745\n",
      "  train_steps_per_second   =      1.436\n",
      "***** eval metrics *****\n",
      "  epoch                   =       4.08\n",
      "  eval_logits/chosen      =    -1.4076\n",
      "  eval_logits/rejected    =     -1.898\n",
      "  eval_logps/chosen       =    -6.9146\n",
      "  eval_logps/rejected     =  -157.5681\n",
      "  eval_loss               =        0.0\n",
      "  eval_rewards/accuracies =        1.0\n",
      "  eval_rewards/chosen     =      0.695\n",
      "  eval_rewards/margins    =    12.8727\n",
      "  eval_rewards/rejected   =   -12.1777\n",
      "  eval_runtime            = 0:00:00.13\n",
      "  eval_samples            =          5\n",
      "  eval_samples_per_second =      7.269\n",
      "  eval_steps_per_second   =      7.269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\py39\\lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "2024-08-21 20:46:42.044115: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-21 20:46:42.752218: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[32m2024-08-21 20:46:44.043\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m217\u001b[0m - \u001b[1mParse args: ScriptArguments(model_type='auto', model_name_or_path='Qwen1.5-1.8B-Chat', tokenizer_name_or_path=None, load_in_8bit=False, load_in_4bit=False, cache_dir='./cache', use_fast_tokenizer=False, torch_dtype='float16', device_map='auto', trust_remote_code=True, dataset_name=None, dataset_config_name=None, train_file_dir='data\\\\reward', validation_file_dir='data\\\\reward', template_name='vicuna', per_device_train_batch_size=1, per_device_eval_batch_size=1, max_source_length=512, max_target_length=256, min_target_length=4, max_train_samples=1000, max_eval_samples=5, overwrite_cache=False, validation_split_percentage=1, preprocessing_num_workers=4, use_peft=True, qlora=False, target_modules='q_proj,k_proj', lora_rank=8, lora_dropout=0.05, lora_alpha=16.0, peft_path=None, do_train=True, do_eval=True, beta=0.1, learning_rate=0.0005, lr_scheduler_type='cosine', warmup_steps=100, weight_decay=0.05, optim='adamw_hf', fp16=True, bf16=False, gradient_checkpointing=True, gradient_accumulation_steps=4, save_steps=50, eval_steps=20, logging_steps=1, output_dir='outputs-dpo-qwen1.5', max_steps=100, eval_strategy='steps', remove_unused_columns=False, report_to='tensorboard')\u001b[0m\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\u001b[32m2024-08-21 20:46:44.169\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m240\u001b[0m - \u001b[1mAdd bos_token: <|im_end|>, bos_token_id: 151645\u001b[0m\n",
      "\u001b[32m2024-08-21 20:46:44.169\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m247\u001b[0m - \u001b[34m\u001b[1mTokenizer: Qwen2Tokenizer(name_or_path='Qwen1.5-1.8B-Chat', vocab_size=151643, model_max_length=32768, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|im_end|>', 'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\u001b[0m\n",
      "\u001b[32m2024-08-21 20:46:44.169\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m275\u001b[0m - \u001b[1mtrain files: data\\reward\\orca_rlhf_mod.jsonl\u001b[0m\n",
      "\u001b[32m2024-08-21 20:46:44.169\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m280\u001b[0m - \u001b[1meval files: data\\reward\\orca_rlhf_mod.jsonl\u001b[0m\n",
      "\u001b[32m2024-08-21 20:46:44.560\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m301\u001b[0m - \u001b[1mRaw datasets: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['system', 'history', 'question', 'response_chosen', 'response_rejected'],\n",
      "        num_rows: 12859\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['system', 'history', 'question', 'response_chosen', 'response_rejected'],\n",
      "        num_rows: 12859\n",
      "    })\n",
      "})\u001b[0m\n",
      "\u001b[32m2024-08-21 20:46:44.564\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m343\u001b[0m - \u001b[34m\u001b[1mExample train_dataset[0]: {'system': '', 'history': [], 'question': \"You will be given a definition of a task first, then some input of the task.\\nThis task is about using the specified sentence and converting the sentence to Resource Description Framework (RDF) triplets of the form (subject, predicate object). The RDF triplets generated must be such that the triplets accurately capture the structure and semantics of the input sentence. The input is a sentence and the output is a list of triplets of the form [subject, predicate, object] that capture the relationships present in the sentence. When a sentence has more than 1 RDF triplet possible, the output must contain all of them.\\n\\nAFC Ajax (amateurs)'s ground is Sportpark De Toekomst where Ajax Youth Academy also play.\\nOutput:\", 'response_chosen': '[\\n  [\"AFC Ajax (amateurs)\", \"has ground\", \"Sportpark De Toekomst\"],\\n  [\"Ajax Youth Academy\", \"plays at\", \"Sportpark De Toekomst\"]\\n]', 'response_rejected': \" Sure, I'd be happy to help! Here are the RDF triplets for the input sentence:\\n\\n[AFC Ajax (amateurs), hasGround, Sportpark De Toekomst]\\n[Ajax Youth Academy, playsAt, Sportpark De Toekomst]\\n\\nExplanation:\\n\\n* AFC Ajax (amateurs) is the subject of the first triplet, and hasGround is the predicate that describes the relationship between AFC Ajax (amateurs) and Sportpark De Toekomst.\\n* Ajax Youth Academy is the subject of the second triplet, and playsAt is the predicate that describes the relationship between Ajax Youth Academy and Sportpark De Toekomst.\\n\\nNote that there may be other possible RDF triplets that could be derived from the input sentence, but the above triplets capture the main relationships present in the sentence.\"}\u001b[0m\n",
      "\n",
      "Running tokenizer on dataset (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]d:\\anaconda\\envs\\py39\\lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\py39\\lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\py39\\lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\py39\\lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\py39\\lib\\site-packages\\transformers\\deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "2024-08-21 20:46:47.909066: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-21 20:46:47.909474: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-21 20:46:47.911283: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-21 20:46:47.912318: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-21 20:46:47.932307: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-21 20:46:48.350834: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-21 20:46:48.352407: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-21 20:46:48.352966: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-21 20:46:48.353261: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-21 20:46:48.360588: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\n",
      "Running tokenizer on dataset (num_proc=4):  25%|██▌       | 250/1000 [00:04<00:13, 55.47 examples/s]\n",
      "Running tokenizer on dataset (num_proc=4): 100%|██████████| 1000/1000 [00:05<00:00, 195.63 examples/s]\n",
      "d:\\anaconda\\envs\\py39\\lib\\site-packages\\datasets\\table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "\n",
      "Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]\n",
      "Filter: 100%|██████████| 1000/1000 [00:00<00:00, 51230.64 examples/s]\n",
      "\u001b[32m2024-08-21 20:46:49.807\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m356\u001b[0m - \u001b[34m\u001b[1mNum train_samples: 98\u001b[0m\n",
      "\u001b[32m2024-08-21 20:46:49.807\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m357\u001b[0m - \u001b[34m\u001b[1mFirst train example:\u001b[0m\n",
      "\u001b[32m2024-08-21 20:46:49.807\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m359\u001b[0m - \u001b[34m\u001b[1mprompt:\n",
      "You are a helpful assistant, who always provide explanation. Think like you are answering to a five year old.</s>USER: in the slain in the spirit counterfeit, they fall backwards, either in response to the wave of the speaker's arm or as a result of a church leader's touch (or push in some cases).\n",
      "Can you repeat this sentence, but capitalize it correctly? ASSISTANT:\u001b[0m\n",
      "\u001b[32m2024-08-21 20:46:49.807\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m360\u001b[0m - \u001b[34m\u001b[1mchosen:\n",
      "In the Slain in the Spirit counterfeit, they fall backwards, either in response to the wave of the speaker's arm or as a result of a church leader's touch (or push in some cases).\u001b[0m\n",
      "\u001b[32m2024-08-21 20:46:49.807\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m361\u001b[0m - \u001b[34m\u001b[1mrejected:\n",
      " OH MY GOSH, YEAH! *excited voice* So, like, in some churches, people might fall backwards when the speaker waves their arm or when a church leader touches them! *giggles* It's like, they get all swoony and fall down, you know? *bouncy voice* And it's called \"Slain in the Spirit\"! *twinkle eyes* Isn't that so cool? *bouncy voice*\u001b[0m\n",
      "\u001b[32m2024-08-21 20:46:49.808\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m373\u001b[0m - \u001b[34m\u001b[1mExample eval_dataset[0]: {'system': '', 'history': [], 'question': \"You will be given a definition of a task first, then some input of the task.\\nThis task is about using the specified sentence and converting the sentence to Resource Description Framework (RDF) triplets of the form (subject, predicate object). The RDF triplets generated must be such that the triplets accurately capture the structure and semantics of the input sentence. The input is a sentence and the output is a list of triplets of the form [subject, predicate, object] that capture the relationships present in the sentence. When a sentence has more than 1 RDF triplet possible, the output must contain all of them.\\n\\nAFC Ajax (amateurs)'s ground is Sportpark De Toekomst where Ajax Youth Academy also play.\\nOutput:\", 'response_chosen': '[\\n  [\"AFC Ajax (amateurs)\", \"has ground\", \"Sportpark De Toekomst\"],\\n  [\"Ajax Youth Academy\", \"plays at\", \"Sportpark De Toekomst\"]\\n]', 'response_rejected': \" Sure, I'd be happy to help! Here are the RDF triplets for the input sentence:\\n\\n[AFC Ajax (amateurs), hasGround, Sportpark De Toekomst]\\n[Ajax Youth Academy, playsAt, Sportpark De Toekomst]\\n\\nExplanation:\\n\\n* AFC Ajax (amateurs) is the subject of the first triplet, and hasGround is the predicate that describes the relationship between AFC Ajax (amateurs) and Sportpark De Toekomst.\\n* Ajax Youth Academy is the subject of the second triplet, and playsAt is the predicate that describes the relationship between Ajax Youth Academy and Sportpark De Toekomst.\\n\\nNote that there may be other possible RDF triplets that could be derived from the input sentence, but the above triplets capture the main relationships present in the sentence.\"}\u001b[0m\n",
      "\u001b[32m2024-08-21 20:46:49.843\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m386\u001b[0m - \u001b[34m\u001b[1mNum eval_samples: 1\u001b[0m\n",
      "\u001b[32m2024-08-21 20:46:49.843\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m387\u001b[0m - \u001b[34m\u001b[1mFirst eval example:\u001b[0m\n",
      "\u001b[32m2024-08-21 20:46:49.843\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m389\u001b[0m - \u001b[34m\u001b[1mprompt:\n",
      "You are an AI assistant. You will be given a task. You must generate a detailed and long answer.</s>USER: Generate an approximately fifteen-word sentence that describes all this data: Midsummer House eatType restaurant; Midsummer House food Chinese; Midsummer House priceRange moderate; Midsummer House customer rating 3 out of 5; Midsummer House near All Bar One ASSISTANT:\u001b[0m\n",
      "\u001b[32m2024-08-21 20:46:49.843\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m390\u001b[0m - \u001b[34m\u001b[1mchosen:\n",
      "Midsummer House is a moderately priced Chinese restaurant with a 3/5 customer rating, located near All Bar One.\u001b[0m\n",
      "\u001b[32m2024-08-21 20:46:49.844\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m391\u001b[0m - \u001b[34m\u001b[1mrejected:\n",
      " Sure! Here's a sentence that describes all the data you provided:\n",
      "\n",
      "\"Midsummer House is a moderately priced Chinese restaurant with a customer rating of 3 out of 5, located near All Bar One, offering a variety of delicious dishes.\"\u001b[0m\n",
      "\u001b[32m2024-08-21 20:46:49.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m403\u001b[0m - \u001b[1mDevice map: auto\u001b[0m\n",
      "\u001b[32m2024-08-21 20:46:51.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m465\u001b[0m - \u001b[1mFine-tuning method: LoRA(PEFT)\u001b[0m\n",
      "\u001b[32m2024-08-21 20:46:51.709\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m469\u001b[0m - \u001b[1mPeft target_modules: ['q_proj', 'k_proj']\u001b[0m\n",
      "\n",
      "Map:   0%|          | 0/98 [00:00<?, ? examples/s]\n",
      "Map:  61%|██████    | 60/98 [00:00<00:00, 582.08 examples/s]\n",
      "Map: 100%|██████████| 98/98 [00:00<00:00, 540.98 examples/s]\n",
      "\n",
      "Map:   0%|          | 0/1 [00:00<?, ? examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 76.09 examples/s]\n",
      "\u001b[32m2024-08-21 20:46:55.076\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m496\u001b[0m - \u001b[1m*** Train ***\u001b[0m\n",
      "d:\\anaconda\\envs\\py39\\lib\\site-packages\\transformers\\optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]d:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\py39\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:698: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "d:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "\n",
      "  1%|          | 1/100 [00:01<01:43,  1.05s/it]\n",
      "                                               \n",
      "\n",
      "  1%|          | 1/100 [00:01<01:43,  1.05s/it]\n",
      "  2%|▏         | 2/100 [00:01<01:18,  1.25it/s]\n",
      "                                               \n",
      "\n",
      "  2%|▏         | 2/100 [00:01<01:18,  1.25it/s]\n",
      "  3%|▎         | 3/100 [00:02<01:07,  1.44it/s]\n",
      "                                               \n",
      "\n",
      "  3%|▎         | 3/100 [00:02<01:07,  1.44it/s]\n",
      "  4%|▍         | 4/100 [00:02<01:02,  1.53it/s]\n",
      "                                               \n",
      "\n",
      "  4%|▍         | 4/100 [00:02<01:02,  1.53it/s]\n",
      "  5%|▌         | 5/100 [00:03<01:00,  1.58it/s]\n",
      "                                               \n",
      "\n",
      "  5%|▌         | 5/100 [00:03<01:00,  1.58it/s]\n",
      "  6%|▌         | 6/100 [00:03<00:57,  1.64it/s]\n",
      "                                               \n",
      "\n",
      "  6%|▌         | 6/100 [00:04<00:57,  1.64it/s]\n",
      "  7%|▋         | 7/100 [00:04<00:56,  1.64it/s]\n",
      "                                               \n",
      "\n",
      "  7%|▋         | 7/100 [00:04<00:56,  1.64it/s]\n",
      "  8%|▊         | 8/100 [00:05<00:55,  1.66it/s]\n",
      "                                               \n",
      "\n",
      "  8%|▊         | 8/100 [00:05<00:55,  1.66it/s]\n",
      "  9%|▉         | 9/100 [00:05<00:53,  1.70it/s]\n",
      "                                               \n",
      "\n",
      "  9%|▉         | 9/100 [00:05<00:53,  1.70it/s]\n",
      " 10%|█         | 10/100 [00:06<00:52,  1.73it/s]\n",
      "                                                \n",
      "\n",
      " 10%|█         | 10/100 [00:06<00:52,  1.73it/s]\n",
      " 11%|█         | 11/100 [00:06<00:50,  1.75it/s]\n",
      "                                                \n",
      "\n",
      " 11%|█         | 11/100 [00:06<00:50,  1.75it/s]\n",
      " 12%|█▏        | 12/100 [00:07<00:50,  1.74it/s]\n",
      "                                                \n",
      "\n",
      " 12%|█▏        | 12/100 [00:07<00:50,  1.74it/s]\n",
      " 13%|█▎        | 13/100 [00:08<00:49,  1.75it/s]\n",
      "                                                \n",
      "\n",
      " 13%|█▎        | 13/100 [00:08<00:49,  1.75it/s]\n",
      " 14%|█▍        | 14/100 [00:08<00:49,  1.74it/s]\n",
      "                                                \n",
      "\n",
      " 14%|█▍        | 14/100 [00:08<00:49,  1.74it/s]\n",
      " 15%|█▌        | 15/100 [00:09<00:49,  1.70it/s]\n",
      "                                                \n",
      "\n",
      " 15%|█▌        | 15/100 [00:09<00:49,  1.70it/s]\n",
      " 16%|█▌        | 16/100 [00:09<00:49,  1.69it/s]\n",
      "                                                \n",
      "\n",
      " 16%|█▌        | 16/100 [00:09<00:49,  1.69it/s]\n",
      " 17%|█▋        | 17/100 [00:10<00:49,  1.69it/s]\n",
      "                                                \n",
      "\n",
      " 17%|█▋        | 17/100 [00:10<00:49,  1.69it/s]\n",
      " 18%|█▊        | 18/100 [00:11<00:49,  1.67it/s]\n",
      "                                                \n",
      "\n",
      " 18%|█▊        | 18/100 [00:11<00:49,  1.67it/s]\n",
      " 19%|█▉        | 19/100 [00:11<00:48,  1.68it/s]\n",
      "                                                \n",
      "\n",
      " 19%|█▉        | 19/100 [00:11<00:48,  1.68it/s]\n",
      " 20%|██        | 20/100 [00:12<00:47,  1.70it/s]\n",
      "                                                \n",
      "\n",
      " 20%|██        | 20/100 [00:12<00:47,  1.70it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                                \n",
      "\n",
      "\n",
      "                                     \n",
      "\u001b[A\n",
      " 20%|██        | 20/100 [00:12<00:47,  1.70it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 333.25it/s]\u001b[A\n",
      "\n",
      "                                              \u001b[A\n",
      " 21%|██        | 21/100 [00:12<00:47,  1.68it/s]\n",
      "                                                \n",
      "\n",
      " 21%|██        | 21/100 [00:12<00:47,  1.68it/s]\n",
      " 22%|██▏       | 22/100 [00:13<00:46,  1.68it/s]\n",
      "                                                \n",
      "\n",
      " 22%|██▏       | 22/100 [00:13<00:46,  1.68it/s]\n",
      " 23%|██▎       | 23/100 [00:13<00:44,  1.73it/s]\n",
      "                                                \n",
      "\n",
      " 23%|██▎       | 23/100 [00:13<00:44,  1.73it/s]\n",
      " 24%|██▍       | 24/100 [00:14<00:43,  1.76it/s]\n",
      "                                                \n",
      "\n",
      " 24%|██▍       | 24/100 [00:14<00:43,  1.76it/s]\n",
      " 25%|██▌       | 25/100 [00:15<00:42,  1.76it/s]\n",
      "                                                \n",
      "\n",
      " 25%|██▌       | 25/100 [00:15<00:42,  1.76it/s]\n",
      " 26%|██▌       | 26/100 [00:15<00:42,  1.76it/s]\n",
      "                                                \n",
      "\n",
      " 26%|██▌       | 26/100 [00:15<00:42,  1.76it/s]\n",
      " 27%|██▋       | 27/100 [00:16<00:41,  1.76it/s]\n",
      "                                                \n",
      "\n",
      " 27%|██▋       | 27/100 [00:16<00:41,  1.76it/s]\n",
      " 28%|██▊       | 28/100 [00:16<00:40,  1.78it/s]\n",
      "                                                \n",
      "\n",
      " 28%|██▊       | 28/100 [00:16<00:40,  1.78it/s]\n",
      " 29%|██▉       | 29/100 [00:17<00:39,  1.78it/s]\n",
      "                                                \n",
      "\n",
      " 29%|██▉       | 29/100 [00:17<00:39,  1.78it/s]\n",
      " 30%|███       | 30/100 [00:17<00:38,  1.80it/s]\n",
      "                                                \n",
      "\n",
      " 30%|███       | 30/100 [00:17<00:38,  1.80it/s]\n",
      " 31%|███       | 31/100 [00:18<00:38,  1.77it/s]\n",
      "                                                \n",
      "\n",
      " 31%|███       | 31/100 [00:18<00:38,  1.77it/s]\n",
      " 32%|███▏      | 32/100 [00:18<00:38,  1.78it/s]\n",
      "                                                \n",
      "\n",
      " 32%|███▏      | 32/100 [00:18<00:38,  1.78it/s]\n",
      " 33%|███▎      | 33/100 [00:19<00:37,  1.78it/s]\n",
      "                                                \n",
      "\n",
      " 33%|███▎      | 33/100 [00:19<00:37,  1.78it/s]\n",
      " 34%|███▍      | 34/100 [00:20<00:37,  1.76it/s]\n",
      "                                                \n",
      "\n",
      " 34%|███▍      | 34/100 [00:20<00:37,  1.76it/s]\n",
      " 35%|███▌      | 35/100 [00:20<00:38,  1.71it/s]\n",
      "                                                \n",
      "\n",
      " 35%|███▌      | 35/100 [00:20<00:38,  1.71it/s]\n",
      " 36%|███▌      | 36/100 [00:21<00:38,  1.67it/s]\n",
      "                                                \n",
      "\n",
      " 36%|███▌      | 36/100 [00:21<00:38,  1.67it/s]\n",
      " 37%|███▋      | 37/100 [00:22<00:38,  1.63it/s]\n",
      "                                                \n",
      "\n",
      " 37%|███▋      | 37/100 [00:22<00:38,  1.63it/s]\n",
      " 38%|███▊      | 38/100 [00:22<00:37,  1.66it/s]\n",
      "                                                \n",
      "\n",
      " 38%|███▊      | 38/100 [00:22<00:37,  1.66it/s]\n",
      " 39%|███▉      | 39/100 [00:23<00:37,  1.64it/s]\n",
      "                                                \n",
      "\n",
      " 39%|███▉      | 39/100 [00:23<00:37,  1.64it/s]\n",
      " 40%|████      | 40/100 [00:23<00:36,  1.64it/s]\n",
      "                                                \n",
      "\n",
      " 40%|████      | 40/100 [00:23<00:36,  1.64it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                                \n",
      "\n",
      "\n",
      "                                     \n",
      "\u001b[A\n",
      " 40%|████      | 40/100 [00:23<00:36,  1.64it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 285.46it/s]\u001b[A\n",
      "\n",
      "                                              \u001b[A\n",
      " 41%|████      | 41/100 [00:24<00:36,  1.63it/s]\n",
      "                                                \n",
      "\n",
      " 41%|████      | 41/100 [00:24<00:36,  1.63it/s]\n",
      " 42%|████▏     | 42/100 [00:25<00:34,  1.67it/s]\n",
      "                                                \n",
      "\n",
      " 42%|████▏     | 42/100 [00:25<00:34,  1.67it/s]\n",
      " 43%|████▎     | 43/100 [00:25<00:33,  1.72it/s]\n",
      "                                                \n",
      "\n",
      " 43%|████▎     | 43/100 [00:25<00:33,  1.72it/s]\n",
      " 44%|████▍     | 44/100 [00:26<00:32,  1.74it/s]\n",
      "                                                \n",
      "\n",
      " 44%|████▍     | 44/100 [00:26<00:32,  1.74it/s]\n",
      " 45%|████▌     | 45/100 [00:26<00:31,  1.76it/s]\n",
      "                                                \n",
      "\n",
      " 45%|████▌     | 45/100 [00:26<00:31,  1.76it/s]\n",
      " 46%|████▌     | 46/100 [00:27<00:30,  1.76it/s]\n",
      "                                                \n",
      "\n",
      " 46%|████▌     | 46/100 [00:27<00:30,  1.76it/s]\n",
      " 47%|████▋     | 47/100 [00:27<00:29,  1.78it/s]\n",
      "                                                \n",
      "\n",
      " 47%|████▋     | 47/100 [00:27<00:29,  1.78it/s]\n",
      " 48%|████▊     | 48/100 [00:28<00:28,  1.80it/s]\n",
      "                                                \n",
      "\n",
      " 48%|████▊     | 48/100 [00:28<00:28,  1.80it/s]\n",
      " 49%|████▉     | 49/100 [00:28<00:28,  1.77it/s]\n",
      "                                                \n",
      "\n",
      " 49%|████▉     | 49/100 [00:28<00:28,  1.77it/s]\n",
      " 50%|█████     | 50/100 [00:29<00:28,  1.78it/s]\n",
      "                                                \n",
      "\n",
      " 50%|█████     | 50/100 [00:29<00:28,  1.78it/s]d:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "d:\\anaconda\\envs\\py39\\lib\\site-packages\\torch\\utils\\checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "\n",
      " 51%|█████     | 51/100 [00:30<00:34,  1.41it/s]\n",
      "                                                \n",
      "\n",
      " 51%|█████     | 51/100 [00:30<00:34,  1.41it/s]\n",
      " 52%|█████▏    | 52/100 [00:31<00:31,  1.51it/s]\n",
      "                                                \n",
      "\n",
      " 52%|█████▏    | 52/100 [00:31<00:31,  1.51it/s]\n",
      " 53%|█████▎    | 53/100 [00:31<00:29,  1.59it/s]\n",
      "                                                \n",
      "\n",
      " 53%|█████▎    | 53/100 [00:31<00:29,  1.59it/s]\n",
      " 54%|█████▍    | 54/100 [00:32<00:27,  1.66it/s]\n",
      "                                                \n",
      "\n",
      " 54%|█████▍    | 54/100 [00:32<00:27,  1.66it/s]\n",
      " 55%|█████▌    | 55/100 [00:32<00:26,  1.68it/s]\n",
      "                                                \n",
      "\n",
      " 55%|█████▌    | 55/100 [00:32<00:26,  1.68it/s]\n",
      " 56%|█████▌    | 56/100 [00:33<00:25,  1.73it/s]\n",
      "                                                \n",
      "\n",
      " 56%|█████▌    | 56/100 [00:33<00:25,  1.73it/s]\n",
      " 57%|█████▋    | 57/100 [00:33<00:24,  1.73it/s]\n",
      "                                                \n",
      "\n",
      " 57%|█████▋    | 57/100 [00:33<00:24,  1.73it/s]\n",
      " 58%|█████▊    | 58/100 [00:34<00:24,  1.73it/s]\n",
      "                                                \n",
      "\n",
      " 58%|█████▊    | 58/100 [00:34<00:24,  1.73it/s]\n",
      " 59%|█████▉    | 59/100 [00:35<00:23,  1.73it/s]\n",
      "                                                \n",
      "\n",
      " 59%|█████▉    | 59/100 [00:35<00:23,  1.73it/s]\n",
      " 60%|██████    | 60/100 [00:35<00:23,  1.73it/s]\n",
      "                                                \n",
      "\n",
      " 60%|██████    | 60/100 [00:35<00:23,  1.73it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                                \n",
      "\n",
      "\n",
      "                                     \n",
      "\u001b[A\n",
      " 60%|██████    | 60/100 [00:35<00:23,  1.73it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 501.29it/s]\u001b[A\n",
      "\n",
      "                                              \u001b[A\n",
      " 61%|██████    | 61/100 [00:36<00:22,  1.70it/s]\n",
      "                                                \n",
      "\n",
      " 61%|██████    | 61/100 [00:36<00:22,  1.70it/s]\n",
      " 62%|██████▏   | 62/100 [00:36<00:21,  1.74it/s]\n",
      "                                                \n",
      "\n",
      " 62%|██████▏   | 62/100 [00:36<00:21,  1.74it/s]\n",
      " 63%|██████▎   | 63/100 [00:37<00:21,  1.75it/s]\n",
      "                                                \n",
      "\n",
      " 63%|██████▎   | 63/100 [00:37<00:21,  1.75it/s]\n",
      " 64%|██████▍   | 64/100 [00:37<00:20,  1.80it/s]\n",
      "                                                \n",
      "\n",
      " 64%|██████▍   | 64/100 [00:37<00:20,  1.80it/s]\n",
      " 65%|██████▌   | 65/100 [00:38<00:19,  1.82it/s]\n",
      "                                                \n",
      "\n",
      " 65%|██████▌   | 65/100 [00:38<00:19,  1.82it/s]\n",
      " 66%|██████▌   | 66/100 [00:38<00:18,  1.82it/s]\n",
      "                                                \n",
      "\n",
      " 66%|██████▌   | 66/100 [00:38<00:18,  1.82it/s]\n",
      " 67%|██████▋   | 67/100 [00:39<00:18,  1.81it/s]\n",
      "                                                \n",
      "\n",
      " 67%|██████▋   | 67/100 [00:39<00:18,  1.81it/s]\n",
      " 68%|██████▊   | 68/100 [00:40<00:17,  1.80it/s]\n",
      "                                                \n",
      "\n",
      " 68%|██████▊   | 68/100 [00:40<00:17,  1.80it/s]\n",
      " 69%|██████▉   | 69/100 [00:40<00:17,  1.82it/s]\n",
      "                                                \n",
      "\n",
      " 69%|██████▉   | 69/100 [00:40<00:17,  1.82it/s]\n",
      " 70%|███████   | 70/100 [00:41<00:16,  1.83it/s]\n",
      "                                                \n",
      "\n",
      " 70%|███████   | 70/100 [00:41<00:16,  1.83it/s]\n",
      " 71%|███████   | 71/100 [00:41<00:16,  1.77it/s]\n",
      "                                                \n",
      "\n",
      " 71%|███████   | 71/100 [00:41<00:16,  1.77it/s]\n",
      " 72%|███████▏  | 72/100 [00:42<00:16,  1.69it/s]\n",
      "                                                \n",
      "\n",
      " 72%|███████▏  | 72/100 [00:42<00:16,  1.69it/s]\n",
      " 73%|███████▎  | 73/100 [00:42<00:15,  1.72it/s]\n",
      "                                                \n",
      "\n",
      " 73%|███████▎  | 73/100 [00:42<00:15,  1.72it/s]\n",
      " 74%|███████▍  | 74/100 [00:43<00:15,  1.65it/s]\n",
      "                                                \n",
      "\n",
      " 74%|███████▍  | 74/100 [00:43<00:15,  1.65it/s]\n",
      " 75%|███████▌  | 75/100 [00:44<00:14,  1.68it/s]\n",
      "                                                \n",
      "\n",
      " 75%|███████▌  | 75/100 [00:44<00:14,  1.68it/s]\n",
      " 76%|███████▌  | 76/100 [00:44<00:13,  1.71it/s]\n",
      "                                                \n",
      "\n",
      " 76%|███████▌  | 76/100 [00:44<00:13,  1.71it/s]\n",
      " 77%|███████▋  | 77/100 [00:45<00:13,  1.73it/s]\n",
      "                                                \n",
      "\n",
      " 77%|███████▋  | 77/100 [00:45<00:13,  1.73it/s]\n",
      " 78%|███████▊  | 78/100 [00:45<00:12,  1.73it/s]\n",
      "                                                \n",
      "\n",
      " 78%|███████▊  | 78/100 [00:45<00:12,  1.73it/s]\n",
      " 79%|███████▉  | 79/100 [00:46<00:11,  1.76it/s]\n",
      "                                                \n",
      "\n",
      " 79%|███████▉  | 79/100 [00:46<00:11,  1.76it/s]\n",
      " 80%|████████  | 80/100 [00:46<00:11,  1.73it/s]\n",
      "                                                \n",
      "\n",
      " 80%|████████  | 80/100 [00:47<00:11,  1.73it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                                \n",
      "\n",
      "\n",
      "                                     \n",
      "\u001b[A\n",
      " 80%|████████  | 80/100 [00:47<00:11,  1.73it/s]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 149.78it/s]\u001b[A\n",
      "\n",
      "                                              \u001b[A\n",
      " 81%|████████  | 81/100 [00:47<00:11,  1.69it/s]\n",
      "                                                \n",
      "\n",
      " 81%|████████  | 81/100 [00:47<00:11,  1.69it/s]\n",
      " 82%|████████▏ | 82/100 [00:48<00:10,  1.70it/s]\n",
      "                                                \n",
      "\n",
      " 82%|████████▏ | 82/100 [00:48<00:10,  1.70it/s]\n",
      " 83%|████████▎ | 83/100 [00:48<00:09,  1.72it/s]\n",
      "                                                \n",
      "\n",
      " 83%|████████▎ | 83/100 [00:48<00:09,  1.72it/s]\n",
      " 84%|████████▍ | 84/100 [00:49<00:09,  1.75it/s]\n",
      "                                                \n",
      "\n",
      " 84%|████████▍ | 84/100 [00:49<00:09,  1.75it/s]\n",
      " 85%|████████▌ | 85/100 [00:49<00:08,  1.75it/s]\n",
      "                                                \n",
      "\n",
      " 85%|████████▌ | 85/100 [00:49<00:08,  1.75it/s]\n",
      " 86%|████████▌ | 86/100 [00:50<00:08,  1.74it/s]\n",
      "                                                \n",
      "\n",
      " 86%|████████▌ | 86/100 [00:50<00:08,  1.74it/s]\n",
      " 87%|████████▋ | 87/100 [00:51<00:10,  1.25it/s]\n",
      "                                                \n",
      "\n",
      " 87%|████████▋ | 87/100 [00:51<00:10,  1.25it/s]\n",
      " 88%|████████▊ | 88/100 [00:53<00:11,  1.06it/s]\n",
      "                                                \n",
      "\n",
      " 88%|████████▊ | 88/100 [00:53<00:11,  1.06it/s]\n",
      " 89%|████████▉ | 89/100 [00:54<00:11,  1.06s/it]\n",
      "                                                \n",
      "\n",
      " 89%|████████▉ | 89/100 [00:54<00:11,  1.06s/it]\n",
      " 90%|█████████ | 90/100 [00:55<00:11,  1.13s/it]\n",
      "                                                \n",
      "\n",
      " 90%|█████████ | 90/100 [00:55<00:11,  1.13s/it]\n",
      " 91%|█████████ | 91/100 [00:57<00:10,  1.20s/it]\n",
      "                                                \n",
      "\n",
      " 91%|█████████ | 91/100 [00:57<00:10,  1.20s/it]\n",
      " 92%|█████████▏| 92/100 [00:58<00:09,  1.23s/it]\n",
      "                                                \n",
      "\n",
      " 92%|█████████▏| 92/100 [00:58<00:09,  1.23s/it]\n",
      " 93%|█████████▎| 93/100 [00:59<00:08,  1.26s/it]\n",
      "                                                \n",
      "\n",
      " 93%|█████████▎| 93/100 [00:59<00:08,  1.26s/it]\n",
      " 94%|█████████▍| 94/100 [01:01<00:07,  1.30s/it]\n",
      "                                                \n",
      "\n",
      " 94%|█████████▍| 94/100 [01:01<00:07,  1.30s/it]\n",
      " 95%|█████████▌| 95/100 [01:02<00:06,  1.34s/it]\n",
      "                                                \n",
      "\n",
      " 95%|█████████▌| 95/100 [01:02<00:06,  1.34s/it]\n",
      " 96%|█████████▌| 96/100 [01:03<00:05,  1.32s/it]\n",
      "                                                \n",
      "\n",
      " 96%|█████████▌| 96/100 [01:03<00:05,  1.32s/it]\n",
      " 97%|█████████▋| 97/100 [01:05<00:03,  1.30s/it]\n",
      "                                                \n",
      "\n",
      " 97%|█████████▋| 97/100 [01:05<00:03,  1.30s/it]\n",
      " 98%|█████████▊| 98/100 [01:06<00:02,  1.29s/it]\n",
      "                                                \n",
      "\n",
      " 98%|█████████▊| 98/100 [01:06<00:02,  1.29s/it]\n",
      " 99%|█████████▉| 99/100 [01:07<00:01,  1.32s/it]\n",
      "                                                \n",
      "\n",
      " 99%|█████████▉| 99/100 [01:07<00:01,  1.32s/it]\n",
      "100%|██████████| 100/100 [01:08<00:00,  1.28s/it]\n",
      "                                                 \n",
      "\n",
      "100%|██████████| 100/100 [01:08<00:00,  1.28s/it]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "                                                 \n",
      "\n",
      "\n",
      "                                     \n",
      "\u001b[A\n",
      "100%|██████████| 100/100 [01:09<00:00,  1.28s/it]\n",
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 225.84it/s]\u001b[A\n",
      "\n",
      "                                              \u001b[A\n",
      "                                                 \n",
      "\n",
      "100%|██████████| 100/100 [01:09<00:00,  1.28s/it]\n",
      "100%|██████████| 100/100 [01:09<00:00,  1.44it/s]\n",
      "\u001b[32m2024-08-21 20:48:04.825\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m504\u001b[0m - \u001b[34m\u001b[1mTraining metrics: {'train_runtime': 69.6288, 'train_samples_per_second': 5.745, 'train_steps_per_second': 1.436, 'train_loss': 0.19257629114462763, 'epoch': 4.08, 'train_samples': 1000}\u001b[0m\n",
      "\u001b[32m2024-08-21 20:48:04.825\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m505\u001b[0m - \u001b[1mSaving model checkpoint to outputs-dpo-qwen1.5\u001b[0m\n",
      "\u001b[32m2024-08-21 20:48:05.930\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m512\u001b[0m - \u001b[1m*** Evaluate ***\u001b[0m\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 56.66it/s]\n",
      "\u001b[32m2024-08-21 20:48:06.092\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mmain\u001b[0m:\u001b[36m518\u001b[0m - \u001b[34m\u001b[1mEval metrics: {'eval_loss': 2.5671938601590227e-06, 'eval_runtime': 0.1376, 'eval_samples_per_second': 7.269, 'eval_steps_per_second': 7.269, 'eval_rewards/chosen': 0.6950359344482422, 'eval_rewards/rejected': -12.17765998840332, 'eval_rewards/accuracies': 1.0, 'eval_rewards/margins': 12.872695922851562, 'eval_logps/rejected': -157.56813049316406, 'eval_logps/chosen': -6.9145660400390625, 'eval_logits/rejected': -1.8979873657226562, 'eval_logits/chosen': -1.4076347351074219, 'epoch': 4.08, 'eval_samples': 5}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python dpo_training.py \\\n",
    "    --model_type auto \\\n",
    "    --model_name_or_path Qwen1.5-1.8B-Chat \\\n",
    "    --train_file_dir data\\reward \\\n",
    "    --validation_file_dir data\\reward \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --use_peft True \\\n",
    "    --max_train_samples 1000 \\\n",
    "    --max_eval_samples 5 \\\n",
    "    --max_steps 100 \\\n",
    "    --eval_steps 20 \\\n",
    "    --save_steps 50 \\\n",
    "    --max_source_length 512 \\\n",
    "    --max_target_length 256 \\\n",
    "    --output_dir outputs-dpo-qwen1.5 \\\n",
    "    --target_modules q_proj,k_proj \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 16 \\\n",
    "    --lora_dropout 0.05 \\\n",
    "    --torch_dtype float16 \\\n",
    "    --fp16 True \\\n",
    "    --device_map auto \\\n",
    "    --report_to tensorboard \\\n",
    "    --remove_unused_columns False \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --cache_dir ./cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is New Volume\n",
      " Volume Serial Number is B4E8-CC63\n",
      "\n",
      " Directory of d:\\llm\\whole_process\\MedicalGPT\n",
      "\n",
      "\n",
      " Directory of d:\\llm\\whole_process\\MedicalGPT\\outputs-dpo-qwen1.5\n",
      "\n",
      "08/21/2024  08:48 PM    <DIR>          .\n",
      "08/21/2024  08:46 PM    <DIR>          ..\n",
      "08/21/2024  08:48 PM               672 adapter_config.json\n",
      "08/21/2024  08:48 PM         6,304,096 adapter_model.safetensors\n",
      "08/21/2024  08:48 PM                85 added_tokens.json\n",
      "08/21/2024  08:48 PM               749 all_results.json\n",
      "08/21/2024  08:48 PM    <DIR>          checkpoint-100\n",
      "08/21/2024  08:47 PM    <DIR>          checkpoint-50\n",
      "08/21/2024  08:48 PM               572 eval_results.json\n",
      "08/21/2024  08:48 PM         1,821,636 merges.txt\n",
      "08/21/2024  08:48 PM             5,091 README.md\n",
      "08/21/2024  08:46 PM    <DIR>          runs\n",
      "08/21/2024  08:48 PM               417 special_tokens_map.json\n",
      "08/21/2024  08:48 PM             1,350 tokenizer_config.json\n",
      "08/21/2024  08:48 PM               200 train_results.json\n",
      "08/21/2024  08:48 PM            54,444 trainer_state.json\n",
      "08/21/2024  08:48 PM             4,920 training_args.bin\n",
      "08/21/2024  08:48 PM         3,535,052 vocab.json\n",
      "              13 File(s)     11,729,284 bytes\n",
      "               5 Dir(s)  433,282,433,024 bytes free\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File Not Found\n"
     ]
    }
   ],
   "source": [
    "%ls -lh outputs-dpo-qwen1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "模型训练结果：\n",
    "- 使用lora训练模型，则保存的lora权重是`adapter_model.bin`, lora配置文件是`adapter_config.json`，合并到base model的方法见`merge_peft_adapter.py`\n",
    "- 日志保存在`output_dir/runs`目录下，可以使用tensorboard查看，启动tensorboard方式如下：`tensorboard --logdir output_dir/runs --host 0.0.0.0 --port 8009`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "lora模型权重合并到base model，合并后的模型保存在`--output_dir`目录下，合并方法如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model_type='auto', base_model='merged-sft', tokenizer_path=None, lora_model='outputs-dpo-qwen1.5', resize_emb=False, output_dir='merged-dpo-qwen1.5/', hf_hub_model_id='', hf_hub_token=None)\n",
      "Base model: merged-sft\n",
      "LoRA model: outputs-dpo-qwen1.5\n",
      "Loading LoRA for causal language model\n",
      "Merging with merge_and_unload...\n",
      "Saving to Hugging Face format...\n",
      "Done! model saved to merged-dpo-qwen1.5/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "!python merge_peft_adapter.py --model_type auto \\\n",
    "    --base_model merged-sft --lora_model outputs-dpo-qwen1.5 --output_dir merged-dpo-qwen1.5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is New Volume\n",
      " Volume Serial Number is B4E8-CC63\n",
      "\n",
      " Directory of d:\\llm\\whole_process\\MedicalGPT\n",
      "\n",
      "\n",
      " Directory of d:\\llm\\whole_process\\MedicalGPT\\merged-dpo-qwen1.5\n",
      "\n",
      "08/21/2024  08:50 PM    <DIR>          .\n",
      "08/21/2024  08:50 PM    <DIR>          ..\n",
      "08/21/2024  08:50 PM                85 added_tokens.json\n",
      "08/21/2024  08:50 PM               722 config.json\n",
      "08/21/2024  08:50 PM               217 generation_config.json\n",
      "08/21/2024  08:50 PM         1,671,853 merges.txt\n",
      "08/21/2024  08:50 PM     3,673,690,400 model.safetensors\n",
      "08/21/2024  08:50 PM               387 special_tokens_map.json\n",
      "08/21/2024  08:50 PM         7,028,015 tokenizer.json\n",
      "08/21/2024  08:50 PM             1,342 tokenizer_config.json\n",
      "08/21/2024  08:50 PM         2,776,833 vocab.json\n",
      "               9 File(s)  3,685,169,854 bytes\n",
      "               2 Dir(s)  429,597,249,536 bytes free\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File Not Found\n"
     ]
    }
   ],
   "source": [
    "%ls -lh merged-dpo-qwen1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%cat` not found.\n"
     ]
    }
   ],
   "source": [
    "#%cat merged-dpo/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Stage3 偏好建模第一次训练完成。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**至此一个完整的训练流程演示完成。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T12:34:29.658428Z",
     "start_time": "2023-06-26T12:34:29.620609Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model_type='auto', base_model='Qwen1.5-1.8B-Chat', lora_model='', tokenizer_path=None, template_name='vicuna', repetition_penalty=1.0, max_new_tokens=512, data_file=None, interactive=False, single_tune=False, temperature=0.7, output_file='./predictions_result.jsonl', eval_batch_size=4, resize_emb=False, load_in_8bit=False, load_in_4bit=False)\n",
      "Qwen2TokenizerFast(name_or_path='Qwen1.5-1.8B-Chat', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "Start inference.\n",
      "===\n",
      "Input: 介绍下北京\n",
      "Output: 北京是中国的首都，位于中国北部，是中华人民共和国的首都，是中华人民共和国的经济、政治、文化、交通、科研、教育和国际交往中心。北京位于华北平原的北部，是世界上最大的城市之一，也是中国的政治、文化、教育、科技、经济、交通、旅游、娱乐和国际交往中心之一。北京的气候属于温带大陆性气候，四季分明，年平均气温约为10℃，冬季寒冷，夏季炎热。北京的地形地貌主要以平原和山地为主，其中，北京的北部和东部是平原，南部和西部是山地。北京的地形复杂多样，包括平原、丘陵、山地、河流、湖泊和湿地等。北京的河流主要有永定河、潮白河、北运河、通惠河和南运河等，它们分别流经北京市区，形成了北京市的主要水系。北京的交通发达，拥有众多的铁路、公路、航空和水运交通网络，包括京九铁路、京广铁路、京沪铁路、京哈铁路、京沈铁路、京港澳高速公路、京开高速公路、京藏高速公路、京通高速公路、京开高速公路、京平高速公路、京通高速公路、京沪高速公路、京哈高速公路、京沈高速公路、京广高速公路、京广铁路、京沪铁路、京哈高速公路、京沈高速公路、京广高速公路、京哈高速公路、京沈高速公路、京广高速公路、京广铁路、京沪铁路、京哈高速公路、京沈高速公路、京广高速公路、京广铁路、京沪铁路、京哈高速公路、京沈高速公路、京广高速公路、京广铁路、京沪铁路、京哈高速公路、京沈高速公路、京广高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京沪高速公路、京\n",
      "\n",
      "===\n",
      "Input: 乙肝和丙肝的区别？\n",
      "Output: 乙肝和丙肝都是由乙型肝炎病毒（HBV）感染引起的肝炎，但它们的传播途径、症状、治疗和预后有所不同。\n",
      "\n",
      "1. 传播途径：乙肝主要通过血液传播，如输血、注射毒品、性接触、母婴传播等。丙肝的传播途径则主要通过血液、性接触和母婴传播。此外，乙型肝炎病毒还可以通过蚊虫叮咬传播，但这种传播方式相对较少。\n",
      "\n",
      "2. 症状：乙肝的症状包括疲劳、食欲不振、恶心、呕吐、黄疸、肝区疼痛、肝功能异常、肝硬化等。丙肝的症状通常较轻，包括疲劳、食欲不振、恶心、呕吐、尿黄、肝区疼痛、肝功能异常等。丙肝患者可能会出现持续性肝炎症状，但病情发展较慢，通常不会出现肝硬化。\n",
      "\n",
      "3. 治疗：乙肝的治疗主要包括抗病毒治疗和对症治疗。抗病毒治疗是治疗乙肝的关键，主要使用抗病毒药物，如拉米夫定、恩替卡韦等，以抑制病毒复制，防止病毒进一步感染肝细胞。对症治疗主要包括休息、饮食调理、避免饮酒、避免接触病毒源等，以减轻症状，促进肝脏功能恢复。\n",
      "\n",
      "4. 预后：乙肝的预后取决于病毒的复制水平、肝细胞的损伤程度、患者的免疫状态以及治疗的及时性和有效性。一般来说，乙肝病毒感染后，如果及时进行抗病毒治疗，大多数患者可以完全康复，且肝功能恢复正常。但是，如果病毒复制活跃，或者患者免疫功能低下，治疗效果可能不佳，甚至可能导致肝硬化、肝癌等严重并发症。\n",
      "\n",
      "总的来说，乙肝和丙肝都是由乙型肝炎病毒引起的肝炎，但它们的传播途径、症状、治疗和预后有所不同。了解这些差异，有助于患者及早发现并治疗肝炎，以减少肝炎对身体的损害。如果您有任何关于乙肝或丙肝的问题，欢迎随时向我提问，我将尽我所能提供详细的解答。\n",
      "\n",
      "save to ./predictions_result.jsonl, size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]d:\\anaconda\\envs\\py39\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:698: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "\n",
      "Generating outputs: 100%|██████████| 1/1 [00:08<00:00,  8.27s/it]\n",
      "Generating outputs: 100%|██████████| 1/1 [00:08<00:00,  8.27s/it]\n"
     ]
    }
   ],
   "source": [
    "!python inference.py --model_type auto --base_model Qwen1.5-1.8B-Chat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-26T12:35:00.864463Z",
     "start_time": "2023-06-26T12:34:47.802087Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model_type='auto', base_model='merged-dpo-qwen1.5', lora_model='', tokenizer_path=None, template_name='vicuna', repetition_penalty=1.0, max_new_tokens=512, data_file=None, interactive=False, single_tune=False, temperature=0.7, output_file='./predictions_result.jsonl', eval_batch_size=4, resize_emb=False, load_in_8bit=False, load_in_4bit=False)\n",
      "Qwen2TokenizerFast(name_or_path='merged-dpo-qwen1.5', vocab_size=151643, model_max_length=32768, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      "Start inference.\n",
      "===\n",
      "Input: 介绍下北京\n",
      "Output: 北京是中国的首都，位于中国北部，是中国政治、经济、文化中心，也是世界上著名的历史文化名城之一。北京是中国四大古都之一，也是世界上拥有世界文化遗产最多的城市，有30多处世界文化遗产，是世界历史文化名城。北京是中华人民共和国的首都，是中华人民共和国的经济中心和政治中心，同时是世界著名古都和现代化国际大都市。北京位于中国北部，东临渤海，西临黄海，北濒燕山，南界华北平原。总面积16647.34平方千米，建成区面积2160平方千米。北京是中华人民共和国的首都，是全国的政治、文化、交通、科研、经济、金融中心，也是世界上著名的历史文化名城和现代化国际大都市。北京是中华人民共和国的首都，是全国的政治、文化、交通、科研、经济、金融中心，也是世界上著名的历史文化名城和现代化国际大都市。北京是中华人民共和国的首都，是全国的政治、文化、交通、科研、经济、金融中心，也是世界上著名的历史文化名城和现代化国际大都市。北京是中华人民共和国的首都，是全国的政治、文化、交通、科研、经济、金融中心，也是世界上著名的历史文化名城和现代化国际大都市。北京是中华人民共和国的首都，是全国的政治、文化、交通、科研、经济、金融中心，也是世界上著名的历史文化名城和现代化国际大都市。北京是中华人民共和国的首都，是全国的政治、文化、交通、科研、经济、金融中心，也是世界上著名的历史文化名城和现代化国际大都市。北京是中华人民共和国的首都，是全国的政治、文化、交通、科研、经济、金融中心，也是世界上著名的历史文化名城和现代化国际大都市。北京是中华人民共和国的首都，是全国的政治、文化、交通、科研、经济、金融中心，也是世界上著名的历史文化名城和现代化国际大都市。北京是中华人民共和国的首都，是全国的政治、文化、交通、科研、经济、金融中心，也是世界上著名的历史文化名城和现代化国际大都市。北京是中华人民共和国的首都，是全国的政治、文化、交通、科研、经济、金融中心，也是世界上著名的历史文化名城和现代化国际大都市。北京是中华人民共和国的首都，是全国的政治、文化、交通、科研、经济、金融中心，也是世界上著名的历史文化名城和现代化国际\n",
      "\n",
      "===\n",
      "Input: 乙肝和丙肝的区别？\n",
      "Output: 乙肝和丙肝都是病毒性肝炎，但是乙型肝炎和丙型肝炎的症状不同。乙肝的症状是疲乏、肝区疼痛、恶心、呕吐、食欲不振、厌油、肝大、黄疸、肝功能异常、肝穿刺活检有假小叶、有肝硬化、肝癌。丙型肝炎的症状是疲乏、厌食、恶心、呕吐、肝区疼痛、肝大、黄疸、肝功能异常、肝穿刺活检有假小叶、有肝硬化、肝癌。这两种肝炎的治疗是不一样的，乙肝是需要抗病毒治疗，而丙型肝炎需要抗病毒治疗。\n",
      "\n",
      "save to ./predictions_result.jsonl, size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "\n",
      "Generating outputs:   0%|          | 0/1 [00:00<?, ?it/s]d:\\anaconda\\envs\\py39\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:698: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "\n",
      "Generating outputs: 100%|██████████| 1/1 [00:08<00:00,  8.42s/it]\n",
      "Generating outputs: 100%|██████████| 1/1 [00:08<00:00,  8.42s/it]\n"
     ]
    }
   ],
   "source": [
    "!python inference.py --model_type auto --base_model merged-dpo-qwen1.5\n",
    "# 或在shell中运行\n",
    "# python inference.py --model_type bloom --base_model merged-dpo --interactive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Input:介绍下南京\n",
    "Response:  南京市位于江苏省西南部，是全国首批历史文化名城、国家中心城市和自由贸易试验区。\n",
    "\n",
    "完。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "vscode": {
   "interpreter": {
    "hash": "f34eed0bebedfc4b6ee51ced43d2c030fe3b92f13c149d072205ca200a67b1ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
